<!doctype html><html lang=en class=no-js><head><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><script async src="https://www.googletagmanager.com/gtag/js?id=G-JPP6RFM2BP"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-JPP6RFM2BP')</script><link rel=alternate hreflang=zh href=https://kubernetes.io/zh/docs/setup/production-environment/windows/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/production-environment/windows/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/production-environment/windows/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/production-environment/windows/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/production-environment/windows/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.82.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/setup/production-environment/windows/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Windows in Kubernetes | Kubernetes</title><meta property="og:title" content="Windows in Kubernetes"><meta property="og:description" content="Production-Grade Container Orchestration"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/setup/production-environment/windows/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Windows in Kubernetes"><meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Windows in Kubernetes"><meta name=twitter:description content="Production-Grade Container Orchestration"><link rel=preload href=/scss/main.min.a4bbbf4cec44bb587f51c9896c6b721ca3ded0a7e145b8a5941d804afbcc0c8d.css as=style><link href=/scss/main.min.a4bbbf4cec44bb587f51c9896c6b721ca3ded0a7e145b8a5941d804afbcc0c8d.css rel=stylesheet integrity><script src=/js/jquery-3.3.1.min.js integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin=anonymous></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png"}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/docs/setup/production-environment/windows/"><meta property="og:title" content="Windows in Kubernetes"><meta name=twitter:title content="Windows in Kubernetes"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/script.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</span></a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/setup/production-environment/windows/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/setup/production-environment/windows/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/setup/production-environment/windows/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/setup/production-environment/windows/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/setup/production-environment/windows/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh/docs/setup/production-environment/windows/>中文 Chinese</a>
<a class=dropdown-item href=/ko/docs/setup/production-environment/windows/>한국어 Korean</a>
<a class=dropdown-item href=/ja/docs/setup/production-environment/windows/>日本語 Japanese</a>
<a class=dropdown-item href=/fr/docs/setup/production-environment/windows/>Français</a>
<a class=dropdown-item href=/uk/docs/setup/production-environment/windows/>Українська</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/setup/production-environment/windows/>Return to the regular view of this page</a>.</p></div><h1 class=title>Windows in Kubernetes</h1><ul><li>1: <a href=#pg-a307d413f1f7430fced233023087e2a1>Intro to Windows support in Kubernetes</a></li><li>2: <a href=#pg-3a51e66c5de55f9093a8dc55742006d3>Guide for scheduling Windows containers in Kubernetes</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-a307d413f1f7430fced233023087e2a1>1 - Intro to Windows support in Kubernetes</h1><p>Windows applications constitute a large portion of the services and
applications that run in many organizations.
<a href=https://aka.ms/windowscontainers>Windows containers</a> provide a modern way to
encapsulate processes and package dependencies, making it easier to use DevOps
practices and follow cloud native patterns for Windows applications.
Kubernetes has become the defacto standard container orchestrator, and the
release of Kubernetes 1.14 includes production support for scheduling Windows
containers on Windows nodes in a Kubernetes cluster, enabling a vast ecosystem
of Windows applications to leverage the power of Kubernetes. Organizations
with investments in Windows-based applications and Linux-based applications
don't have to look for separate orchestrators to manage their workloads,
leading to increased operational efficiencies across their deployments,
regardless of operating system.</p><h2 id=windows-containers-in-kubernetes>Windows containers in Kubernetes</h2><p>To enable the orchestration of Windows containers in Kubernetes, include
Windows nodes in your existing Linux cluster. Scheduling Windows containers in
<a class=glossary-tooltip title="A Pod represents a set of running containers in your cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> on Kubernetes is similar to
scheduling Linux-based containers.</p><p>In order to run Windows containers, your Kubernetes cluster must include
multiple operating systems, with control plane nodes running Linux and workers
running either Windows or Linux depending on your workload needs. Windows
Server 2019 is the only Windows operating system supported, enabling
<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node>Kubernetes Node</a>
on Windows (including kubelet,
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/containerd>container runtime</a>,
and kube-proxy). For a detailed explanation of Windows distribution channels
see the <a href=https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19>Microsoft documentation</a>.</p><p>The Kubernetes control plane, including the
<a href=/docs/concepts/overview/components/>master components</a>,
continues to run on Linux.
There are no plans to have a Windows-only Kubernetes cluster.</p><p>In this document, when we talk about Windows containers we mean Windows
containers with process isolation. Windows containers with
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container>Hyper-V isolation</a>
is planned for a future release.</p><h2 id=supported-functionality-and-limitations>Supported Functionality and Limitations</h2><h3 id=supported-functionality>Supported Functionality</h3><h4 id=windows-os-version-support>Windows OS Version Support</h4><p>Refer to the following table for Windows operating system support in
Kubernetes. A single heterogeneous Kubernetes cluster can have both Windows
and Linux worker nodes. Windows containers have to be scheduled on Windows
nodes and Linux containers on Linux nodes.</p><table><thead><tr><th>Kubernetes version</th><th>Windows Server LTSC releases</th><th>Windows Server SAC releases</th><th></th></tr></thead><tbody><tr><td><em>Kubernetes v1.19</em></td><td>Windows Server 2019</td><td>Windows Server ver 1909, Windows Server ver 2004</td><td></td></tr><tr><td><em>Kubernetes v1.20</em></td><td>Windows Server 2019</td><td>Windows Server ver 1909, Windows Server ver 2004</td><td></td></tr><tr><td><em>Kubernetes v1.21</em></td><td>Windows Server 2019</td><td>Windows Server ver 2004, Windows Server ver 20H2</td><td></td></tr></tbody></table><p>Information on the different Windows Server servicing channels including their
support models can be found at
<a href=https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19>Windows Server servicing channels</a>.</p><p>We don't expect all Windows customers to update the operating system for their
apps frequently. Upgrading your applications is what dictates and necessitates
upgrading or introducing new nodes to the cluster. For the customers that
chose to upgrade their operating system for containers running on Kubernetes,
we will offer guidance and step-by-step instructions when we add support for a
new operating system version. This guidance will include recommended upgrade
procedures for upgrading user applications together with cluster nodes.
Windows nodes adhere to Kubernetes
<a href=/docs/setup/release/version-skew-policy/>version-skew policy</a> (node to control plane
versioning) the same way as Linux nodes do today.</p><p>The Windows Server Host Operating System is subject to the
<a href=https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing>Windows Server</a>
licensing. The Windows Container images are subject to the
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/images-eula>Supplemental License Terms for Windows containers</a>.</p><p>Windows containers with process isolation have strict compatibility rules,
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility>where the host OS version must match the container base image OS version</a>.
Once we support Windows containers with Hyper-V isolation in Kubernetes, the
limitation and compatibility rules will change.</p><h4 id=pause-image>Pause Image</h4><p>Microsoft maintains a Windows pause infrastructure container at
<code>mcr.microsoft.com/oss/kubernetes/pause:3.4.1</code>.
Kubernetes maintains a multi-architecture image <code>k8s.gcr.io/pause:3.5</code> that
supports Linux as well as Windows.</p><h4 id=compute>Compute</h4><p>From an API and kubectl perspective, Windows containers behave in much the
same way as Linux-based containers. However, there are some notable
differences in key functionality which are outlined in the
<a href=#limitations>limitation section</a>.</p><p>Key Kubernetes elements work the same way in Windows as they do in Linux. In
this section, we talk about some of the key workload enablers and how they map
to Windows.</p><ul><li><p><a href=/docs/concepts/workloads/pods/>Pods</a></p><p>A Pod is the basic building block of Kubernetes–the smallest and simplest
unit in the Kubernetes object model that you create or deploy. You may not
deploy Windows and Linux containers in the same Pod. All containers in a Pod
are scheduled onto a single Node where each Node represents a specific
platform and architecture. The following Pod capabilities, properties and
events are supported with Windows containers:</p><ul><li>Single or multiple containers per Pod with process isolation and volume sharing</li><li>Pod status fields</li><li>Readiness and Liveness probes</li><li>postStart & preStop container lifecycle events</li><li>ConfigMap, Secrets: as environment variables or volumes</li><li>EmptyDir</li><li>Named pipe host mounts</li><li>Resource limits</li></ul></li><li><p><a href=/docs/concepts/workloads/controllers/>Controllers</a></p><p>Kubernetes controllers handle the desired state of Pods. The following
workload controllers are supported with Windows containers:</p><ul><li>ReplicaSet</li><li>ReplicationController</li><li>Deployments</li><li>StatefulSets</li><li>DaemonSet</li><li>Job</li><li>CronJob</li></ul></li><li><p><a href=/docs/concepts/services-networking/service/>Services</a></p><p>A Kubernetes Service is an abstraction which defines a logical set of Pods
and a policy by which to access them - sometimes called a micro-service. You
can use services for cross-operating system connectivity. In Windows, services
can utilize the following types, properties and capabilities:</p><ul><li>Service Environment variables</li><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li><li>Headless services</li></ul></li></ul><p>Pods, Controllers and Services are critical elements to managing Windows
workloads on Kubernetes. However, on their own they are not enough to enable
the proper lifecycle management of Windows workloads in a dynamic cloud native
environment. We added support for the following features:</p><ul><li>Pod and container metrics</li><li>Horizontal Pod Autoscaler support</li><li>kubectl Exec</li><li>Resource Quotas</li><li>Scheduler preemption</li></ul><h4 id=container-runtime>Container Runtime</h4><h5 id=docker-ee>Docker EE</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p>Docker EE-basic 19.03+ is the recommended container runtime for all Windows
Server versions. This works with the dockershim code included in the kubelet.</p><h5 id=cri-containerd>CRI-ContainerD</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p><a class=glossary-tooltip title="A container runtime with an emphasis on simplicity, robustness and portability" data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=ContainerD>ContainerD</a> 1.4.0+ can
also be used as the container runtime for Windows Kubernetes nodes.</p><p>Learn how to
<a href=/docs/setup/production-environment/container-runtimes/#install-containerd>install ContainerD on a Windows</a>.</p><h4 id=persistent-storage>Persistent Storage</h4><p>Kubernetes <a href=/docs/concepts/storage/volumes/>volumes</a> enable complex
applications, with data persistence and Pod volume sharing requirements, to be
deployed on Kubernetes. Management of persistent volumes associated with a
specific storage back-end or protocol includes actions such as:
provisioning/de-provisioning/resizing of volumes, attaching/detaching a volume
to/from a Kubernetes node and mounting/dismounting a volume to/from individual
containers in a pod that needs to persist data. The code implementing these
volume management actions for a specific storage back-end or protocol is
shipped in the form of a Kubernetes volume
<a href=/docs/concepts/storage/volumes/#types-of-volumes>plugin</a>. The following
broad classes of Kubernetes volume plugins are supported on Windows:</p><h5 id=in-tree-volume-plugins>In-tree Volume Plugins</h5><p>Code associated with in-tree volume plugins ship as part of the core
Kubernetes code base. Deployment of in-tree volume plugins do not require
installation of additional scripts or deployment of separate containerized
plugin components. These plugins can handle: provisioning/de-provisioning and
resizing of volumes in the storage backend, attaching/detaching of volumes
to/from a Kubernetes node and mounting/dismounting a volume to/from individual
containers in a pod. The following in-tree plugins support Windows nodes:</p><ul><li><a href=/docs/concepts/storage/volumes/#awselasticblockstore>awsElasticBlockStore</a></li><li><a href=/docs/concepts/storage/volumes/#azuredisk>azureDisk</a></li><li><a href=/docs/concepts/storage/volumes/#azurefile>azureFile</a></li><li><a href=/docs/concepts/storage/volumes/#gcepersistentdisk>gcePersistentDisk</a></li><li><a href=/docs/concepts/storage/volumes/#vspherevolume>vsphereVolume</a></li></ul><h5 id=flexvolume-plugins>FlexVolume Plugins</h5><p>Code associated with <a href=/docs/concepts/storage/volumes/#flexVolume>FlexVolume</a>
plugins ship as out-of-tree scripts or binaries that need to be deployed
directly on the host. FlexVolume plugins handle attaching/detaching of volumes
to/from a Kubernetes node and mounting/dismounting a volume to/from individual
containers in a pod. Provisioning/De-provisioning of persistent volumes
associated with FlexVolume plugins may be handled through an external
provisioner that is typically separate from the FlexVolume plugins. The
following FlexVolume
<a href=https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows>plugins</a>,
deployed as powershell scripts on the host, support Windows nodes:</p><ul><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd>SMB</a></li><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd>iSCSI</a></li></ul><h5 id=csi-plugins>CSI Plugins</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div><p>Code associated with <a class=glossary-tooltip title="The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers." data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> plugins
ship as out-of-tree scripts and binaries that are typically distributed as
container images and deployed using standard Kubernetes constructs like
DaemonSets and StatefulSets. CSI plugins handle a wide range of volume
management actions in Kubernetes: provisioning/de-provisioning/resizing of
volumes, attaching/detaching of volumes to/from a Kubernetes node and
mounting/dismounting a volume to/from individual containers in a pod,
backup/restore of persistent data using snapshots and cloning. CSI plugins
typically consist of node plugins (that run on each node as a DaemonSet) and
controller plugins.</p><p>CSI node plugins (especially those associated with persistent volumes exposed
as either block devices or over a shared file-system) need to perform various
privileged operations like scanning of disk devices, mounting of file systems,
etc. These operations differ for each host operating system. For Linux worker
nodes, containerized CSI node plugins are typically deployed as privileged
containers. For Windows worker nodes, privileged operations for containerized
CSI node plugins is supported using
<a href=https://github.com/kubernetes-csi/csi-proxy>csi-proxy</a>, a community-managed,
stand-alone binary that needs to be pre-installed on each Windows node. Please
refer to the deployment guide of the CSI plugin you wish to deploy for further
details.</p><h4 id=networking>Networking</h4><p>Networking for Windows containers is exposed through
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>CNI plugins</a>.
Windows containers function similarly to virtual machines in regards to
networking. Each container has a virtual network adapter (vNIC) which is
connected to a Hyper-V virtual switch (vSwitch). The Host Networking Service
(HNS) and the Host Compute Service (HCS) work together to create containers
and attach container vNICs to networks. HCS is responsible for the management
of containers whereas HNS is responsible for the management of networking
resources such as:</p><ul><li>Virtual networks (including creation of vSwitches)</li><li>Endpoints / vNICs</li><li>Namespaces</li><li>Policies (Packet encapsulations, Load-balancing rules, ACLs, NAT'ing rules, etc.)</li></ul><p>The following service spec types are supported:</p><ul><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li></ul><h5 id=network-modes>Network modes</h5><p>Windows supports five different networking drivers/modes: L2bridge, L2tunnel,
Overlay, Transparent, and NAT. In a heterogeneous cluster with Windows and
Linux worker nodes, you need to select a networking solution that is
compatible on both Windows and Linux. The following out-of-tree plugins are
supported on Windows, with recommendations on when to use each CNI:</p><table><thead><tr><th>Network Driver</th><th>Description</th><th>Container Packet Modifications</th><th>Network Plugins</th><th>Network Plugin Characteristics</th></tr></thead><tbody><tr><td>L2bridge</td><td>Containers are attached to an external vSwitch. Containers are attached
to the underlay network, although the physical network doesn't need to learn
the container. MACs because they are rewritten on ingress/egress.</td><td>MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS
OutboundNAT policy.</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge>win-bridge<a>,
<a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a>,
Flannel host-gateway uses win-bridge</td><td>win-bridge uses L2bridge network mode,
connects containers to the underlay of hosts, offering best performance.
Requires user-defined routes (UDR) for inter-node connectivity.</td></tr><tr><td>L2Tunnel</td><td>This is a special case of l2bridge, but only used on Azure. All packets
are sent to the virtualization host where SDN policy is applied.</td><td>MAC rewritten, IP visible on the underlay network</td><td><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a></td><td>Azure-CNI allows integration of containers with Azure vNET, and allows them
to leverage the set of capabilities that
<a href=https://azure.microsoft.com/en-us/services/virtual-network/>Azure Virtual Network</a>
provides. For example, securely connect to Azure services or use Azure NSGs.
See <a href=https://docs.microsoft.com/en-us/azure/aks/concepts-network#azure-cni-advanced-networking>azure-cni</a>
for some examples.</td></tr><tr><td>Overlay (Overlay networking for Windows in Kubernetes is in <b>Alpha</b> stage)</td><td>Containers are given a vNIC connected to an external vSwitch. Each overlay
network gets its own IP subnet, defined by a custom IP prefix.The overlay
network driver uses VXLAN encapsulation.</td><td>Encapsulated with an outer header.</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay>Win-overlay</a>,
Flannel VXLAN (uses win-overlay)</td><td>win-overlay should be used when virtual container networks are desired to
be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs
to be re-used for different overlay networks (which have different VNID tags)
if you are restricted on IPs in your datacenter. This option requires
<a href=https://support.microsoft.com/help/4489899>KB4489899</a> on Windows Server
2019.</td></tr><tr><td>Transparent (special use case for <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>)</td><td>Requires an external vSwitch. Containers are attached to an external
vSwitch which enables intra-pod communication via logical networks (logical
switches and routers).</td><td>Packet is encapsulated either via
<a href=https://datatracker.ietf.org/doc/draft-gross-geneve/>GENEVE</a>,
<a href=https://datatracker.ietf.org/doc/draft-davie-stt/>STT</a> tunneling to reach
pods which are not on the same host.<br>Packets are forwarded or dropped
via the tunnel metadata information supplied by the ovn network controller.<br>NAT is done for north-south communication.</td><td><a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a></td><td>Deploy via <a href=https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib>Ansible</a>.
Distributed ACLs can be applied via Kubernetes policies. IPAM support.
Load-balancing can be achieved without kube-proxy. NATing is done without
using iptables/netsh.</td></tr><tr><td>NAT (<b>not used in Kubernetes</b>)</td><td>Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is
provided using an internal component called
<a href=https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations>WinNAT</a>.</td><td>MAC and IP is rewritten to host MAC/IP.</td><td><a href=https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat>nat</a></td><td>Included here for completeness</td></tr></tbody></table><p>As outlined above, the <a href=https://github.com/coreos/flannel>Flannel</a> CNI
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>meta plugin</a>
is also supported on
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel#windows-support-experimental>Windows</a>
via the <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>VXLAN network backend</a>
(<strong>alpha support</strong> ; delegates to win-overlay) and
<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw>host-gateway network backend</a>
(stable support; delegates to win-bridge). This plugin supports delegating to
one of the reference CNI plugins (win-overlay, win-bridge), to work in
conjunction with Flannel daemon on Windows (Flanneld) for automatic node
subnet lease assignment and HNS network creation. This plugin reads in its own
configuration file (cni.conf), and aggregates it with the environment
variables from the FlannelD generated subnet.env file. It then delegates to
one of the reference CNI plugins for network plumbing, and sends the correct
configuration containing the node-assigned subnet to the IPAM plugin (e.g.
host-local).</p><p>For the node, pod, and service objects, the following network flows are
supported for TCP/UDP traffic:</p><ul><li>Pod -> Pod (IP)</li><li>Pod -> Pod (Name)</li><li>Pod -> Service (Cluster IP)</li><li>Pod -> Service (PQDN, but only if there are no ".")</li><li>Pod -> Service (FQDN)</li><li>Pod -> External (IP)</li><li>Pod -> External (DNS)</li><li>Node -> Pod</li><li>Pod -> Node</li></ul><h5 id=ipam>IP address management (IPAM)</h5><p>The following IPAM options are supported on Windows:</p><ul><li><a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local>Host-local</a></li><li>HNS IPAM (Inbox platform IPAM, this is a fallback when no IPAM is set)</li><li><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md>Azure-vnet-ipam</a> (for azure-cni only)</li></ul><h5 id=load-balancing-and-services>Load balancing and Services</h5><p>On Windows, you can use the following settings to configure Services and load
balancing behavior:</p><table><caption style=display:none>Windows Service Settings</caption><thead><tr><th>Feature</th><th>Description</th><th>Supported Kubernetes version</th><th>Supported Windows OS build</th><th>How to enable</th></tr><thead><tbody><tr><td>Session affinity</td><td>Ensures that connections from a particular client are passed to the same
Pod each time.</td><td>v1.20+</td><td><a href=https://blogs.windows.com/windowsexperience/2020/01/28/announcing-windows-server-vnext-insider-preview-build-19551/>Windows Server vNext Insider Preview Build 19551</a> (or higher)</td><td>Set <code>service.spec.sessionAffinity</code> to "ClientIP"</td></tr><tr><td>Direct Server Return (DSR)</td><td>Load balancing mode where the IP address fixups and the LBNAT occurs at
the container vSwitch port directly; service traffic arrives with the source
IP set as the originating pod IP.</td><td>v1.20+</td><td>Windows Server 2019</td><td>Set the following flags in kube-proxy:
<code>--feature-gates="WinDSR=true" --enable-dsr=true</code></td></tr><tr><td>Preserve-Destination</td><td>Skips DNAT of service traffic, thereby preserving the virtual IP of the target
service in packets reaching the backend Pod. Also disables node-node forwarding.</td><td>v1.20+</td><td>Windows Server, version 1903 (or higher)</td><td>Set <code>"preserve-destination": "true"</code> in service annotations
and enable DSR in kube-proxy.</td></tr><tr><td>IPv4/IPv6 dual-stack networking</td><td>Native IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from,
and within a cluster</td><td>v1.19+</td><td>Windows Server, version 2004 (or higher)</td><td>See <a href=#ipv4ipv6-dual-stack>IPv4/IPv6 dual-stack</a></td></tr><tr><td>Client IP preservation</td><td>Ensures that source IP of incoming ingress traffic gets preserved. Also
disables node-node forwarding.</td><td>v1.20+</td><td>Windows Server, version 2019 (or higher)</td><td>Set <code>service.spec.externalTrafficPolicy</code> to "Local" and enable
DSR in kube-proxy.</td></tr></tbody></table><h4 id=ipv4-ipv6-dual-stack>IPv4/IPv6 dual-stack</h4><p>You can enable IPv4/IPv6 dual-stack networking for <code>l2bridge</code> networks using
the <code>IPv6DualStack</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>. See
<a href=/docs/concepts/services-networking/dual-stack#enable-ipv4ipv6-dual-stack>enable IPv4/IPv6 dual stack</a>
for more details.</p><p>On Windows, using IPv6 with Kubernetes require Windows Server, version 2004
(kernel version 10.0.19041.610) or later.</p><p>Overlay (VXLAN) networks on Windows do not support dual-stack networking today.</p><h3 id=limitations>Limitations</h3><p>Windows is only supported as a worker node in the Kubernetes architecture and
component matrix. This means that a Kubernetes cluster must always include
Linux master nodes, zero or more Linux worker nodes, and zero or more Windows
worker nodes.</p><h4 id=resource-handling>Resource Handling</h4><p>Linux cgroups are used as a pod boundary for resource controls in Linux.
Containers are created within that boundary for network, process and file
system isolation. The cgroups APIs can be used to gather cpu/io/memory stats.
In contrast, Windows uses a Job object per container with a system namespace
filter to contain all processes in a container and provide logical isolation
from the host. There is no way to run a Windows container without the
namespace filtering in place. This means that system privileges cannot be
asserted in the context of the host, and thus privileged containers are not
available on Windows. Containers cannot assume an identity from the host
because the Security Account Manager (SAM) is separate.</p><h4 id=resource-reservations>Resource Reservations</h4><h5 id=memory-reservations>Memory Reservations</h5><p>Windows does not have an out-of-memory process killer as Linux does. Windows
always treats all user-mode memory allocations as virtual, and pagefiles are
mandatory. The net effect is that Windows won't reach out of memory conditions
the same way Linux does, and processes page to disk instead of being subject
to out of memory (OOM) termination. If memory is over-provisioned and all
physical memory is exhausted, then paging can slow down performance.</p><p>Keeping memory usage within reasonable bounds is possible using the kubelet
parameters <code>--kubelet-reserve</code> and/or <code>--system-reserve</code> to account for memory
usage on the node (outside of containers). This reduces
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>NodeAllocatable</a>.</p><p>As you deploy workloads, use resource limits (must set only limits or limits
must equal requests) on containers. This also subtracts from NodeAllocatable
and prevents the scheduler from adding more pods once a node is full.</p><p>A best practice to avoid over-provisioning is to configure the kubelet with a
system reserved memory of at least 2GB to account for Windows, Docker, and
Kubernetes processes.</p><h5 id=cpu-reservations>CPU Reservations</h5><p>To account for Windows, Docker and other Kubernetes host processes it is
recommended to reserve a percentage of CPU so they are able to respond to
events. This value needs to be scaled based on the number of CPU cores
available on the Windows node.To determine this percentage a user should
identify the maximum pod density for each of their nodes and monitor the CPU
usage of the system services choosing a value that meets their workload needs.</p><p>Keeping CPU usage within reasonable bounds is possible using the kubelet
parameters <code>--kubelet-reserve</code> and/or <code>--system-reserve</code> to account for CPU
usage on the node (outside of containers). This reduces
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>NodeAllocatable</a>.</p><h4 id=feature-restrictions>Feature Restrictions</h4><ul><li>TerminationGracePeriod: not implemented</li><li>Single file mapping: to be implemented with CRI-ContainerD</li><li>Termination message: to be implemented with CRI-ContainerD</li><li>Privileged Containers: not currently supported in Windows containers</li><li>HugePages: not currently supported in Windows containers</li><li>The existing node problem detector is Linux-only and requires privileged
containers. In general, we don't expect this to be used on Windows because
privileged containers are not supported</li><li>Not all features of shared namespaces are supported (see API section for
more details)</li></ul><h4 id=difference-in-behavior-of-flags-when-compared-to-linux>Difference in behavior of flags when compared to Linux</h4><p>The behavior of the following kubelet flags is different on Windows nodes as described below:</p><ul><li><p><code>--kubelet-reserve</code>, <code>--system-reserve</code> , and <code>--eviction-hard</code> flags update
Node Allocatable</p></li><li><p>Eviction by using <code>--enforce-node-allocable</code> is not implemented.</p></li><li><p>Eviction by using <code>--eviction-hard</code> and <code>--eviction-soft</code> are not implemented.</p></li><li><p><code>MemoryPressure</code> Condition is not implemented.</p></li><li><p>There are no OOM eviction actions taken by the kubelet.</p></li><li><p>Kubelet running on the windows node does not have memory restrictions.
<code>--kubelet-reserve</code> and <code>--system-reserve</code> do not set limits on kubelet or
processes running on the host. This means kubelet or a process on the host
could cause memory resource starvation outside the node-allocatable and
scheduler</p></li><li><p>An additional flag to set the priority of the kubelet process is available
on the Windows nodes called <code>--windows-priorityclass</code>. This flag allows
kubelet process to get more CPU time slices when compared to other processes
running on the Windows host. More information on the allowable values and
their meaning is available at
<a href=https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class>Windows Priority Classes</a>.
In order for kubelet to always have enough CPU cycles it is recommended to set
this flag to <code>ABOVE_NORMAL_PRIORITY_CLASS</code> and above.</p></li></ul><h4 id=storage>Storage</h4><p>Windows has a layered filesystem driver to mount container layers and create a
copy filesystem based on NTFS. All file paths in the container are resolved
only within the context of that container.</p><ul><li><p>With Docker Volume mounts can only target a directory in the container, and
not an individual file. This limitation does not exist with CRI-containerD.</p></li><li><p>Volume mounts cannot project files or directories back to the host
filesystem</p></li><li><p>Read-only filesystems are not supported because write access is always
required for the Windows registry and SAM database. However, read-only
volumes are supported</p></li><li><p>Volume user-masks and permissions are not available. Because the SAM is not
shared between the host & container, there's no mapping between them. All
permissions are resolved within the context of the container</p></li></ul><p>As a result, the following storage functionality is not supported on Windows nodes:</p><ul><li>Volume subpath mounts. Only the entire volume can be mounted in a Windows container.</li><li>Subpath volume mounting for Secrets</li><li>Host mount projection</li><li>DefaultMode (due to UID/GID dependency)</li><li>Read-only root filesystem. Mapped volumes still support readOnly</li><li>Block device mapping</li><li>Memory as the storage medium</li><li>File system features like uui/guid, per-user Linux filesystem permissions</li><li>NFS based storage/volume support</li><li>Expanding the mounted volume (resizefs)</li></ul><h4 id=networking-limitations>Networking</h4><p>Windows Container Networking differs in some important ways from Linux
networking. The <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture>Microsoft documentation for Windows Container Networking</a>
contains additional details and background.</p><p>The Windows host networking service and virtual switch implement namespacing
and can create virtual NICs as needed for a pod or container. However, many
configurations such as DNS, routes, and metrics are stored in the Windows
registry database rather than /etc/... files as they are on Linux. The Windows
registry for the container is separate from that of the host, so concepts like
mapping /etc/resolv.conf from the host into a container don't have the same
effect they would on Linux. These must be configured using Windows APIs run in
the context of that container. Therefore CNI implementations need to call the
HNS instead of relying on file mappings to pass network details into the pod
or container.</p><p>The following networking functionality is not supported on Windows nodes</p><ul><li><p>Host networking mode is not available for Windows pods.</p></li><li><p>Local NodePort access from the node itself fails (works for other nodes or
external clients).</p></li><li><p>Accessing service VIPs from nodes will be available with a future release of
Windows Server.</p></li><li><p>A single service can only support up to 64 backend pods / unique destination IPs.</p></li><li><p>Overlay networking support in kube-proxy is a beta feature. In addition, it
requires <a href=https://support.microsoft.com/en-us/help/4482887/windows-10-update-kb4482887>KB4482887</a>
to be installed on Windows Server 2019.</p></li><li><p>Local Traffic Policy in non-DSR mode.</p></li><li><p>Windows containers connected to overlay networks do not support
communicating over the IPv6 stack. There is outstanding Windows platform
work required to enable this network driver to consume IPv6 addresses and
subsequent Kubernetes work in kubelet, kube-proxy, and CNI plugins.</p></li><li><p>Outbound communication using the ICMP protocol via the win-overlay,
win-bridge, and Azure-CNI plugin. Specifically, the Windows data plane
(<a href=https://www.microsoft.com/en-us/research/project/azure-virtual-filtering-platform/>VFP</a>)
doesn't support ICMP packet transpositions. This means:</p><ul><li><p>ICMP packets directed to destinations within the same network (e.g. pod to
pod communication via ping) work as expected and without any limitations</p></li><li><p>TCP/UDP packets work as expected and without any limitations</p></li><li><p>ICMP packets directed to pass through a remote network (e.g. pod to
external internet communication via ping) cannot be transposed and thus
will not be routed back to their source</p></li><li><p>Since TCP/UDP packets can still be transposed, one can substitute
<code>ping &lt;destination></code> with <code>curl &lt;destination></code> to be able to debug connectivity
to the outside world.</p></li></ul></li></ul><p>These features were added in Kubernetes v1.15:</p><ul><li><code>kubectl port-forward</code></li></ul><h5 id=cni-plugins>CNI Plugins</h5><ul><li><p>Windows reference network plugins <code>win-bridge</code> and <code>win-overlay</code> do not
currently implement <a href=https://github.com/containernetworking/cni/blob/master/SPEC.md>CNI spec</a>
v0.4.0 due to missing "CHECK" implementation.</p></li><li><p>The Flannel VXLAN CNI has the following limitations on Windows:</p><ol><li><p>Node-pod connectivity isn't possible by design. It's only possible for
local pods with Flannel v0.12.0 (or higher).</p></li><li><p>We are restricted to using VNI 4096 and UDP port 4789. The VNI limitation
is being worked on and will be overcome in a future release (open-source
flannel changes). See the official
<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>Flannel VXLAN</a>
backend docs for more details on these parameters.</p></li></ol></li></ul><h5 id=dns-limitations>DNS</h5><ul><li><p>ClusterFirstWithHostNet is not supported for DNS. Windows treats all names
with a '.' as a FQDN and skips PQDN resolution</p></li><li><p>On Linux, you have a DNS suffix list, which is used when trying to resolve
PQDNs. On Windows, we only have 1 DNS suffix, which is the DNS suffix
associated with that pod's namespace (mydns.svc.cluster.local for example).
Windows can resolve FQDNs and services or names resolvable with only that
suffix. For example, a pod spawned in the default namespace, will have the DNS
suffix <code>default.svc.cluster.local</code>. On a Windows pod, you can resolve both
<code>kubernetes.default.svc.cluster.local</code> and <code>kubernetes</code>, but not the
in-betweens, like <code>kubernetes.default</code> or <code>kubernetes.default.svc</code>.</p></li><li><p>On Windows, there are multiple DNS resolvers that can be used. As these come
with slightly different behaviors, using the <code>Resolve-DNSName</code> utility for
name query resolutions is recommended.</p></li></ul><h5 id=ipv6>IPv6</h5><p>Kubernetes on Windows does not support single-stack "IPv6-only" networking.
However,dual-stack IPv4/IPv6 networking for pods and nodes with single-family
services is supported.
See <a href=#ipv4ipv6-dual-stack>IPv4/IPv6 dual-stack networking</a> for more details.</p><h5 id=session-affinity>Session affinity</h5><p>Setting the maximum session sticky time for Windows services using
<code>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds</code> is not supported.</p><h5 id=security>Security</h5><p>Secrets are written in clear text on the node's volume (as compared to
tmpfs/in-memory on linux). This means customers have to do two things:</p><ol><li>Use file ACLs to secure the secrets file location</li><li>Use volume-level encryption using
<a href=https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server>BitLocker</a></li></ol><p><a href=/docs/tasks/configure-pod-container/configure-runasusername>RunAsUsername</a>
can be specified for Windows Pod's or Container's to execute the Container
processes as a node-default user. This is roughly equivalent to
<a href=/docs/concepts/policy/pod-security-policy/#users-and-groups>RunAsUser</a>.</p><p>Linux specific pod security context privileges such as SELinux, AppArmor,
Seccomp, Capabilities (POSIX Capabilities), and others are not supported.</p><p>In addition, as mentioned already, privileged containers are not supported on
Windows.</p><h4 id=api>API</h4><p>There are no differences in how most of the Kubernetes APIs work for Windows.
The subtleties around what's different come down to differences in the OS and
container runtime. In certain situations, some properties on workload APIs
such as Pod or Container were designed with an assumption that they are
implemented on Linux, failing to run on Windows.</p><p>At a high level, these OS concepts are different:</p><ul><li><p>Identity - Linux uses userID (UID) and groupID (GID) which are represented
as integer types. User and group names are not canonical - they are an alias
in <code>/etc/groups</code> or <code>/etc/passwd</code> back to UID+GID. Windows uses a larger
binary security identifier (SID) which is stored in the Windows Security
Access Manager (SAM) database. This database is not shared between the host
and containers, or between containers.</p></li><li><p>File permissions - Windows uses an access control list based on SIDs, rather
than a bitmask of permissions and UID+GID</p></li><li><p>File paths - convention on Windows is to use <code>\</code> instead of <code>/</code>. The Go IO
libraries accept both types of file path separators. However, when you're
setting a path or command line that's interpreted inside a container, <code>\</code> may
be needed.</p></li><li><p>Signals - Windows interactive apps handle termination differently, and can
implement one or more of these:</p><ul><li><p>A UI thread handles well-defined messages including <code>WM_CLOSE</code></p></li><li><p>Console apps handle ctrl-c or ctrl-break using a Control Handler</p></li><li><p>Services register a Service Control Handler function that can accept
<code>SERVICE_CONTROL_STOP</code> control codes</p></li></ul></li></ul><p>Exit Codes follow the same convention where 0 is success, nonzero is failure.
The specific error codes may differ across Windows and Linux. However, exit
codes passed from the Kubernetes components (kubelet, kube-proxy) are
unchanged.</p><h5 id=v1-container>V1.Container</h5><ul><li><p>V1.Container.ResourceRequirements.limits.cpu and
V1.Container.ResourceRequirements.limits.memory - Windows doesn't use hard
limits for CPU allocations. Instead, a share system is used. The existing
fields based on millicores are scaled into relative shares that are followed
by the Windows scheduler.
See <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/helpers_windows.go>kuberuntime/helpers_windows.go</a>,
and <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/resource-controls>resource controls in Microsoft docs</a></p><ul><li>Huge pages are not implemented in the Windows container runtime, and are
not available. They require
<a href=https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support>asserting a user privilege</a>
that's not configurable for containers.</li></ul></li><li><p>V1.Container.ResourceRequirements.requests.cpu and
V1.Container.ResourceRequirements.requests.memory - Requests are subtracted
from node available resources, so they can be used to avoid overprovisioning a
node. However, they cannot be used to guarantee resources in an
overprovisioned node. They should be applied to all containers as a best
practice if the operator wants to avoid overprovisioning entirely.</p></li><li><p>V1.Container.SecurityContext.allowPrivilegeEscalation - not possible on
Windows, none of the capabilities are hooked up</p></li><li><p>V1.Container.SecurityContext.Capabilities - POSIX capabilities are not
implemented on Windows</p></li><li><p>V1.Container.SecurityContext.privileged - Windows doesn't support privileged
containers</p></li><li><p>V1.Container.SecurityContext.procMount - Windows doesn't have a /proc filesystem</p></li><li><p>V1.Container.SecurityContext.readOnlyRootFilesystem - not possible on
Windows, write access is required for registry & system processes to run
inside the container</p></li><li><p>V1.Container.SecurityContext.runAsGroup - not possible on Windows, no GID support</p></li><li><p>V1.Container.SecurityContext.runAsNonRoot - Windows does not have a root
user. The closest equivalent is ContainerAdministrator which is an identity
that doesn't exist on the node.</p></li><li><p>V1.Container.SecurityContext.runAsUser - not possible on Windows, no UID
support as int.</p></li><li><p>V1.Container.SecurityContext.seLinuxOptions - not possible on Windows, no SELinux</p></li><li><p>V1.Container.terminationMessagePath - this has some limitations in that
Windows doesn't support mapping single files. The default value is
<code>/dev/termination-log</code>, which does work because it does not exist on Windows by
default.</p></li></ul><h5 id=v1-pod>V1.Pod</h5><ul><li><p>V1.Pod.hostIPC, v1.pod.hostpid - host namespace sharing is not possible on Windows</p></li><li><p>V1.Pod.hostNetwork - There is no Windows OS support to share the host network</p></li><li><p>V1.Pod.dnsPolicy - <code>ClusterFirstWithHostNet</code> is not supported because Host
Networking is not supported on Windows.</p></li><li><p>V1.Pod.podSecurityContext - see V1.PodSecurityContext below</p></li><li><p>V1.Pod.shareProcessNamespace - this is a beta feature, and depends on Linux
namespaces which are not implemented on Windows. Windows cannot share
process namespaces or the container's root filesystem. Only the network can be
shared.</p></li><li><p>V1.Pod.terminationGracePeriodSeconds - this is not fully implemented in
Docker on Windows, see:
<a href=https://github.com/moby/moby/issues/25982>reference</a>. The behavior today is
that the <code>ENTRYPOINT</code> process is sent <code>CTRL_SHUTDOWN_EVENT</code>, then Windows waits 5
seconds by default, and finally shuts down all processes using the normal
Windows shutdown behavior. The 5 second default is actually in the Windows
registry <a href=https://github.com/moby/moby/issues/25982#issuecomment-426441183>inside the container</a>,
so it can be overridden when the container is built.</p></li><li><p>V1.Pod.volumeDevices - this is a beta feature, and is not implemented on
Windows. Windows cannot attach raw block devices to pods.</p></li><li><p>V1.Pod.volumes - EmptyDir, Secret, ConfigMap, HostPath - all work and have
tests in TestGrid</p><ul><li>V1.emptyDirVolumeSource - the Node default medium is disk on Windows.
Memory is not supported, as Windows does not have a built-in RAM disk.</li></ul></li><li><p>V1.VolumeMount.mountPropagation - mount propagation is not supported on Windows.</p></li></ul><h5 id=v1-podsecuritycontext>V1.PodSecurityContext</h5><p>None of the PodSecurityContext fields work on Windows. They're listed here for
reference.</p><ul><li><p>V1.PodSecurityContext.SELinuxOptions - SELinux is not available on Windows</p></li><li><p>V1.PodSecurityContext.RunAsUser - provides a UID, not available on Windows</p></li><li><p>V1.PodSecurityContext.RunAsGroup - provides a GID, not available on Windows</p></li><li><p>V1.PodSecurityContext.RunAsNonRoot - Windows does not have a root user. The
closest equivalent is ContainerAdministrator which is an identity that
doesn't exist on the node.</p></li><li><p>V1.PodSecurityContext.SupplementalGroups - provides GID, not available on Windows</p></li><li><p>V1.PodSecurityContext.Sysctls - these are part of the Linux sysctl
interface. There's no equivalent on Windows.</p></li></ul><h4 id=operating-system-version-restrictions>Operating System Version Restrictions</h4><p>Windows has strict compatibility rules, where the host OS version must match
the container base image OS version. Only Windows containers with a container
operating system of Windows Server 2019 are supported. Hyper-V isolation of
containers, enabling some backward compatibility of Windows container image
versions, is planned for a future release.</p><h2 id=troubleshooting>Getting Help and Troubleshooting</h2><p>Your main source of help for troubleshooting your Kubernetes cluster should
start with this
<a href=/docs/tasks/debug-application-cluster/troubleshooting/>section</a>. Some
additional, Windows-specific troubleshooting help is included in this section.
Logs are an important element of troubleshooting issues in Kubernetes. Make
sure to include them any time you seek troubleshooting assistance from other
contributors. Follow the instructions in the SIG-Windows
<a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>contributing guide on gathering logs</a>.</p><ul><li><p>How do I know start.ps1 completed successfully?</p><p>You should see kubelet, kube-proxy, and (if you chose Flannel as your
networking solution) flanneld host-agent processes running on your node, with
running logs being displayed in separate PowerShell windows. In addition to
this, your Windows node should be listed as "Ready" in your Kubernetes
cluster.</p></li><li><p>Can I configure the Kubernetes node processes to run in the background as services?</p><p>Kubelet and kube-proxy are already configured to run as native Windows
Services, offering resiliency by re-starting the services automatically in the
event of failure (for example a process crash). You have two options for
configuring these node components as services.</p><ul><li><p>As native Windows Services</p><p>Kubelet & kube-proxy can be run as native Windows Services using <code>sc.exe</code>.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># Create the services for kubelet and kube-proxy in two separate commands</span>
sc.exe create &lt;component_name&gt; binPath= <span style=color:#b44>&#34;&lt;path_to_binary&gt; --service &lt;other_args&gt;&#34;</span>

<span style=color:#080;font-style:italic># Please note that if the arguments contain spaces, they must be escaped.</span>
sc.exe create kubelet binPath= <span style=color:#b44>&#34;C:\kubelet.exe --service --hostname-override &#39;minion&#39; &lt;other_args&gt;&#34;</span>

<span style=color:#080;font-style:italic># Start the services</span>
<span style=color:#a2f>Start-Service</span> kubelet
<span style=color:#a2f>Start-Service</span> kube-proxy

<span style=color:#080;font-style:italic># Stop the service</span>
<span style=color:#a2f>Stop-Service</span> kubelet (-Force)
<span style=color:#a2f>Stop-Service</span> kube-proxy (-Force)

<span style=color:#080;font-style:italic># Query the service status</span>
<span style=color:#a2f>Get-Service</span> kubelet
<span style=color:#a2f>Get-Service</span> kube-proxy
</code></pre></div></li><li><p>Using nssm.exe</p><p>You can also always use alternative service managers like
<a href=https://nssm.cc/><code>nssm.exe</code></a> to run these processes (flanneld, kubelet &
kube-proxy) in the background for you. You can use this
<a href=https://github.com/Microsoft/SDN/tree/master/Kubernetes/flannel/register-svc.ps1>sample script</a>,
leveraging <code>nssm.exe</code> to register kubelet, kube-proxy, and <code>flanneld.exe</code>
to run as Windows services in the background.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>register-svc</span>.ps1 -NetworkMode &lt;Network mode&gt; -ManagementIP &lt;Windows Node IP&gt; -ClusterCIDR &lt;Cluster subnet&gt; -KubeDnsServiceIP &lt;Kube-dns Service IP&gt; -LogDir &lt;Directory to place logs&gt;
</code></pre></div><p>The parameters are explained below:</p><ul><li><code>NetworkMode</code>: The network mode l2bridge (flannel host-gw, also the
default value) or overlay (flannel vxlan) chosen as a network solution</li><li><code>ManagementIP</code>: The IP address assigned to the Windows node. You can use
<code>ipconfig</code> to find this.</li><li><code>ClusterCIDR</code>: The cluster subnet range. (Default: 10.244.0.0/16)</li><li><code>KubeDnsServiceIP</code>: The Kubernetes DNS service IP. (Default: 10.96.0.10)</li><li><code>LogDir</code>: The directory where kubelet and kube-proxy logs are redirected
into their respective output files. (Default value C:\k)</li></ul><p>If the above referenced script is not suitable, you can manually configure
<code>nssm.exe</code> using the following examples.</p><p>Register flanneld.exe:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>nssm install flanneld C:\flannel\flanneld.exe
nssm <span style=color:#a2f>set </span>flanneld AppParameters --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;ManagementIP&gt; --ip-masq=1 --kube-subnet-mgr=1
nssm <span style=color:#a2f>set </span>flanneld AppEnvironmentExtra NODE_NAME=&lt;hostname&gt;
nssm <span style=color:#a2f>set </span>flanneld AppDirectory C:\flannel
nssm <span style=color:#a2f>start </span>flanneld
</code></pre></div><p>Register kubelet.exe:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#080;font-style:italic># Microsoft releases the pause infrastructure container at mcr.microsoft.com/oss/kubernetes/pause:3.4.1</span>
nssm install kubelet C:\k\kubelet.exe
nssm <span style=color:#a2f>set </span>kubelet AppParameters --hostname-override=&lt;hostname&gt; --v=6 --pod-infra-container-image=mcr.microsoft.com/oss/kubernetes/pause<span>:</span>3.4.1 --resolv-conf=<span style=color:#b44>&#34;&#34;</span> --allow-privileged=true --enable-debugging-handlers --cluster-dns=&lt;DNS-service-IP&gt; --cluster-domain=cluster.local --kubeconfig=c:\k\config --hairpin-mode=promiscuous-bridge --image-pull-progress-deadline=20m --cgroups-per-qos=false  --log-dir=&lt;log directory&gt; --logtostderr=false --enforce-node-allocatable=<span style=color:#b44>&#34;&#34;</span> --network-plugin=cni --cni-bin-dir=c:\k\cni --cni-conf-dir=c:\k\cni\config
nssm <span style=color:#a2f>set </span>kubelet AppDirectory C:\k
nssm <span style=color:#a2f>start </span>kubelet
</code></pre></div><p>Register kube-proxy.exe (l2bridge / host-gw):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>nssm install kube-proxy C:\k\kube-proxy.exe
nssm <span style=color:#a2f>set </span>kube-proxy AppDirectory c:\k
nssm <span style=color:#a2f>set </span>kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --hostname-override=&lt;hostname&gt;--kubeconfig=c:\k\config --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
nssm.exe <span style=color:#a2f>set </span>kube-proxy AppEnvironmentExtra KUBE_NETWORK=cbr0
nssm <span style=color:#a2f>set </span>kube-proxy DependOnService kubelet
nssm <span style=color:#a2f>start </span>kube-proxy
</code></pre></div><p>Register kube-proxy.exe (overlay / vxlan):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>nssm install kube-proxy C:\k\kube-proxy.exe
nssm <span style=color:#a2f>set </span>kube-proxy AppDirectory c:\k
nssm <span style=color:#a2f>set </span>kube-proxy AppParameters --v=4 --proxy-mode=kernelspace --feature-gates=<span style=color:#b44>&#34;WinOverlay=true&#34;</span> --hostname-override=&lt;hostname&gt; --kubeconfig=c:\k\config --network-name=vxlan0 --source-vip=&lt;source-vip&gt; --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
nssm <span style=color:#a2f>set </span>kube-proxy DependOnService kubelet
nssm <span style=color:#a2f>start </span>kube-proxy
</code></pre></div><p>For initial troubleshooting, you can use the following flags in
<a href=https://nssm.cc/><code>nssm.exe</code></a> to redirect stdout and stderr to a output file:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>nssm <span style=color:#a2f>set </span>&lt;Service Name&gt; AppStdout C:\k\mysvc.log
nssm <span style=color:#a2f>set </span>&lt;Service Name&gt; AppStderr C:\k\mysvc.log
</code></pre></div><p>For additional details, see official <a href=https://nssm.cc/usage>nssm usage</a> docs.</p></li></ul></li><li><p>My Windows Pods do not have network connectivity</p><p>If you are using virtual machines, ensure that MAC spoofing is enabled on
all the VM network adapter(s).</p></li><li><p>My Windows Pods cannot ping external resources</p><p>Windows Pods do not have outbound rules programmed for the ICMP protocol
today. However, TCP/UDP is supported. When trying to demonstrate connectivity
to resources outside of the cluster, please substitute <code>ping &lt;IP></code> with
corresponding <code>curl &lt;IP></code> commands.</p><p>If you are still facing problems, most likely your network configuration in
<a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf>cni.conf</a>
deserves some extra attention. You can always edit this static file. The
configuration update will apply to any newly created Kubernetes resources.</p><p>One of the Kubernetes networking requirements (see
<a href=/docs/concepts/cluster-administration/networking/>Kubernetes network model</a>)
is for cluster communication to occur without NAT internally. To honor this
requirement, there is an
<a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf#L20>ExceptionList</a>
for all the communication where we do not want outbound NAT to occur. However,
this also means that you need to exclude the external IP you are trying to
query from the ExceptionList. Only then will the traffic originating from your
Windows pods be SNAT'ed correctly to receive a response from the outside
world. In this regard, your ExceptionList in <code>cni.conf</code> should look as
follows:</p><pre><code class=language-conf data-lang=conf>&quot;ExceptionList&quot;: [
    &quot;10.244.0.0/16&quot;,  # Cluster subnet
    &quot;10.96.0.0/12&quot;,   # Service subnet
    &quot;10.127.130.0/24&quot; # Management (host) subnet
]
</code></pre></li><li><p>My Windows node cannot access NodePort service</p><p>Local NodePort access from the node itself fails. This is a known
limitation. NodePort access works from other nodes or external clients.</p></li><li><p>vNICs and HNS endpoints of containers are being deleted</p><p>This issue can be caused when the <code>hostname-override</code> parameter is not
passed to
<a href=/docs/reference/command-line-tools-reference/kube-proxy/>kube-proxy</a>.
To resolve it, users need to pass the hostname to kube-proxy as follows:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell>C:\k\kube-proxy.exe --hostname-override=$(hostname)
</code></pre></div></li><li><p>With flannel my nodes are having issues after rejoining a cluster</p><p>Whenever a previously deleted node is being re-joined to the cluster,
flannelD tries to assign a new pod subnet to the node. Users should remove the
old pod subnet configuration files in the following paths:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Remove-Item</span> C:\k\SourceVip.json
<span style=color:#a2f>Remove-Item</span> C:\k\SourceVipRequest.json
</code></pre></div></li><li><p>After launching <code>start.ps1</code>, flanneld is stuck in "Waiting for the Network
to be created"</p><p>There are numerous reports of this
<a href=https://github.com/coreos/flannel/issues/1066>issue</a>; most likely it is a
timing issue for when the management IP of the flannel network is set. A
workaround is to relaunch start.ps1 or relaunch it manually as follows:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>PS </span>C:&gt; <span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;NODE_NAME&#34;</span>, <span style=color:#b44>&#34;&lt;Windows_Worker_Hostname&gt;&#34;</span>)
<span style=color:#a2f>PS </span>C:&gt; C:\flannel\flanneld.exe --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;Windows_Worker_Node_IP&gt; --ip-masq=1 --kube-subnet-mgr=1
</code></pre></div></li><li><p>My Windows Pods cannot launch because of missing <code>/run/flannel/subnet.env</code></p><p>This indicates that Flannel didn't launch correctly. You can either try to
restart flanneld.exe or you can copy the files over manually from
<code>/run/flannel/subnet.env</code> on the Kubernetes master to
<code>C:\run\flannel\subnet.env</code> on the Windows worker node and modify the
<code>FLANNEL_SUBNET</code> row to a different number. For example, if node subnet
10.244.4.1/24 is desired:</p><pre><code class=language-none data-lang=none>FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.4.1/24
FLANNEL_MTU=1500
FLANNEL_IPMASQ=true
</code></pre></li><li><p>My Windows node cannot access my services using the service IP</p><p>This is a known limitation of the current networking stack on Windows.
Windows Pods are able to access the service IP however.</p></li><li><p>No network adapter is found when starting kubelet</p><p>The Windows networking stack needs a virtual adapter for Kubernetes
networking to work. If the following commands return no results (in an admin
shell), virtual network creation — a necessary prerequisite for Kubelet to
work — has failed:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=color:#a2f>Get-HnsNetwork</span> | ? Name <span style=color:#666>-ieq</span> <span style=color:#b44>&#34;cbr0&#34;</span>
<span style=color:#a2f>Get-NetAdapter</span> | ? Name <span style=color:#666>-Like</span> <span style=color:#b44>&#34;vEthernet (Ethernet*&#34;</span>
</code></pre></div><p>Often it is worthwhile to modify the
<a href=https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1#L7>InterfaceName</a>
parameter of the start.ps1 script, in cases where the host's network adapter
isn't "Ethernet". Otherwise, consult the output of the <code>start-kubelet.ps1</code>
script to see if there are errors during virtual network creation.</p></li><li><p>My Pods are stuck at "Container Creating" or restarting over and over</p><p>Check that your pause image is compatible with your OS version. The
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/deploying-resources>instructions</a>
assume that both the OS and the containers are version 1803. If you have a
later version of Windows, such as an Insider build, you need to adjust the
images accordingly. Please refer to the Microsoft's
<a href=https://hub.docker.com/u/microsoft/>Docker repository</a> for images.
Regardless, both the pause image Dockerfile and the sample service expect
the image to be tagged as :latest.</p></li><li><p>DNS resolution is not properly working</p><p>Check the <a href=#dns-limitations>DNS limitations for Windows</a>.</p></li><li><p><code>kubectl port-forward</code> fails with "unable to do port forwarding: wincat not found"</p><p>This was implemented in Kubernetes 1.15 by including wincat.exe in the
pause infrastructure container <code>mcr.microsoft.com/oss/kubernetes/pause:3.4.1</code>.
Be sure to use these versions or newer ones. If you would like to build your
own pause infrastructure container be sure to include
<a href=https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/cmd/wincat>wincat</a>.</p></li><li><p>My Kubernetes installation is failing because my Windows Server node is
behind a proxy</p><p>If you are behind a proxy, the following PowerShell environment variables
must be defined:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-PowerShell data-lang=PowerShell><span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTP_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:80/&#34;</span>, <span style=color:#800>[EnvironmentVariableTarget]</span>::Machine)
<span style=color:#800>[Environment]</span>::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTPS_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:443/&#34;</span>, <span style=color:#800>[EnvironmentVariableTarget]</span>::Machine)
</code></pre></div></li><li><p>What is a <code>pause</code> container?</p><p>In a Kubernetes Pod, an infrastructure or "pause" container is first created
to host the container endpoint. Containers that belong to the same pod,
including infrastructure and worker containers, share a common network
namespace and endpoint (same IP and port space). Pause containers are needed
to accommodate worker containers crashing or restarting without losing any of
the networking configuration.</p><p>The "pause" (infrastructure) image is hosted on Microsoft Container Registry
(MCR). You can access it using <code>mcr.microsoft.com/oss/kubernetes/pause:3.4.1</code>.
For more details, see the
<a href=https://github.com/kubernetes-sigs/windows-testing/blob/master/images/pause/Dockerfile>DOCKERFILE</a>.</p></li></ul><h3 id=further-investigation>Further investigation</h3><p>If these steps don't resolve your problem, you can get help running Windows
containers on Windows nodes in Kubernetes through:</p><ul><li><p>StackOverflow <a href=https://stackoverflow.com/questions/tagged/windows-server-container>Windows Server Container</a> topic</p></li><li><p>Kubernetes Official Forum <a href=https://discuss.kubernetes.io/>discuss.kubernetes.io</a></p></li><li><p>Kubernetes Slack <a href=https://kubernetes.slack.com/messages/sig-windows>#SIG-Windows Channel</a></p></li></ul><h2 id=reporting-issues-and-feature-requests>Reporting Issues and Feature Requests</h2><p>If you have what looks like a bug, or you would like to make a feature
request, please use the
<a href=https://github.com/kubernetes/kubernetes/issues>GitHub issue tracking system</a>.
You can open issues on
<a href=https://github.com/kubernetes/kubernetes/issues/new/choose>GitHub</a> and
assign them to SIG-Windows. You should first search the list of issues in case
it was reported previously and comment with your experience on the issue and
add additional logs. SIG-Windows Slack is also a great avenue to get some
initial support and troubleshooting ideas prior to creating a ticket.</p><p>If filing a bug, please include detailed information about how to reproduce
the problem, such as:</p><ul><li>Kubernetes version: kubectl version</li><li>Environment details: Cloud provider, OS distro, networking choice and
configuration, and Docker version</li><li>Detailed steps to reproduce the problem</li><li><a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>Relevant logs</a></li><li>Tag the issue sig/windows by commenting on the issue with <code>/sig windows</code> to
bring it to a SIG-Windows member's attention</li></ul><h2 id=what-s-next>What's next</h2><p>We have a lot of features in our roadmap. An abbreviated high level list is
included below, but we encourage you to view our
<a href=https://github.com/orgs/kubernetes/projects/8>roadmap project</a> and help us make
Windows support better by
<a href=https://github.com/kubernetes/community/blob/master/sig-windows/>contributing</a>.</p><h3 id=hyper-v-isolation>Hyper-V isolation</h3><p>Hyper-V isolation is required to enable the following use cases for Windows
containers in Kubernetes:</p><ul><li><p>Hypervisor-based isolation between pods for additional security</p></li><li><p>Backwards compatibility allowing a node to run a newer Windows Server
version without requiring containers to be rebuilt</p></li><li><p>Specific CPU/NUMA settings for a pod</p></li><li><p>Memory isolation and reservations</p></li></ul><p>Hyper-V isolation support will be added in a later release and will require
CRI-Containerd.</p><h3 id=deployment-with-kubeadm-and-cluster-api>Deployment with kubeadm and cluster API</h3><p>Kubeadm is becoming the de facto standard for users to deploy a Kubernetes
cluster. Windows node support in kubeadm is currently a work-in-progress but a
guide is available
<a href=/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>here</a>. We are
also making investments in cluster API to ensure Windows nodes are properly
provisioned.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3a51e66c5de55f9093a8dc55742006d3>2 - Guide for scheduling Windows containers in Kubernetes</h1><p>Windows applications constitute a large portion of the services and applications that run in many organizations.
This guide walks you through the steps to configure and deploy a Windows container in Kubernetes.</p><h2 id=objectives>Objectives</h2><ul><li>Configure an example deployment to run Windows containers on the Windows node</li><li>(Optional) Configure an Active Directory Identity for your Pod using Group Managed Service Accounts (GMSA)</li></ul><h2 id=before-you-begin>Before you begin</h2><ul><li>Create a Kubernetes cluster that includes a
control plane and a <a href=/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>worker node running Windows Server</a></li><li>It is important to note that creating and deploying services and workloads on Kubernetes
behaves in much the same way for Linux and Windows containers.
<a href=/docs/reference/kubectl/overview/>Kubectl commands</a> to interface with the cluster are identical.
The example in the section below is provided to jumpstart your experience with Windows containers.</li></ul><h2 id=getting-started-deploying-a-windows-container>Getting Started: Deploying a Windows container</h2><p>To deploy a Windows container on Kubernetes, you must first create an example application.
The example YAML file below creates a simple webserver application.
Create a service spec named <code>win-webserver.yaml</code> with the contents below:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># the port that this service should serve on</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>NodePort<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windowswebserver<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- powershell.exe<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- -command<span style=color:#bbb>
</span><span style=color:#bbb>        </span>- <span style=color:#b44>&#34;&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add(&#39;http://*:80/&#39;) ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host(&#39;Listening at http://*:80/&#39;) ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host &#39;&#39; ;Write-Host(&#39;&gt; {0}&#39; -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header=&#39;&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;&#39; ;$$callerCountsString=&#39;&#39; ;$$callerCounts.Keys | % { $$callerCountsString+=&#39;&lt;p&gt;IP {0} callerCount {1} &#39; -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=&#39;&lt;/body&gt;&lt;/html&gt;&#39; ;$$content=&#39;{0}{1}{2}&#39; -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(&#39;&lt; {0}&#39; -f $$responseStatus)  } ; &#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span></code></pre></div><blockquote class="note callout"><div><strong>Note:</strong> Port mapping is also supported, but for simplicity in this example
the container port 80 is exposed directly to the service.</div></blockquote><ol><li><p>Check that all nodes are healthy:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get nodes
</code></pre></div></li><li><p>Deploy the service and watch for pod updates:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f win-webserver.yaml
kubectl get pods -o wide -w
</code></pre></div><p>When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.</p></li><li><p>Check that the deployment succeeded. To verify:</p><ul><li>Two containers per pod on the Windows node, use <code>docker ps</code></li><li>Two pods listed from the Linux control plane node, use <code>kubectl get pods</code></li><li>Node-to-pod communication across the network, <code>curl</code> port 80 of your pod IPs from the Linux control plane node
to check for a web server response</li><li>Pod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node)
using docker exec or kubectl exec</li><li>Service-to-pod communication, <code>curl</code> the virtual service IP (seen under <code>kubectl get services</code>)
from the Linux control plane node and from individual pods</li><li>Service discovery, <code>curl</code> the service name with the Kubernetes <a href=/docs/concepts/services-networking/dns-pod-service/#services>default DNS suffix</a></li><li>Inbound connectivity, <code>curl</code> the NodePort from the Linux control plane node or machines outside of the cluster</li><li>Outbound connectivity, <code>curl</code> external IPs from inside the pod using kubectl exec</li></ul></li></ol><blockquote class="note callout"><div><strong>Note:</strong> Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack.
Only Windows pods are able to access service IPs.</div></blockquote><h2 id=observability>Observability</h2><h3 id=capturing-logs-from-workloads>Capturing logs from workloads</h3><p>Logs are an important element of observability; they enable users to gain insights
into the operational aspect of workloads and are a key ingredient to troubleshooting issues.
Because Windows containers and workloads inside Windows containers behave differently from Linux containers,
users had a hard time collecting logs, limiting operational visibility.
Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows)
or push entries to the application event log.
<a href=https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor>LogMonitor</a>, an open source tool by Microsoft,
is the recommended way to monitor configured log sources inside a Windows container.
LogMonitor supports monitoring event logs, ETW providers, and custom application logs,
piping them to STDOUT for consumption by <code>kubectl logs &lt;pod></code>.</p><p>Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files
to all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.</p><h2 id=using-configurable-container-usernames>Using configurable Container usernames</h2><p>Starting with Kubernetes v1.16, Windows containers can be configured to run their entrypoints and processes
with different usernames than the image defaults.
The way this is achieved is a bit different from the way it is done for Linux containers.
Learn more about it <a href=/docs/tasks/configure-pod-container/configure-runasusername/>here</a>.</p><h2 id=managing-workload-identity-with-group-managed-service-accounts>Managing Workload Identity with Group Managed Service Accounts</h2><p>Starting with Kubernetes v1.14, Windows container workloads can be configured to use Group Managed Service Accounts (GMSA).
Group Managed Service Accounts are a specific type of Active Directory account that provides automatic password management,
simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.
Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA.
Learn more about configuring and using GMSA for Windows containers <a href=/docs/tasks/configure-pod-container/configure-gmsa/>here</a>.</p><h2 id=taints-and-tolerations>Taints and Tolerations</h2><p>Users today need to use some combination of taints and node selectors in order to
keep Linux and Windows workloads on their respective OS-specific nodes.
This likely imposes a burden only on Windows users. The recommended approach is outlined below,
with one of its main goals being that this approach should not break compatibility for existing Linux workloads.</p><h3 id=ensuring-os-specific-workloads-land-on-the-appropriate-container-host>Ensuring OS-specific workloads land on the appropriate container host</h3><p>Users can ensure Windows containers can be scheduled on the appropriate host using Taints and Tolerations.
All Kubernetes nodes today have the following default labels:</p><ul><li>kubernetes.io/os = [windows|linux]</li><li>kubernetes.io/arch = [amd64|arm64|...]</li></ul><p>If a Pod specification does not specify a nodeSelector like <code>"kubernetes.io/os": windows</code>,
it is possible the Pod can be scheduled on any host, Windows or Linux.
This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux.
The best practice is to use a nodeSelector.</p><p>However, we understand that in many cases users have a pre-existing large number of deployments for Linux containers,
as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with Operators.
In those situations, you may be hesitant to make the configuration change to add nodeSelectors.
The alternative is to use Taints. Because the kubelet can set Taints during registration,
it could easily be modified to automatically add a taint when running on Windows only.</p><p>For example: <code>--register-with-taints='os=windows:NoSchedule'</code></p><p>By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods).
In order for a Windows Pod to be scheduled on a Windows node,
it would need both the nodeSelector and the appropriate matching toleration to choose Windows.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;os&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></code></pre></div><h3 id=handling-multiple-windows-versions-in-the-same-cluster>Handling multiple Windows versions in the same cluster</h3><p>The Windows Server version used by each pod must match that of the node. If you want to use multiple Windows
Server versions in the same cluster, then you should set additional node labels and nodeSelectors.</p><p>Kubernetes 1.17 automatically adds a new label <code>node.kubernetes.io/windows-build</code> to simplify this.
If you're running an older version, then it's recommended to add this label manually to Windows nodes.</p><p>This label reflects the Windows major, minor, and build number that need to match for compatibility.
Here are values used today for each Windows Server version.</p><table><thead><tr><th>Product Name</th><th>Build Number(s)</th></tr></thead><tbody><tr><td>Windows Server 2019</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1809</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1903</td><td>10.0.18362</td></tr></tbody></table><h3 id=simplifying-with-runtimeclass>Simplifying with RuntimeClass</h3><p><a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>RuntimeClass</a> can be used to simplify the process of using taints and tolerations.
A cluster administrator can create a <code>RuntimeClass</code> object which is used to encapsulate these taints and tolerations.</p><ol><li>Save this file to <code>runtimeClasses.yml</code>. It includes the appropriate <code>nodeSelector</code>
for the Windows OS, architecture, and version.</li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;docker&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduling</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;windows&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/arch</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;amd64&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>os<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Equal<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span></code></pre></div><ol><li>Run <code>kubectl create -f runtimeClasses.yml</code> using as a cluster administrator</li><li>Add <code>runtimeClassName: windows-2019</code> as appropriate to Pod specs</li></ol><p>For example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>800Mi<span style=color:#bbb>
</span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>.1<span style=color:#bbb>
</span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>300Mi<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb> </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></code></pre></div></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/popper-1.14.3.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=/js/bootstrap-4.3.1.min.js integrity=sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM crossorigin=anonymous></script><script src=/js/main.min.40616251a9b6e4b689e7769be0340661efa4d7ebb73f957404e963e135b4ed52.js integrity="sha256-QGFiUam25LaJ53ab4DQGYe+k1+u3P5V0BOlj4TW07VI=" crossorigin=anonymous></script></body></html>