<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Roorkee robots, releases and racing: the Kubernetes 1.21 release interview</title><link>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</link><pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>With Kubernetes 1.22 due out next week, now is a great time to look back on 1.21. The release team for that version was led by &lt;a href="https://twitter.com/theonlynabarun">Nabarun Pal&lt;/a> from VMware.&lt;/p>
&lt;p>Back in April I &lt;a href="https://kubernetespodcast.com/episode/146-kubernetes-1.21/">interviewed Nabarun&lt;/a> on the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a>; the latest in a series of release lead conversations that started back with 1.11, not long after the show started back in 2018.&lt;/p>
&lt;p>In these interviews we learn a little about the release, but also about the process behind it, and the story behind the person chosen to lead it. Getting to know a community member is my favourite part of the show each week, and so I encourage you to &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe wherever you get your podcasts&lt;/a>. With a release coming next week, you can probably guess what our next topic will be!&lt;/p>
&lt;p>&lt;em>This transcript has been edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: You have a Bachelor of Technology in Metallurgical and Materials Engineering. How are we doing at turning lead into gold?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Well, last I checked, we have yet to find the philosopher's stone!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the more important parts of the process?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We're not doing that well in terms of getting alchemists up and running. There is some improvement in nuclear technology, where you can turn lead into gold, but I would guess buying gold would be much more efficient.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Or Bitcoin? It depends what you want to do with the gold.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yeah, seeing the increasing prices of Bitcoin, you'd probably prefer to bet on that. But, don't take this as a suggestion. I'm not a registered investment advisor, and I don't give investment advice!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But you are, of course, a trained materials engineer. How did you get into that line of education?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We had a graded and equated exam structure, where you sit a single exam, and then based on your performance in that exam, you can try any of the universities which take those scores into account. I went to the Indian Institute of Technology, Roorkee.&lt;/p>
&lt;p>Materials engineering interested me a lot. I had a passion for computer science since childhood, but I also liked material science, so I wanted to explore that field. I did a lot of exploration around material science and metallurgy in my freshman and sophomore years, but then computing, since it was a passion, crept into the picture.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's dig in there a little bit. What did computing look like during your childhood?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It was a very interesting journey. I started exploring computers back when I was seven or eight. For my first programming language, if you call it a programming language, I explored LOGO.&lt;/p>
&lt;p>You have a turtle on the screen, and you issue commands to it, like move forward or rotate or pen up or pen down. You basically draw geometric figures. I could visually see how I could draw a square and how I could draw a triangle. It was an interesting journey after that. I learned BASIC, then went to some amount of HTML, JavaScript.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's interesting to me because Logo and BASIC were probably my first two programming languages, but I think there was probably quite a gap in terms of when HTML became a thing after those two! Did your love of computing always lead you down the path towards programming, or were you interested as a child in using computers for games or application software? What led you specifically into programming?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Programming came in late. Not just in computing, but in life, I'm curious with things. When my parents got me my first computer, I was curious. I was like, &amp;quot;how does this operating system work?&amp;quot; What even is running it? Using a television and using a computer is a different experience, but usability is kind of the same thing. The HCI device for a television is a remote, whereas with a computer, I had a keyboard and a mouse. I used to tinker with the box and reinstall operating systems.&lt;/p>
&lt;p>We used to get magazines back then. They used to bundle OpenSuse or Debian, and I used to install them. It was an interesting experience, 15 years back, how Linux used to be. I have been a tinkerer all around, and that's what eventually led me to programming.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: With an interest in both the physical and ethereal aspects of technology, you did a lot of robotics challenges during university. That's something that I am not surprised to hear from someone who has a background in Logo, to be honest. There's Mindstorms, and a lot of other technology that is based around robotics that a lot of LOGO people got into. How was that something that came about for you?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: When I joined my university, apart from studying materials, one of the things they used to really encourage was to get involved in a lot of extracurricular activities. One which interested me was robotics. I joined &lt;a href="https://github.com/marsiitr">my college robotics team&lt;/a> and participated in a lot of challenges.&lt;/p>
&lt;p>Predominantly, we used to participate in this competition called &lt;a href="https://en.wikipedia.org/wiki/ABU_Robocon">ABU Robocon&lt;/a>, which is an event conducted by the Asia-Pacific Broadcasting Union. What they used to do was, every year, one of the participating countries in the contest would provide a problem statement. For example, one year, they asked us to build a badminton-playing robot. They asked us to build a rugby playing robot or a Frisbee thrower, and there are some interesting problem statements around the challenge: you can't do this. You can't do that. Weight has to be like this. Dimensions have to be like that.&lt;/p>
&lt;p>I got involved in that, and most of my time at university, I used to spend there. Material science became kind of a backburner for me, and my hobby became my full time thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And you were not only involved there in terms of the project and contributions to it, but you got involved as a secretary of the team, effectively, doing a lot of the organization, which is a thread that will come up as we speak about Kubernetes.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Over the course of time, when I gained more knowledge into how the team works, it became very natural that I graduated up the ladder and then managed juniors. I became the joint secretary of the robotics club in our college. This was more of a broad, engaging role in evangelizing robotics at the university, to promote events, to help students to see the value in learning robotics - what you gain out of that mechanically or electronically, or how do you develop your logic by programming robots.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Your first job after graduation was working at a company called Algoshelf, but you were also an intern there while you were at school?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Algoshelf was known as Rorodata when I joined them as an intern. This was also an interesting opportunity for me in the sense that I was always interested in writing programs which people would use. One of the things that I did there was build an open source Function as a Service framework, if I may call it that - it was mostly turning Python functions into web servers without even writing any code. The interesting bit there was that it was targeted toward data scientists, and not towards programmers. We had to understand the pain of data scientists, that they had to learn a lot of programming in order to even deploy their machine learning models, and we wanted to solve that problem.&lt;/p>
&lt;p>They offered me a job after my internship, and I kept on working for them after I graduated from university. There, I got introduced to Kubernetes, so we pivoted into a product structure where the very same thing I told you, the Functions as a Service thing, could be deployed in Kubernetes. I was exploring Kubernetes to use it as a scalable platform. Instead of managing pets, we wanted to manage cattle, as in, we wanted to have a very highly distributed architecture.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Not actual cattle. I've been to India. There are a lot of cows around.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yeah, not actual cattle. That is a bit tough.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When Algoshelf we're looking at picking up Kubernetes, what was the evaluation process like? Were you looking at other tools at the time? Or had enough time passed that Kubernetes was clearly the platform that everyone was going to use?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Algoshelf was a natural evolution. Before Kubernetes, we used to deploy everything on a single big AWS server, using systemd. Everything was a systemd service, and everything was deployed using Fabric. Fabric is a Python package which essentially is like Ansible, but much leaner, as it does not have all the shims and things that Ansible has.&lt;/p>
&lt;p>Then we asked &amp;quot;what if we need to scale out to different machines?&amp;quot; Kubernetes was in the hype. We hopped onto the hype train to see whether Kubernetes was worth it for us. And that's where my journey started, exploring the ecosystem, exploring the community. How can we improve the community in essence?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A couple of times now you've mentioned as you've grown in a role, becoming part of the organization and the arranging of the group. You've talked about working in Python. You had submitted some talks to Pycon India. And I understand you're now a tech lead for that conference. What does the tech community look like in India and how do you describe your involvement in it?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: My involvement with the community began when I was at university. When I was working as an intern at Algoshelf, I was introduced to this-- I never knew about PyCon India, or tech conferences in general.&lt;/p>
&lt;p>The person that I was working with just asked me, like hey, did you submit a talk to PyCon India? It's very useful, the library that we were making. So I &lt;a href="https://www.nabarun.in/talk/2017/pyconindia/#1">submitted a talk&lt;/a> to PyCon India in 2017. Eventually the talk got selected. That was not my first speaking opportunity, it was my second. I also spoke at PyData Delhi on a similar thing that I worked on in my internship.&lt;/p>
&lt;p>It has been a journey since then. I talked about the same thing at FOSSASIA Summit in Singapore, and got really involved with the Python community because it was what I used to work on back then.&lt;/p>
&lt;p>After giving all those talks at conferences, I got also introduced to this amazing group called &lt;a href="https://dgplug.org/">dgplug&lt;/a>, which is an acronym for the Durgapur Linux Users Group. It is a group started in-- I don't remember the exact year, but it was around 12 to 13 years back, by someone called Kushal Das, with the ideology of &lt;a href="https://foss.training/">training students into being better open source contributors&lt;/a>.&lt;/p>
&lt;p>I liked the idea and got involved with in teaching last year. It is not limited to students. Professionals can also join in. It's about making anyone better at upstream contributions, making things sustainable. I started training people on Vim, on how to use text editors. so they are more efficient and productive. In general life, text editors are a really good tool.&lt;/p>
&lt;p>The other thing was the shell. How do you navigate around the Linux shell and command line? That has been a fun experience.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's very interesting to think about that, because my own involvement with a Linux User Group was probably around the year 2000. And back then we were teaching people how to install things-- Linux on CD was kinda new at that point in time. There was a lot more of, what is this new thing and how do we get involved? When the internet took off around that time, all of that stuff moved online - you no longer needed to go meet a group of people in a room to talk about Linux. And I haven't really given much thought to the concept of a LUG since then, but it's great to see it having turned into something that's now about contributing, rather than just about how you get things going for yourself.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Exactly. So as I mentioned earlier, my journey into Linux was installing SUSE from DVDs that came bundled with magazines. Back then it was a pain installing things because you did not get any instructions. There has certainly been a paradigm shift now. People are more open to reading instructions online, downloading ISOs, and then just installing them. So we really don't need to do that as part of LUGs.&lt;/p>
&lt;p>We have shifted more towards enabling people to contribute to whichever project that they use. For example, if you're using Fedora, contribute to Fedora; make things better. It's just about giving back to the community in any way possible.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You're also involved in the &lt;a href="https://www.meetup.com/Bangalore-Kubernetes-Meetup/">Kubernetes Bangalore meetup group&lt;/a>. Does that group have a similar mentality?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: The Kubernetes Bangalore meetup group is essentially focused towards spreading the knowledge of Kubernetes and the aligned products in the ecosystem, whatever there is in the Cloud Native Landscape, in various ways. For example, to evangelize about using them in your company or how people use them in existing ways.&lt;/p>
&lt;p>So a few months back in February, we did something like a &lt;a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">Kubernetes contributor workshop&lt;/a>. It was one of its kind in India. It was the first one if I recall correctly. We got a lot of traction and community members interested in contributing to Kubernetes and a lot of other projects. And this is becoming a really valuable thing.&lt;/p>
&lt;p>I'm not much involved in the organization of the group. There are really great people already organizing it. I keep on being around and attending the meetups and trying to answer any questions if people have any.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One way that it is possible to contribute to the Kubernetes ecosystem is through the release process. You've &lt;a href="https://blog.naba.run/posts/release-enhancements-journey/">written a blog&lt;/a> which talks about your journey through that. It started in Kubernetes 1.17, where you took a shadow role for that release. Tell me about what it was like to first take that plunge.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Taking the plunge was a big step, I would say. It should not have been that way. After getting into the team, I saw that it is really encouraged that you should just apply to the team - but then write truthfully about yourself. What do you want? Write your passionate goal, why you want to be in the team.&lt;/p>
&lt;p>So even right now the shadow applications are open for the next release. I wanted to give that a small shoutout. If you want to contribute to the Kubernetes release team, please do apply. The form is pretty simple. You just need to say why do you want to contribute to the release team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What was your answer to that question?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It was a bit tricky. I have this philosophy of contributing to projects that I use in my day-to-day life. I use a lot of open source projects daily, and I started contributing to Kubernetes primarily because I was using the Kubernetes Python client. That was one of my first contributions.&lt;/p>
&lt;p>When I was contributing to that, I explored the release team and it interested me a lot, particularly how interesting and varied the mechanics of releasing Kubernetes are. For most software projects, it's usually whenever you decide that you have made meaningful progress in terms of features, you release it. But Kubernetes is not like that. We follow a regular release cadence. And all those aspects really interested me. I actually applied for the first time in Kubernetes 1.16, but got rejected.&lt;/p>
&lt;p>But I still applied to Kubernetes 1.17, and I got into the enhancements team. That team was led by &lt;a href="https://kubernetespodcast.com/episode/126-research-steering-honking/">MrBobbyTables, Bob Killen&lt;/a>, back then, and &lt;a href="https://kubernetespodcast.com/episode/131-kubernetes-1.20/">Jeremy Rickard&lt;/a> was one of my co-shadows in the team. I shadowed enhancements again. Then I lead enhancements in 1.19. I then shadowed the lead in 1.20 and eventually led the 1.21 team. That's what my journey has been.&lt;/p>
&lt;p>My suggestion to people is don't be afraid of failure. Even if you don't get selected, it's perfectly fine. You can still contribute to the release team. Just hop on the release calls, raise your hand, and introduce yourself.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Between the 1.20 and 1.21 releases, you moved to work on the upstream contribution team at VMware. I've noticed that VMware is hiring a lot of great upstream contributors at the moment. Is this something that &lt;a href="https://kubernetespodcast.com/episode/130-kubecon-na-2020/">Stephen Augustus&lt;/a> had his fingerprints all over? Is there something in the water?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: A lot of people have fingerprints on this process. Stephen certainly had his fingerprints on it, I would say. We are expanding the team of upstream contributors primarily because the product that we are working for is based on Kubernetes. It helps us a lot in driving processes upstream and helping out the community as a whole, because everyone then gets enabled and benefits from what we contribute to the community.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I understand that the Tanzu team is being built out in India at the moment, but I guess you probably haven't been able to meet them in person yet?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yes and no. I did not meet any of them after joining VMware, but I met a lot of my teammates, before I joined VMware, at KubeCons. For example, I met Nikhita, I met Dims, I met Stephen at KubeCon. I am yet to meet other members of the team and I'm really excited to catch up with them once everything comes out of lockdown and we go back to our normal lives.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes, everyone that I speak to who has changed jobs in the pandemic says it's a very odd experience, just nothing really being different. And the same perhaps for people who are working on open source moving companies as well. They're doing the same thing, perhaps just for a different employer.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: As we say in the community, see you in another Slack in some time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We now turn to the recent release of Kubernetes 1.21. First of all, congratulations on that.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Thank you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: &lt;a href="https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/">The announcement&lt;/a> says the release consists of 51 enhancements, 13 graduating to stable, 16 moving to beta, 20 entering alpha, and then two features that have been deprecated. How would you summarize this release?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: One of the big points for this release is that it is the largest release of all time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Really?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yep. 1.20 was the largest release back then, but 1.21 got more enhancements, primarily due to a lot of changes that we did to the process.&lt;/p>
&lt;p>In the 1.21 release cycle, we did a few things differently compared to other release cycles-- for example, in the enhancement process. An enhancement, in the Kubernetes context, is basically a feature proposal. You will hear the terminology &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/README.md">Kubernetes Enhancement Proposals&lt;/a>, or KEP, a lot in the community. An enhancement is a broad thing encapsulated in a specific document.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I like to think of it as a thing that's worth having a heading in the release notes.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Indeed. Until the 1.20 release cycle, what we used to do was-- the release team has a vertical called enhancements. The enhancements team members used to ping each of the enhancement issues and ask whether they want to be part of the release cycle or not. The authors would decide, or talk to their SIG, and then come back with the answer, as to whether they wanted to be part of the cycle.&lt;/p>
&lt;p>In this release, what we did was we eliminated that process and asked the SIGs proactively to discuss amongst themselves, what they wanted to pitch in for this release cycle. What set of features did they want to graduate this release? They may introduce things in alpha, graduate things to beta or stable, or they may also deprecate features.&lt;/p>
&lt;p>What this did was promote a lot of async processes, and at the same time, give power back to the community. The community decides what they want in the release and then comes back collectively. It also reduces a lot of stress on the release team who previously had to ask people consistently what they wanted to pitch in for the release. You now have a deadline. You discuss amongst your SIG what your roadmap is and what it looks like for the near future. Maybe this release, and the next two. And you put all of those answers into a Google spreadsheet. Spreadsheets are still a thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The Kubernetes ecosystem runs entirely on Google Spreadsheets.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It does, and a lot of Google Docs for meeting notes! We did a lot of process improvements, which essentially led to a better release. This release cycle we had 13 enhancements graduating to stable, 16 which moved to beta, and 20 enhancements which were net new features into the ecosystem, and came in as alpha.&lt;/p>
&lt;p>Along with that are features set for deprecation. One of them was PodSecurityPolicy. That has been a point of discussion in the Kubernetes user base and we also published &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">a blog post about it&lt;/a>. All credit to SIG Security who have been on top of things as to find a replacement for PodSecurityPolicy even before this release cycle ended, so that they could at least have a proposal of what will happen next.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk about some old things and some new things. You mentioned PodSecurityPolicy there. That's a thing that's been around a long time and is being deprecated. Two things that have been around a long time and that are now being promoted to stable are CronJobs and PodDisruptionBudgets, both of which were introduced in Kubernetes 1.4, which came out in 2016. Why do you think it took so long for them both to go stable?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I might not have a definitive answer to your question. One of the things that I feel is they might be already so good that nobody saw that they were beta features, and just kept on using them.&lt;/p>
&lt;p>One of the things that I noticed when reading for the CronJobs graduation from beta to stable was the new controller. Users might not see this, but there has been a drastic change in the CronJob controller v2. What it essentially does is goes from a poll-based method of checking what users have defined as CronJobs to a queue architecture, which is the modern method of defining controllers. That has been one of the really good improvements in the case of CronJobs. Instead of the controller working in O(N) time, you now have constant time complexity.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A lot of these features that have been in beta for a long time, like you say, people have an expectation that they are complete. With PodSecurityPolicy, it's being deprecated, which is allowed because it's a feature that never made it out of beta. But how do you think people will react to it going away? And does that say something about the need for the process to make sure that features don't just languish in beta forever, which has been introduced recently?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: That's true. One of the driving factors, when contributors are thinking of graduating beta features has been the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/1635-prevent-permabeta/README.md">&amp;quot;prevention of perma-beta&amp;quot; KEP&lt;/a>. Back in 1.19 we &lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/">introduced this process&lt;/a> where each of the beta resources were marked for deprecation and removal in a certain time frame-- three releases for deprecation and another release for removal. That's also a motivating factor for eventually rethinking as to how beta resources work for us in the community. That is also very effective, I would say.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do remember that Gmail was in beta for eight years.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I did not know that!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Nothing in Kubernetes is quite that old yet, but we'll get there. Of the 20 new enhancements, do you have a favorite or any that you'd like to call out?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: There are two specific features in 1.21 that I'm really interested in, and are coming as net new features. One of them is the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor">persistent volume health monitor&lt;/a>, which gives the users the capability to actually see whether the backing volumes, which power persistent volumes in Kubernetes, are deleted or not. For example, the volumes may get deleted due to an inadvertent event, or they may get corrupted. That information is basically surfaced out as a field so that the user can leverage it in any way.&lt;/p>
&lt;p>The other feature is the proposal for &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cli/859-kubectl-headers">adding headers with the command name to kubectl requests&lt;/a>. We have always set the user-agent information when doing those kind of requests, but the proposal is to add what command the user put in so that we can enable more telemetry, and cluster administrators can determine the usage patterns of how people are using the cluster. I'm really excited about these kind of features coming into play.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You're the first release lead from the Asia-Pacific region, or more accurately, outside of the US and Europe. Most meetings in the Kubernetes ecosystem are traditionally in the window of overlap between the US and Europe, in the morning in California and the evening here in the UK. What's it been like to work outside of the time zones that the community had previously been operating in?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It has been a fun and a challenging proposition, I would say. In the last two-ish years that I have been contributing to Kubernetes, the community has also transformed from a lot of early morning Pacific calls to more towards async processes. For example, we in the release team have transformed our processes so we don't do updates in the calls anymore. What we do is ask for updates ahead of time, and then in the call, we just discuss things which need to be discussed synchronously in the team.&lt;/p>
&lt;p>We leverage the meetings right now more for discussions. But we also don't come to decisions in those discussions, because if any stakeholder is not present on the call, it puts them at a disadvantage. We are trying to talk more on Slack, publicly, or talk on mailing lists. That's where most of the discussion should happen, and also to gain lazy consensus. What I mean by lazy consensus is come up with a pre-decision kind of thing, but then also invite feedback from the broader community about what people would like them to see about that specific thing being discussed. This is where we as a community are also transforming a lot, but there is a lot more headroom to grow.&lt;/p>
&lt;p>The release team also started to have EU/APAC burndown meetings. In addition to having one meeting focused towards the US and European time zones, we also do a meeting which is more suited towards European and Asia-Pacific time zones. One of the driving factors for those decisions was that the release team is seeing a lot of participation from a variety of time zones. To give you one metric, we had release team members this cycle from UTC+8 all through UTC-8 - 16 hours of span. It's really difficult to accommodate all of those zones in a single meeting. And it's not just those 16 hours of span - what about the other eight hours?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yeah, you're missing New Zealand. You could add another 5 hours of span right there.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Exactly. So we will always miss people in meetings, and that's why we should also innovate more, have different kinds of meetings. But that also may not be very sustainable in the future. Will people attend duplicate meetings? Will people follow both of the meetings? More meetings is one of the solutions.&lt;/p>
&lt;p>The other solution is you have threaded discussions on some medium, be it Slack or be it a mailing list. Then, people can just pitch in whenever it is work time for them. Then, at the end of the day, a 24-hour rolling period, you digest it, and then push it out as meeting notes. That's what the Contributor Experience Special Interest Group is doing - shout-out to them for moving to that process. I may be wrong here, but I think once every two weeks, they do async updates on Slack. And that is a really nice thing to have, improving variety of geographies that people can contribute from.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Once you've put everything together that you hope to be in your release, you create a release candidate build. How do you motivate people to test those?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: That's a very interesting question. It is difficult for us to motivate people into trying out these candidates. It's mostly people who are passionate about Kubernetes who try out the release candidates and see for themselves what the bugs are. I remember &lt;a href="https://twitter.com/dims/status/1377272238420934656">Dims tweeting out a call&lt;/a> that if somebody tries out the release candidate and finds a good bug or caveat, they could get a callout in the KubeCon keynote. That's one of the incentives - if you want to be called out in a KubeCon keynote, please try our release candidates.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Or get a new pair of Kubernetes socks?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We would love to give out goodies to people who try out our release candidates and find bugs. For example, if you want the brand new release team logo as a sticker, just hit me up. If you find a bug in a 1.22 release candidate, I would love to be able to send you some coupon codes for the store. Don't quote me on this, but do reach out.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now the release is out, is it time for you to put your feet up? What more things do you have to do, and how do you feel about the path ahead for yourself?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I was discussing this with the team yesterday. Even after the release, we had kind of a water-cooler conversation. I just pasted in a Zoom link to all the release team members and said, hey, do you want to chat? One of the things that I realized that I'm really missing is the daily burndowns right now. I will be around in the release team and the SIG Release meetings, helping out the new lead in transitioning. And even my job, right now, is not over. I'm working with Taylor, who is the emeritus advisor for 1.21, on figuring out some of the mechanics for the next release cycle. I'm also documenting what all we did as part of the process and as part of the process changes, and making sure the next release cycle is up and running.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've done a lot of these release lead interviews now, and there's a question which we always like to ask, which is, what will you write down in the transition envelope? Savitha Raghunathan is the release lead for 1.22. What is the advice that you will pass on to her?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Three words-- &lt;strong>Do, Delegate, and Defer&lt;/strong>. Categorize things into those three buckets as to what you should do right away, what you need to defer, and things that you can delegate to your shadows or other release team members. That's one of the mantras that works really well when leading a team. It is not just in the context of the release team, but it's in the context of managing any team.&lt;/p>
&lt;p>The other bit is &lt;strong>over-communicate&lt;/strong>. No amount of communication is enough. What I've realized is the community is always willing to help you. One of the big examples that I can give is the day before release was supposed to happen, we were seeing a lot of test failures, and then one of the community members had an idea-- why don't you just send an email? I was like, &amp;quot;that sounds good. We can send an email mentioning all the flakes and call out for help to the broader Kubernetes developer community.&amp;quot; And eventually, once we sent out the email, lots of people came in to help us in de-flaking the tests and trying to find out the root cause as to why those tests were failing so often. Big shout out to Antonio and all the SIG Network folks who came to pitch in.&lt;/p>
&lt;p>No matter how many names I mention, it will never be enough. A lot of people, even outside the release team, have helped us a lot with this release. And that's where the release theme comes in - &lt;strong>Power to the Community&lt;/strong>. I'm really stoked by how this community behaves and how people are willing to help you all the time. It's not about what they're telling you to do, but it's what they're also interested in, they're passionate about.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the things you're passionate about is Formula One. Do you think Lewis Hamilton is going to take it away this year?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It's a fair probability that Lewis will win the title this year as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Which would take him to eight all time career wins. And thus-- &lt;a href="https://www.nytimes.com/2020/11/15/sports/autoracing/lewis-hamilton-schumacher-formula-one-record.html">he's currently tied with Michael Schumacher&lt;/a>-- would pull him ahead.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yes. Michael Schumacher was my first favorite F1 driver, I would say. It feels a bit heartbreaking to see someone break Michael's record.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How do you feel about &lt;a href="https://www.formula1.com/en/latest/article.breaking-mick-schumacher-to-race-for-haas-in-2021-as-famous-surname-returns.66XTVfSt80GrZe91lvWVwJ.html">Michael Schumacher's son joining the contest?&lt;/a>&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I feel good. Mick Schumacher is in the fray right now. And I wish we could see him, in a few years, in a Ferrari. The Schumacher family back to Ferrari would be really great to see. But then, my fan favorite has always been McLaren, partly because I like the chemistry of Lando and Carlos over the last two years. It was heartbreaking to see Carlos go to Ferrari. But then we have Lando and Daniel Ricciardo in the team. They're also fun people.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/theonlynabarun">Nabarun Pal&lt;/a> is on the Tanzu team at VMware and served as the Kubernetes 1.21 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Updating NGINX-Ingress to use the stable Ingress API</title><link>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> James Strong, Ricardo Katz&lt;/p>
&lt;p>With all Kubernetes APIs, there is a process to creating, maintaining, and
ultimately deprecating them once they become GA. The networking.k8s.io API group is no
different. The upcoming Kubernetes 1.22 release will remove several deprecated APIs
that are relevant to networking:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>networking.k8s.io/v1beta1&lt;/code> API version of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class">IngressClass&lt;/a>&lt;/li>
&lt;li>all beta versions of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>: &lt;code>extensions/v1beta1&lt;/code> and &lt;code>networking.k8s.io/v1beta1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>On a v1.22 Kubernetes cluster, you'll be able to access Ingress and IngressClass
objects through the stable (v1) APIs, but access via their beta APIs won't be possible.
This change has been in
in discussion since
&lt;a href="https://github.com/kubernetes/kubernetes/issues/43214">2017&lt;/a>,
&lt;a href="https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/">2019&lt;/a> with
1.16 Kubernetes API deprecations, and most recently in
KEP-1453:
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1453-ingress-api#122">Graduate Ingress API to GA&lt;/a>.&lt;/p>
&lt;p>During community meetings, the networking Special Interest Group has decided to continue
supporting Kubernetes versions older than 1.22 with Ingress-NGINX version 0.47.0.
Support for Ingress-NGINX will continue for six months after Kubernetes 1.22
is released. Any additional bug fixes and CVEs for Ingress-NGINX will be
addressed on a need-by-need basis.&lt;/p>
&lt;p>Ingress-NGINX will have separate branches and releases of Ingress-NGINX to
support this model, mirroring the Kubernetes project process. Future
releases of the Ingress-NGINX project will track and support the latest
versions of Kubernetes.&lt;/p>
&lt;table>&lt;caption style="display: none;">Ingress NGINX supported version with Kubernetes Versions&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Kubernetes version&lt;/th>
&lt;th style="text-align:left">Ingress-NGINX version&lt;/th>
&lt;th style="text-align:left">Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">v1.22&lt;/td>
&lt;td style="text-align:left">v1.0.0-alpha.2&lt;/td>
&lt;td style="text-align:left">New features, plus bug fixes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.21&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.20&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.19&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. Fixes only provided until 6 months after Kubernetes v1.22.0 is released.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Because of the updates in Kubernetes 1.22, &lt;strong>v0.47.0&lt;/strong> will not work with
Kubernetes 1.22.&lt;/p>
&lt;h1 id="what-you-need-to-do">What you need to do&lt;/h1>
&lt;p>The team is currently in the process of upgrading ingress-nginx to support
the v1 migration, you can track the progress
&lt;a href="https://github.com/kubernetes/ingress-nginx/pull/7156">here&lt;/a>.&lt;br>
We're not making feature improvements to &lt;code>ingress-nginx&lt;/code> until after the support for
Ingress v1 is complete.&lt;/p>
&lt;p>In the meantime to ensure no compatibility issues:&lt;/p>
&lt;ul>
&lt;li>Update to the latest version of Ingress-NGINX; currently
&lt;a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v0.47.0">v0.47.0&lt;/a>&lt;/li>
&lt;li>After Kubernetes 1.22 is released, ensure you are using the latest version of
Ingress-NGINX that supports the stable APIs for Ingress and IngressClass.&lt;/li>
&lt;li>Test Ingress-NGINX version v1.0.0-alpha.2 with Cluster versions &amp;gt;= 1.19
and report any issues to the projects Github page.&lt;/li>
&lt;/ul>
&lt;p>The community’s feedback and support in this effort is welcome. The
Ingress-NGINX Sub-project regularly holds community meetings where we discuss
this and other issues facing the project. For more information on the sub-project,
please see &lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes Release Cadence Change: Here’s What You Need To Know</title><link>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Celeste Horgan, Adolfo García Veytia, James Laverack, Jeremy Rickard&lt;/p>
&lt;p>On April 23, 2021, the Release Team merged a Kubernetes Enhancement Proposal (KEP) changing the Kubernetes release cycle from four releases a year (once a quarter) to three releases a year.&lt;/p>
&lt;p>This blog post provides a high level overview about what this means for the Kubernetes community's contributors and maintainers.&lt;/p>
&lt;h2 id="what-s-changing-and-when">What's changing and when&lt;/h2>
&lt;p>Starting with the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.22">Kubernetes 1.22 release&lt;/a>, a lightweight policy will drive the creation of each release schedule. This policy states:&lt;/p>
&lt;ul>
&lt;li>The first Kubernetes release of a calendar year should start at the second or third
week of January to provide people more time for contributors coming back from the
end of year holidays.&lt;/li>
&lt;li>The last Kubernetes release of a calendar year should be finished by the middle of
December.&lt;/li>
&lt;li>A Kubernetes release cycle has a length of approximately 15 weeks.&lt;/li>
&lt;li>The week of KubeCon + CloudNativeCon is not considered a 'working week' for SIG Release. The Release Team will not hold meetings or make decisions in this period.&lt;/li>
&lt;li>An explicit SIG Release break of at least two weeks between each cycle will
be enforced.&lt;/li>
&lt;/ul>
&lt;p>As a result, Kubernetes will follow a three releases per year cadence. Kubernetes 1.23 will be the final release of the 2021 calendar year. This new policy results in a very predictable release schedule, allowing us to forecast upcoming release dates:&lt;/p>
&lt;p>&lt;em>Proposed Kubernetes Release Schedule for the remainder of 2021&lt;/em>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Week Number in Year&lt;/th>
&lt;th>Release Number&lt;/th>
&lt;th>Release Week&lt;/th>
&lt;th>Note&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>35&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1 (August 23)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>16 (December 07)&lt;/td>
&lt;td>KubeCon + CloudNativeCon NA Break (Oct 11-15)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;em>Proposed Kubernetes Release Schedule for 2022&lt;/em>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Week Number in Year&lt;/th>
&lt;th>Release Number&lt;/th>
&lt;th>Release Week&lt;/th>
&lt;th>Note&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1 (January 03)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>15 (April 12)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1 (April 26)&lt;/td>
&lt;td>KubeCon + CloudNativeCon EU likely to occur&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>15 (August 09)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>34&lt;/td>
&lt;td>1.26&lt;/td>
&lt;td>1 (August 22&lt;/td>
&lt;td>KubeCon + CloudNativeCon NA likely to occur&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>49&lt;/td>
&lt;td>1.26&lt;/td>
&lt;td>14 (December 06)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>These proposed dates reflect only the start and end dates, and they are subject to change. The Release Team will select dates for enhancement freeze, code freeze, and other milestones at the start of each release. For more information on these milestones, please refer to the &lt;a href="https://www.k8s.dev/resources/release/#phases">release phases&lt;/a> documentation. Feedback from prior releases will feed into this process.&lt;/p>
&lt;h2 id="what-this-means-for-end-users">What this means for end users&lt;/h2>
&lt;p>The major change end users will experience is a slower release cadence and a slower rate of enhancement graduation. Kubernetes release artifacts, release notes, and all other aspects of any given release will stay the same.&lt;/p>
&lt;p>Prior to this change an enhancement could graduate from alpha to stable in 9 months. With the change in cadence, this will stretch to 12 months. Additionally, graduation of features over the last few releases has in some part been driven by release team activities.&lt;/p>
&lt;p>With fewer releases, users can expect to see the rate of feature graduation slow. Users can also expect releases to contain a larger number of enhancements that they need to be aware of during upgrades. However, with fewer releases to consume per year, it's intended that end user organizations will spend less time on upgrades and gain more time on supporting their Kubernetes clusters. It also means that Kubernetes releases are in support for a slightly longer period of time, so bug fixes and security patches will be available for releases for a longer period of time.&lt;/p>
&lt;h2 id="what-this-means-for-kubernetes-contributors">What this means for Kubernetes contributors&lt;/h2>
&lt;p>With a lower release cadence, contributors have more time for project enhancements, feature development, planning, and testing. A slower release cadence also provides more room for maintaining their mental health, preparing for events like KubeCon + CloudNativeCon or work on downstream integrations.&lt;/p>
&lt;h2 id="why-we-decided-to-change-the-release-cadence">Why we decided to change the release cadence&lt;/h2>
&lt;p>The Kubernetes 1.19 cycle was far longer than usual. SIG Release extended it to lessen the burden on both Kubernetes contributors and end users due the COVID-19 pandemic. Following this extended release, the Kubernetes 1.20 release became the third, and final, release for 2020.&lt;/p>
&lt;p>As the Kubernetes project matures, the number of enhancements per cycle grows, along with the burden on contributors, the Release Engineering team. Downstream consumers and integrators also face increased challenges keeping up with &lt;a href="https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/">ever more feature-packed releases&lt;/a>. A wider project adoption means the complexity of supporting a rapidly evolving platform affects a bigger downstream chain of consumers.&lt;/p>
&lt;p>Changing the release cadence from four to three releases per year balances a variety of factors for stakeholders: while it's not strictly an LTS policy, consumers and integrators will get longer support terms for each minor version as the extended release cycles lead to the &lt;a href="https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/">previous three releases being supported&lt;/a> for a longer period. Contributors get more time to &lt;a href="https://www.cncf.io/blog/2021/04/12/enhancing-the-kubernetes-enhancements-process/">mature enhancements&lt;/a> and &lt;a href="https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md">get them ready for production&lt;/a>.&lt;/p>
&lt;p>Finally, the management overhead for SIG Release and the Release Engineering team diminishes allowing the team to spend more time on improving the quality of the software releases and the tooling that drives them.&lt;/p>
&lt;h2 id="how-you-can-help">How you can help&lt;/h2>
&lt;p>Join the &lt;a href="https://github.com/kubernetes/sig-release/discussions/1566">discussion&lt;/a> about communicating future release dates and be sure to be on the lookout for post release surveys.&lt;/p>
&lt;h2 id="where-you-can-find-out-more">Where you can find out more&lt;/h2>
&lt;ul>
&lt;li>Read the KEP &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-release/2572-release-cadence">here&lt;/a>&lt;/li>
&lt;li>Join the &lt;a href="https://groups.google.com/g/kubernetes-dev">kubernetes-dev&lt;/a> mailing list&lt;/li>
&lt;li>Join &lt;a href="https://slack.k8s.io">Kubernetes Slack&lt;/a> and follow the #announcements channel&lt;/li>
&lt;/ul></description></item><item><title>Blog: Spotlight on SIG Usability</title><link>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kunal Kushwaha, Civo&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Are you interested in learning about what &lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">SIG Usability&lt;/a> does and how you can get involved? Well, you're at the right place. SIG Usability is all about making Kubernetes more accessible to new folks, and its main activity is conducting user research for the community. In this blog, we have summarized our conversation with &lt;a href="https://twitter.com/morengab">Gaby Moreno&lt;/a>, who walks us through the various aspects of being a part of the SIG and shares some insights about how others can get involved.&lt;/p>
&lt;p>Gaby is a co-lead for SIG Usability. She works as a Product Designer at IBM and enjoys working on the user experience of open, hybrid cloud technologies like Kubernetes, OpenShift, Terraform, and Cloud Foundry.&lt;/p>
&lt;h2 id="a-summary-of-our-conversation">A summary of our conversation&lt;/h2>
&lt;h3 id="q-could-you-tell-us-a-little-about-what-sig-usability-does">Q. Could you tell us a little about what SIG Usability does?&lt;/h3>
&lt;p>A. SIG Usability at a high level started because there was no dedicated user experience team for Kubernetes. The extent of SIG Usability is focussed on the end-client ease of use of the Kubernetes project. The main activity is user research for the community, which includes speaking to Kubernetes users.&lt;/p>
&lt;p>This covers points like user experience and accessibility. The objectives of the SIG are to guarantee that the Kubernetes project is maximally usable by people of a wide range of foundations and capacities, such as incorporating internationalization and ensuring the openness of documentation.&lt;/p>
&lt;h3 id="q-why-should-new-and-existing-contributors-consider-joining-sig-usability">Q. Why should new and existing contributors consider joining SIG Usability?&lt;/h3>
&lt;p>A. There are plenty of territories where new contributors can begin. For example:&lt;/p>
&lt;ul>
&lt;li>User research projects, where people can help understand the usability of the end-user experiences, including error messages, end-to-end tasks, etc.&lt;/li>
&lt;li>Accessibility guidelines for Kubernetes community artifacts, examples include: internationalization of documentation, color choices for people with color blindness, ensuring compatibility with screen reader technology, user interface design for core components with user interfaces, and more.&lt;/li>
&lt;/ul>
&lt;h3 id="q-what-do-you-do-to-help-new-contributors-get-started">Q. What do you do to help new contributors get started?&lt;/h3>
&lt;p>A. New contributors can get started by shadowing one of the user interviews, going through user interview transcripts, analyzing them, and designing surveys.&lt;/p>
&lt;p>SIG Usability is also open to new project ideas. If you have an idea, we’ll do what we can to support it. There are regular SIG Meetings where people can ask their questions live. These meetings are also recorded for those who may not be able to attend. As always, you can reach out to us on Slack as well.&lt;/p>
&lt;h3 id="q-what-does-the-survey-include">Q. What does the survey include?&lt;/h3>
&lt;p>A. In simple terms, the survey gathers information about how people use Kubernetes, such as trends in learning to deploy a new system, error messages they receive, and workflows.&lt;/p>
&lt;p>One of our goals is to standardize the responses accordingly. The ultimate goal is to analyze survey responses for important user stories whose needs aren't being met.&lt;/p>
&lt;h3 id="q-are-there-any-particular-skills-you-d-like-to-recruit-for-what-skills-are-contributors-to-sig-usability-likely-to-learn">Q. Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?&lt;/h3>
&lt;p>A. Although contributing to SIG Usability does not have any pre-requisites as such, experience with user research, qualitative research, or prior experience with how to conduct an interview would be great plus points. Quantitative research, like survey design and screening, is also helpful and something that we expect contributors to learn.&lt;/p>
&lt;h3 id="q-what-are-you-getting-positive-feedback-on-and-what-s-coming-up-next-for-sig-usability">Q. What are you getting positive feedback on, and what’s coming up next for SIG Usability?&lt;/h3>
&lt;p>A. We have had new members joining and coming to monthly meetings regularly and showing interests in becoming a contributor and helping the community. We have also had a lot of people reach out to us via Slack showcasing their interest in the SIG.&lt;/p>
&lt;p>Currently, we are focused on finishing the study mentioned in our &lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">talk&lt;/a>, also our project for this year. We are always happy to have new contributors join us.&lt;/p>
&lt;h3 id="q-any-closing-thoughts-resources-you-d-like-to-share">Q: Any closing thoughts/resources you’d like to share?&lt;/h3>
&lt;p>A. We love meeting new contributors and assisting them in investigating different Kubernetes project spaces. We will work with and team up with other SIGs to facilitate engaging with end-users, running studies, and help them integrate accessible design practices into their development practices.&lt;/p>
&lt;p>Here are some resources for you to get started:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">GitHub&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/g/kubernetes-sig-usability">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fusability">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.slack.com/archives/CLC5EF63T">Slack channel #sig-usability&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>SIG Usability hosted a &lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">KubeCon talk&lt;/a> about studying Kubernetes users' experiences. The talk focuses on updates to the user study projects, understanding who is using Kubernetes, what they are trying to achieve, how the project is addressing their needs, and where we need to improve the project and the client experience. Join the SIG's update to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream usability team as a contributor!&lt;/p></description></item><item><title>Blog: Kubernetes API and Feature Removals In 1.22: Here’s What You Need To Know</title><link>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Krishna Kilari (Amazon Web Services), Tim Bannister (The Scale Factory)&lt;/p>
&lt;p>As the Kubernetes API evolves, APIs are periodically reorganized or upgraded.
When APIs evolve, the old APIs they replace are deprecated, and eventually removed.
See &lt;a href="#kubernetes-api-removals">Kubernetes API removals&lt;/a> to read more about Kubernetes'
policy on removing APIs.&lt;/p>
&lt;p>We want to make sure you're aware of some upcoming removals. These are
beta APIs that you can use in current, supported Kubernetes versions,
and they are already deprecated. The reason for all of these removals
is that they have been superseded by a newer, stable (“GA”) API.&lt;/p>
&lt;p>Kubernetes 1.22, due for release in August 2021, will remove a number of deprecated
APIs.
&lt;a href="https://www.kubernetes.dev/resources/release/">Kubernetes 1.22 Release Information&lt;/a>
has details on the schedule for the v1.22 release.&lt;/p>
&lt;h2 id="api-changes">API removals for Kubernetes v1.22&lt;/h2>
&lt;p>The &lt;strong>v1.22&lt;/strong> release will stop serving the API versions we've listed immediately below.
These are all beta APIs that were previously deprecated in favor of newer and more stable
API versions.&lt;/p>
&lt;!-- sorted by API group -->
&lt;ul>
&lt;li>Beta versions of the &lt;code>ValidatingWebhookConfiguration&lt;/code> and &lt;code>MutatingWebhookConfiguration&lt;/code> API (the &lt;strong>admissionregistration.k8s.io/v1beta1&lt;/strong> API versions)&lt;/li>
&lt;li>The beta &lt;code>CustomResourceDefinition&lt;/code> API (&lt;strong>apiextensions.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>APIService&lt;/code> API (&lt;strong>apiregistration.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>TokenReview&lt;/code> API (&lt;strong>authentication.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>Beta API versions of &lt;code>SubjectAccessReview&lt;/code>, &lt;code>LocalSubjectAccessReview&lt;/code>, &lt;code>SelfSubjectAccessReview&lt;/code> (API versions from &lt;strong>authorization.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>CertificateSigningRequest&lt;/code> API (&lt;strong>certificates.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>Lease&lt;/code> API (&lt;strong>coordination.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>All beta &lt;code>Ingress&lt;/code> APIs (the &lt;strong>extensions/v1beta1&lt;/strong> and &lt;strong>networking.k8s.io/v1beta1&lt;/strong> API versions)&lt;/li>
&lt;/ul>
&lt;p>The Kubernetes documentation covers these
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22">API removals for v1.22&lt;/a> and explains
how each of those APIs change between beta and stable.&lt;/p>
&lt;h2 id="what-to-do">What to do&lt;/h2>
&lt;p>We're going to run through each of the resources that are affected by these removals
and explain the steps you'll need to take.&lt;/p>
&lt;dl>
&lt;dt>&lt;code>Ingress&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>networking.k8s.io/v1&lt;/strong>
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress&lt;/a> API,
&lt;a href="https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#ingress-graduates-to-general-availability">available since v1.19&lt;/a>.&lt;br>
The related API &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-class-v1/">IngressClass&lt;/a>
is designed to complement the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
concept, allowing you to configure multiple kinds of Ingress within one cluster.
If you're currently using the deprecated
&lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-ingress-class-deprecated">&lt;code>kubernetes.io/ingress.class&lt;/code>&lt;/a>
annotation, plan to switch to using the &lt;code>.spec.ingressClassName&lt;/code> field instead.&lt;br>
On any cluster running Kubernetes v1.19 or later, you can use the v1 API to
retrieve or update existing Ingress objects, even if they were created using an
older API version.
&lt;p>When you convert an Ingress to the v1 API, you should review each rule in that Ingress.
Older Ingresses use the legacy &lt;code>ImplementationSpecific&lt;/code> path type. Instead of &lt;code>ImplementationSpecific&lt;/code>, switch &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types">path matching&lt;/a> to either &lt;code>Prefix&lt;/code> or &lt;code>Exact&lt;/code>. One of the benefits of moving to these alternative path types is that it becomes easier to migrate between different Ingress classes.&lt;/p>
&lt;p>&lt;strong>ⓘ&lt;/strong> As well as upgrading &lt;em>your&lt;/em> own use of the Ingress API as a client, make sure that
every ingress controller that you use is compatible with the v1 Ingress API.
Read &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#prerequisites">Ingress Prerequisites&lt;/a>
for more context about Ingress and ingress controllers.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>ValidatingWebhookConfiguration&lt;/code> and &lt;code>MutatingWebhookConfiguration&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>admissionregistration.k8s.io/v1&lt;/strong> API versions of
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/validating-webhook-configuration-v1/">ValidatingWebhookConfiguration&lt;/a>
and &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/mutating-webhook-configuration-v1/">MutatingWebhookConfiguration&lt;/a>,
available since v1.16.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.&lt;/dd>
&lt;dt>&lt;code>CustomResourceDefinition&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/">CustomResourceDefinition&lt;/a>
&lt;strong>apiextensions.k8s.io/v1&lt;/strong> API, available since v1.16.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. If you defined any custom resources in your cluster, those
are still served after you upgrade.
&lt;p>If you're using external CustomResourceDefinitions, you can use
&lt;a href="#kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/a> to translate existing manifests to use the newer API.
Because there are some functional differences between beta and stable CustomResourceDefinitions,
our advice is to test out each one to make sure it works how you expect after the upgrade.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>APIService&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>apiregistration.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/api-service-v1/">APIService&lt;/a>
API, available since v1.10.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.
If you already have API aggregation using an APIService object, this aggregation continues
to work after you upgrade.&lt;/dd>
&lt;dt>&lt;code>TokenReview&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>authentication.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-review-v1/">TokenReview&lt;/a>
API, available since v1.10.
&lt;p>As well as serving this API via HTTP, the Kubernetes API server uses the same format to
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">send&lt;/a>
TokenReviews to webhooks. The v1.22 release continues to use the v1beta1 API for TokenReviews
sent to webhooks by default. See &lt;a href="#looking-ahead">Looking ahead&lt;/a> for some specific tips about
switching to the stable API.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>SubjectAccessReview&lt;/code>, &lt;code>SelfSubjectAccessReview&lt;/code> and &lt;code>LocalSubjectAccessReview&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>authorization.k8s.io/v1&lt;/strong> versions of those
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/">authorization APIs&lt;/a>, available since v1.6.&lt;/dd>
&lt;dt>&lt;code>CertificateSigningRequest&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>certificates.k8s.io/v1&lt;/strong>
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/certificate-signing-request-v1/">CertificateSigningRequest&lt;/a>
API, available since v1.19.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. Existing issued certificates retain their validity when you upgrade.&lt;/dd>
&lt;dt>&lt;code>Lease&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>coordination.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Lease&lt;/a>
API, available since v1.14.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version.&lt;/dd>
&lt;/dl>
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/h3>
&lt;p>There is a plugin to &lt;code>kubectl&lt;/code> that provides the &lt;code>kubectl convert&lt;/code> subcommand.
It's an official plugin that you can download as part of Kubernetes.
See &lt;a href="https://kubernetes.io/releases/download/">Download Kubernetes&lt;/a> for more details.&lt;/p>
&lt;p>You can use &lt;code>kubectl convert&lt;/code> to update manifest files to use a different API
version. For example, if you have a manifest in source control that uses the beta
Ingress API, you can check that definition out,
and run
&lt;code>kubectl convert -f &amp;lt;manifest&amp;gt; --output-version &amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;&lt;/code>.
You can use the &lt;code>kubectl convert&lt;/code> command to automatically convert an
existing manifest.&lt;/p>
&lt;p>For example, to convert an older Ingress definition to
&lt;code>networking.k8s.io/v1&lt;/code>, you can run:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl convert -f ./legacy-ingress.yaml --output-version networking.k8s.io/v1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The automatic conversion uses a similar technique to how the Kubernetes control plane
updates objects that were originally created using an older API version. Because it's
a mechanical conversion, you might need to go in and change the manifest to adjust
defaults etc.&lt;/p>
&lt;h3 id="rehearse-for-the-upgrade">Rehearse for the upgrade&lt;/h3>
&lt;p>If you manage your cluster's API server component, you can try out these API
removals before you upgrade to Kubernetes v1.22.&lt;/p>
&lt;p>To do that, add the following to the kube-apiserver command line arguments:&lt;/p>
&lt;p>&lt;code>--runtime-config=admissionregistration.k8s.io/v1beta1=false,apiextensions.k8s.io/v1beta1=false,apiregistration.k8s.io/v1beta1=false,authentication.k8s.io/v1beta1=false,authorization.k8s.io/v1beta1=false,certificates.k8s.io/v1beta1=false,coordination.k8s.io/v1beta1=false,extensions/v1beta1/ingresses=false,networking.k8s.io/v1beta1=false&lt;/code>&lt;/p>
&lt;p>(as a side effect, this also turns off v1beta1 of EndpointSlice - watch out for
that when you're testing).&lt;/p>
&lt;p>Once you've switched all the kube-apiservers in your cluster to use that setting,
those beta APIs are removed. You can test that API clients (&lt;code>kubectl&lt;/code>, deployment
tools, custom controllers etc) still work how you expect, and you can revert if
you need to without having to plan a more disruptive downgrade.&lt;/p>
&lt;h3 id="advice-for-software-authors">Advice for software authors&lt;/h3>
&lt;p>Maybe you're reading this because you're a developer of an addon or other
component that integrates with Kubernetes?&lt;/p>
&lt;p>If you develop an Ingress controller, webhook authenticator, an API aggregation, or
any other tool that relies on these deprecated APIs, you should already have started
to switch your software over.&lt;/p>
&lt;p>You can use the tips in
&lt;a href="#rehearse-for-the-upgrade">Rehearse for the upgrade&lt;/a> to run your own Kubernetes
cluster that only uses the new APIs, and make sure that your code works OK.
For your documentation, make sure readers are aware of any steps they should take
for the Kubernetes v1.22 upgrade.&lt;/p>
&lt;p>Where possible, give your users a hand to adopt the new APIs early - perhaps in a
test environment - so they can give you feedback about any problems.&lt;/p>
&lt;p>There are some &lt;a href="#looking-ahead">more deprecations&lt;/a> coming in Kubernetes v1.25,
so plan to have those covered too.&lt;/p>
&lt;h2 id="kubernetes-api-removals">Kubernetes API removals&lt;/h2>
&lt;p>Here's some background about why Kubernetes removes some APIs, and also a promise
about &lt;em>stable&lt;/em> APIs in Kubernetes.&lt;/p>
&lt;p>Kubernetes follows a defined
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a> for its
features, including the Kubernetes API. That policy allows for replacing stable
(“GA”) APIs from Kubernetes. Importantly, this policy means that a stable API only
be deprecated when a newer stable version of that same API is available.&lt;/p>
&lt;p>That stability guarantee matters: if you're using a stable Kubernetes API, there
won't ever be a new version released that forces you to switch to an alpha or beta
feature.&lt;/p>
&lt;p>Earlier stages are different. Alpha features are under test and potentially
incomplete. Almost always, alpha features are disabled by default.
Kubernetes releases can and do remove alpha features that haven't worked out.&lt;/p>
&lt;p>After alpha, comes beta. These features are typically enabled by default; if the
testing works out, the feature can graduate to stable. If not, it might need
a redesign.&lt;/p>
&lt;p>Last year, Kubernetes officially
&lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/#avoiding-permanent-beta">adopted&lt;/a>
a policy for APIs that have reached their beta phase:&lt;/p>
&lt;blockquote>
&lt;p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts
a countdown. The beta-quality API now has three releases …
to either:&lt;/p>
&lt;ul>
&lt;li>reach GA, and deprecate the beta, or&lt;/li>
&lt;li>have a new beta version (and deprecate the previous beta).&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;em>At the time of that article, three Kubernetes releases equated to roughly nine
calendar months. Later that same month, Kubernetes
adopted a new
release cadence of three releases per calendar year, so the countdown period is
now roughly twelve calendar months.&lt;/em>&lt;/p>
&lt;p>Whether an API removal is because of a beta feature graduating to stable, or
because that API hasn't proved successful, Kubernetes will continue to remove
APIs by following its deprecation policy and making sure that migration options
are documented.&lt;/p>
&lt;h3 id="looking-ahead">Looking ahead&lt;/h3>
&lt;p>There's a setting that's relevant if you use webhook authentication checks.
A future Kubernetes release will switch to sending TokenReview objects
to webhooks using the &lt;code>authentication.k8s.io/v1&lt;/code> API by default. At the moment,
the default is to send &lt;code>authentication.k8s.io/v1beta1&lt;/code> TokenReviews to webhooks,
and that's still the default for Kubernetes v1.22.
However, you can switch over to the stable API right now if you want:
add &lt;code>--authentication-token-webhook-version=v1&lt;/code> to the command line options for
the kube-apiserver, and check that webhooks for authentication still work how you
expected.&lt;/p>
&lt;p>Once you're happy it works OK, you can leave the &lt;code>--authentication-token-webhook-version=v1&lt;/code>
option set across your control plane.&lt;/p>
&lt;p>The &lt;strong>v1.25&lt;/strong> release that's planned for next year will stop serving beta versions of
several Kubernetes APIs that are stable right now and have been for some time.
The same v1.25 release will &lt;strong>remove&lt;/strong> PodSecurityPolicy, which is deprecated and won't
graduate to stable. See
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>
for more information.&lt;/p>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25">list of API removals&lt;/a>
planned for Kubernetes 1.25 is:&lt;/p>
&lt;ul>
&lt;li>The beta &lt;code>CronJob&lt;/code> API (&lt;strong>batch/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>EndpointSlice&lt;/code> API (&lt;strong>networking.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>PodDisruptionBudget&lt;/code> API (&lt;strong>policy/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>PodSecurityPolicy&lt;/code> API (&lt;strong>policy/v1beta1&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;h2 id="want-to-know-more">Want to know more?&lt;/h2>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements
of pending deprecations in the release notes for
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecations">1.19&lt;/a>,
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">1.20&lt;/a>,
and &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">1.21&lt;/a>.&lt;/p>
&lt;p>For information on the process of deprecation and removal, check out the official Kubernetes
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy&lt;/a>
document.&lt;/p></description></item><item><title>Blog: Announcing Kubernetes Community Group Annual Reports</title><link>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</link><pubDate>Mon, 28 Jun 2021 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Divya Mohan&lt;/p>
&lt;figure>&lt;a href="https://www.cncf.io/reports/kubernetes-community-annual-report-2020/">
&lt;img src="https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/k8s_annual_report_2020.svg"
alt="Community annual report 2020"/> &lt;/a>
&lt;/figure>
&lt;p>Given the growth and scale of the Kubernetes project, the existing reporting mechanisms were proving to be inadequate and challenging.
Kubernetes is a large open source project. With over 100000 commits just to the main k/kubernetes repository, hundreds of other code
repositories in the project, and thousands of contributors, there's a lot going on. In fact, there are 37 contributor groups at the time of
writing. We also value all forms of contribution and not just code changes.&lt;/p>
&lt;p>With that context in mind, the challenge of reporting on all this activity was a call to action for exploring better options. Therefore
inspired by the Apache Software Foundation’s &lt;a href="https://www.apache.org/foundation/board/reporting">open guide to PMC Reporting&lt;/a> and the
&lt;a href="https://www.cncf.io/cncf-annual-report-2020/">CNCF project Annual Reporting&lt;/a>, the Kubernetes project is proud to announce the
&lt;strong>Kubernetes Community Group Annual Reports for Special Interest Groups (SIGs) and Working Groups (WGs)&lt;/strong>. In its flagship edition,
the &lt;a href="https://www.cncf.io/reports/kubernetes-community-annual-report-2020/">2020 Summary report&lt;/a> focuses on bettering the
Kubernetes ecosystem by assessing and promoting the healthiness of the groups within the upstream community.&lt;/p>
&lt;p>Previously, the mechanisms for the Kubernetes project overall to report on groups and their activities were
&lt;a href="https://k8s.devstats.cncf.io/">devstats&lt;/a>, GitHub data, issues, to measure the healthiness of a given UG/WG/SIG/Committee. As a
project spanning several diverse communities, it was essential to have something that captured the human side of things. With 50,000+
contributors, it’s easy to assume that the project has enough help and this report surfaces more information than /help-wanted and
/good-first-issue for end users. This is how we sustain the project. Paraphrasing one of the Steering Committee members,
&lt;a href="https://github.com/parispittman">Paris Pittman&lt;/a>, “There was a requirement for tighter feedback loops - ones that involved more than just
GitHub data and issues. Given that Kubernetes, as a project, has grown in scale and number of contributors over the years, we have
outgrown the existing reporting mechanisms.&amp;quot;&lt;/p>
&lt;p>The existing communication channels between the Steering committee members and the folks leading the groups and committees were also required
to be made as open and as bi-directional as possible. Towards achieving this very purpose, every group and committee has been assigned a
liaison from among the steering committee members for kick off, help, or guidance needed throughout the process. According to
&lt;a href="https://github.com/dims">Davanum Srinivas a.k.a. dims&lt;/a>, “... That was one of the main motivations behind this report. People (leading the
groups/committees) know that they can reach out to us and there’s a vehicle for them to reach out to us… This is our way of setting up a
two-way feedback for them.&amp;quot; The progress on these action items would be updated and tracked on the monthly Steering Committee meetings
ensuring that this is not a one-off activity. Quoting &lt;a href="https://github.com/nikhita">Nikhita Raghunath&lt;/a>, one of the Steering Committee members,
“... Once we have a base, the liaisons will work with these groups to ensure that the problems are resolved. When we have a report next year,
we’ll have a look at the progress made and how we could still do better. But the idea is definitely to not stop at the report.”&lt;/p>
&lt;p>With this report, we hope to empower our end user communities with information that they can use to identify ways in which they can support
the project as well as a sneak peek into the roadmap for upcoming features. As a community, we thrive on feedback and would love to hear your
views about the report. You can get in touch with the &lt;a href="https://github.com/kubernetes/steering#contact">Steering Committee&lt;/a> via
&lt;a href="https://kubernetes.slack.com/messages/steering-committee">Slack&lt;/a> or via the &lt;a href="steering@kubernetes.io">mailing list&lt;/a>.&lt;/p></description></item><item><title>Blog: Writing a Controller for Pod Labels</title><link>https://kubernetes.io/blog/2021/06/21/writing-a-controller-for-pod-labels/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/06/21/writing-a-controller-for-pod-labels/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Arthur Busser (Padok)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Operators&lt;/a> are proving to be an excellent solution to
running stateful distributed applications in Kubernetes. Open source tools like
the &lt;a href="https://sdk.operatorframework.io/">Operator SDK&lt;/a> provide ways to build reliable and maintainable
operators, making it easier to extend Kubernetes and implement custom
scheduling.&lt;/p>
&lt;p>Kubernetes operators run complex software inside your cluster. The open source
community has already built &lt;a href="https://operatorhub.io/">many operators&lt;/a> for distributed
applications like Prometheus, Elasticsearch, or Argo CD. Even outside of
open source, operators can help to bring new functionality to your Kubernetes
cluster.&lt;/p>
&lt;p>An operator is a set of &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources&lt;/a> and a
set of &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">controllers&lt;/a>. A controller watches for changes to specific
resources in the Kubernetes API and reacts by creating, updating, or deleting
resources.&lt;/p>
&lt;p>The Operator SDK is best suited for building fully-featured operators.
Nonetheless, you can use it to write a single controller. This post will walk
you through writing a Kubernetes controller in Go that will add a &lt;code>pod-name&lt;/code>
label to pods that have a specific annotation.&lt;/p>
&lt;h2 id="why-do-we-need-a-controller-for-this">Why do we need a controller for this?&lt;/h2>
&lt;p>I recently worked on a project where we needed to create a Service that routed
traffic to a specific Pod in a ReplicaSet. The problem is that a Service can
only select pods by label, and all pods in a ReplicaSet have the same labels.
There are two ways to solve this problem:&lt;/p>
&lt;ol>
&lt;li>Create a Service without a selector and manage the Endpoints or
EndpointSlices for that Service directly. We would need to write a custom
controller to insert our Pod's IP address into those resources.&lt;/li>
&lt;li>Add a label to the Pod with a unique value. We could then use this label in
our Service's selector. Again, we would need to write a custom controller to
add this label.&lt;/li>
&lt;/ol>
&lt;p>A controller is a control loop that tracks one or more Kubernetes resource
types. The controller from option n°2 above only needs to track pods, which
makes it simpler to implement. This is the option we are going to walk through
by writing a Kubernetes controller that adds a &lt;code>pod-name&lt;/code> label to our pods.&lt;/p>
&lt;p>StatefulSets &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-name-label">do this natively&lt;/a> by adding a
&lt;code>pod-name&lt;/code> label to each Pod in the set. But what if we don't want to or can't
use StatefulSets?&lt;/p>
&lt;p>We rarely create pods directly; most often, we use a Deployment, ReplicaSet, or
another high-level resource. We can specify labels to add to each Pod in the
PodSpec, but not with dynamic values, so no way to replicate a StatefulSet's
&lt;code>pod-name&lt;/code> label.&lt;/p>
&lt;p>We tried using a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating admission webhook&lt;/a>. When
anyone creates a Pod, the webhook patches the Pod with a label containing the
Pod's name. Disappointingly, this does not work: not all pods have a name before
being created. For instance, when the ReplicaSet controller creates a Pod, it
sends a &lt;code>namePrefix&lt;/code> to the Kubernetes API server and not a &lt;code>name&lt;/code>. The API
server generates a unique name before persisting the new Pod to etcd, but only
after calling our admission webhook. So in most cases, we can't know a Pod's
name with a mutating webhook.&lt;/p>
&lt;p>Once a Pod exists in the Kubernetes API, it is mostly immutable, but we can
still add a label. We can even do so from the command line:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl label my-pod my-label-key&lt;span style="color:#666">=&lt;/span>my-label-value
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We need to watch for changes to any pods in the Kubernetes API and add the label
we want. Rather than do this manually, we are going to write a controller that
does it for us.&lt;/p>
&lt;h2 id="bootstrapping-a-controller-with-the-operator-sdk">Bootstrapping a controller with the Operator SDK&lt;/h2>
&lt;p>A controller is a reconciliation loop that reads the desired state of a resource
from the Kubernetes API and takes action to bring the cluster's actual state
closer to the desired state.&lt;/p>
&lt;p>In order to write this controller as quickly as possible, we are going to use
the Operator SDK. If you don't have it installed, follow the
&lt;a href="https://sdk.operatorframework.io/docs/installation/">official documentation&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">$ operator-sdk version
operator-sdk version: &amp;quot;v1.4.2&amp;quot;, commit: &amp;quot;4b083393be65589358b3e0416573df04f4ae8d9b&amp;quot;, kubernetes version: &amp;quot;v1.19.4&amp;quot;, go version: &amp;quot;go1.15.8&amp;quot;, GOOS: &amp;quot;darwin&amp;quot;, GOARCH: &amp;quot;amd64&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Let's create a new directory to write our controller in:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">mkdir label-operator &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#a2f">cd&lt;/span> label-operator
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, let's initialize a new operator, to which we will add a single controller.
To do this, you will need to specify a domain and a repository. The domain
serves as a prefix for the group your custom Kubernetes resources will belong
to. Because we are not going to be defining custom resources, the domain does
not matter. The repository is going to be the name of the Go module we are going
to write. By convention, this is the repository where you will be storing your
code.&lt;/p>
&lt;p>As an example, here is the command I ran:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Feel free to change the domain and repo values.&lt;/span>
operator-sdk init --domain&lt;span style="color:#666">=&lt;/span>padok.fr --repo&lt;span style="color:#666">=&lt;/span>github.com/busser/label-operator
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, we need a create a new controller. This controller will handle pods and
not a custom resource, so no need to generate the resource code. Let's run this
command to scaffold the code we need:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">operator-sdk create api --group&lt;span style="color:#666">=&lt;/span>core --version&lt;span style="color:#666">=&lt;/span>v1 --kind&lt;span style="color:#666">=&lt;/span>Pod --controller&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span> --resource&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">false&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We now have a new file: &lt;code>controllers/pod_controller.go&lt;/code>. This file contains a
&lt;code>PodReconciler&lt;/code> type with two methods that we need to implement. The first is
&lt;code>Reconcile&lt;/code>, and it looks like this for now:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">Reconcile&lt;/span>(ctx context.Context, req ctrl.Request) (ctrl.Result, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
_ = r.Log.&lt;span style="color:#00a000">WithValues&lt;/span>(&lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, req.NamespacedName)
&lt;span style="color:#080;font-style:italic">// your logic here
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>Reconcile&lt;/code> method is called whenever a Pod is created, updated, or deleted.
The name and namespace of the Pod are in the &lt;code>ctrl.Request&lt;/code> the method receives
as a parameter.&lt;/p>
&lt;p>The second method is &lt;code>SetupWithManager&lt;/code> and for now it looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">SetupWithManager&lt;/span>(mgr ctrl.Manager) &lt;span style="color:#0b0;font-weight:bold">error&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.&lt;span style="color:#00a000">NewControllerManagedBy&lt;/span>(mgr).
&lt;span style="color:#080;font-style:italic">// Uncomment the following line adding a pointer to an instance of the controlled resource as an argument
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// For().
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#00a000">Complete&lt;/span>(r)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>SetupWithManager&lt;/code> method is called when the operator starts. It serves to
tell the operator framework what types our &lt;code>PodReconciler&lt;/code> needs to watch. To
use the same &lt;code>Pod&lt;/code> type used by Kubernetes internally, we need to import some of
its code. All of the Kubernetes source code is open source, so you can import
any part you like in your own Go code. You can find a complete list of available
packages in the Kubernetes source code or &lt;a href="https://pkg.go.dev/k8s.io/api">here on pkg.go.dev&lt;/a>. To
use pods, we need the &lt;code>k8s.io/api/core/v1&lt;/code> package.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> controllers
&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> corev1 &lt;span style="color:#b44">&amp;#34;k8s.io/api/core/v1&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Lets use the &lt;code>Pod&lt;/code> type in &lt;code>SetupWithManager&lt;/code> to tell the operator framework we
want to watch pods:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">SetupWithManager&lt;/span>(mgr ctrl.Manager) &lt;span style="color:#0b0;font-weight:bold">error&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.&lt;span style="color:#00a000">NewControllerManagedBy&lt;/span>(mgr).
&lt;span style="color:#00a000">For&lt;/span>(&lt;span style="color:#666">&amp;amp;&lt;/span>corev1.Pod{}).
&lt;span style="color:#00a000">Complete&lt;/span>(r)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before moving on, we should set the RBAC permissions our controller needs. Above
the &lt;code>Reconcile&lt;/code> method, we have some default permissions:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete
&lt;/span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods/status,verbs=get;update;patch
&lt;/span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods/finalizers,verbs=update
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We don't need all of those. Our controller will never interact with a Pod's
status or its finalizers. It only needs to read and update pods. Lets remove the
unnecessary permissions and keep only what we need:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;update;patch
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We are now ready to write our controller's reconciliation logic.&lt;/p>
&lt;h2 id="implementing-reconciliation">Implementing reconciliation&lt;/h2>
&lt;p>Here is what we want our &lt;code>Reconcile&lt;/code> method to do:&lt;/p>
&lt;ol>
&lt;li>Use the Pod's name and namespace from the &lt;code>ctrl.Request&lt;/code> to fetch the Pod
from the Kubernetes API.&lt;/li>
&lt;li>If the Pod has an &lt;code>add-pod-name-label&lt;/code> annotation, add a &lt;code>pod-name&lt;/code> label to
the Pod; if the annotation is missing, don't add the label.&lt;/li>
&lt;li>Update the Pod in the Kubernetes API to persist the changes made.&lt;/li>
&lt;/ol>
&lt;p>Lets define some constants for the annotation and label:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">const&lt;/span> (
addPodNameLabelAnnotation = &lt;span style="color:#b44">&amp;#34;padok.fr/add-pod-name-label&amp;#34;&lt;/span>
podNameLabel = &lt;span style="color:#b44">&amp;#34;padok.fr/pod-name&amp;#34;&lt;/span>
)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The first step in our reconciliation function is to fetch the Pod we are working
on from the Kubernetes API:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// Reconcile handles a reconciliation request for a Pod.
&lt;/span>&lt;span style="color:#080;font-style:italic">// If the Pod has the addPodNameLabelAnnotation annotation, then Reconcile
&lt;/span>&lt;span style="color:#080;font-style:italic">// will make sure the podNameLabel label is present with the correct value.
&lt;/span>&lt;span style="color:#080;font-style:italic">// If the annotation is absent, then Reconcile will make sure the label is too.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">Reconcile&lt;/span>(ctx context.Context, req ctrl.Request) (ctrl.Result, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
log &lt;span style="color:#666">:=&lt;/span> r.Log.&lt;span style="color:#00a000">WithValues&lt;/span>(&lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, req.NamespacedName)
&lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 0: Fetch the Pod from the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">var&lt;/span> pod corev1.Pod
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Get&lt;/span>(ctx, req.NamespacedName, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to fetch Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Our &lt;code>Reconcile&lt;/code> method will be called when a Pod is created, updated, or
deleted. In the deletion case, our call to &lt;code>r.Get&lt;/code> will return a specific error.
Let's import the package that defines this error:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> controllers
&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> apierrors &lt;span style="color:#b44">&amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can now handle this specific error and — since our controller does not care
about deleted pods — explicitly ignore it:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 0: Fetch the Pod from the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">var&lt;/span> pod corev1.Pod
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Get&lt;/span>(ctx, req.NamespacedName, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> apierrors.&lt;span style="color:#00a000">IsNotFound&lt;/span>(err) {
&lt;span style="color:#080;font-style:italic">// we&amp;#39;ll ignore not-found errors, since we can get them on deleted requests.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to fetch Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, lets edit our Pod so that our dynamic label is present if and only if our
annotation is present:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 1: Add or remove the label.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
labelShouldBePresent &lt;span style="color:#666">:=&lt;/span> pod.Annotations[addPodNameLabelAnnotation] &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>
labelIsPresent &lt;span style="color:#666">:=&lt;/span> pod.Labels[podNameLabel] &lt;span style="color:#666">==&lt;/span> pod.Name
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> labelShouldBePresent &lt;span style="color:#666">==&lt;/span> labelIsPresent {
&lt;span style="color:#080;font-style:italic">// The desired state and actual state of the Pod are the same.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// No further action is required by the operator at this moment.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> log.&lt;span style="color:#00a000">Info&lt;/span>(&lt;span style="color:#b44">&amp;#34;no update required&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> labelShouldBePresent {
&lt;span style="color:#080;font-style:italic">// If the label should be set but is not, set it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">if&lt;/span> pod.Labels &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
pod.Labels = &lt;span style="color:#a2f">make&lt;/span>(&lt;span style="color:#a2f;font-weight:bold">map&lt;/span>[&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>]&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>)
}
pod.Labels[podNameLabel] = pod.Name
log.&lt;span style="color:#00a000">Info&lt;/span>(&lt;span style="color:#b44">&amp;#34;adding label&amp;#34;&lt;/span>)
} &lt;span style="color:#a2f;font-weight:bold">else&lt;/span> {
&lt;span style="color:#080;font-style:italic">// If the label should not be set but is, remove it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f">delete&lt;/span>(pod.Labels, podNameLabel)
log.&lt;span style="color:#00a000">Info&lt;/span>(&lt;span style="color:#b44">&amp;#34;removing label&amp;#34;&lt;/span>)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, let's push our updated Pod to the Kubernetes API:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 2: Update the Pod in the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Update&lt;/span>(ctx, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to update Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>When writing our updated Pod to the Kubernetes API, there is a risk that the Pod
has been updated or deleted since we first read it. When writing a Kubernetes
controller, we should keep in mind that we are not the only actors in the
cluster. When this happens, the best thing to do is start the reconciliation
from scratch, by requeuing the event. Lets do exactly that:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 2: Update the Pod in the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Update&lt;/span>(ctx, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> apierrors.&lt;span style="color:#00a000">IsConflict&lt;/span>(err) {
&lt;span style="color:#080;font-style:italic">// The Pod has been updated since we read it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// Requeue the Pod to try to reconciliate again.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{Requeue: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> apierrors.&lt;span style="color:#00a000">IsNotFound&lt;/span>(err) {
&lt;span style="color:#080;font-style:italic">// The Pod has been deleted since we read it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// Requeue the Pod to try to reconciliate again.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{Requeue: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to update Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let's remember to return successfully at the end of the method:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And that's it! We are now ready to run the controller on our cluster.&lt;/p>
&lt;h2 id="run-the-controller-on-your-cluster">Run the controller on your cluster&lt;/h2>
&lt;p>To run our controller on your cluster, we need to run the operator. For that,
all you will need is &lt;code>kubectl&lt;/code>. If you don't have a Kubernetes cluster at hand,
I recommend you start one locally with &lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">KinD (Kubernetes in Docker)&lt;/a>.&lt;/p>
&lt;p>All it takes to run the operator from your machine is this command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">make run
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After a few seconds, you should see the operator's logs. Notice that our
controller's &lt;code>Reconcile&lt;/code> method was called for all pods already running in the
cluster.&lt;/p>
&lt;p>Let's keep the operator running and, in another terminal, create a new Pod:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl run --image&lt;span style="color:#666">=&lt;/span>nginx my-nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The operator should quickly print some logs, indicating that it reacted to the
Pod's creation and subsequent changes in status:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Lets check the Pod's labels:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">$ kubectl get pod my-nginx --show-labels
NAME READY STATUS RESTARTS AGE LABELS
my-nginx 1/1 Running 0 11m run=my-nginx
&lt;/code>&lt;/pre>&lt;p>Let's add an annotation to the Pod so that our controller knows to add our
dynamic label to it:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl annotate pod my-nginx padok.fr/add-pod-name-label&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice that the controller immediately reacted and produced a new line in its
logs:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">INFO controllers.Pod adding label {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code class="language-terminal" data-lang="terminal">$ kubectl get pod my-nginx --show-labels
NAME READY STATUS RESTARTS AGE LABELS
my-nginx 1/1 Running 0 13m padok.fr/pod-name=my-nginx,run=my-nginx
&lt;/code>&lt;/pre>&lt;p>Bravo! You just successfully wrote a Kubernetes controller capable of adding
labels with dynamic values to resources in your cluster.&lt;/p>
&lt;p>Controllers and operators, both big and small, can be an important part of your
Kubernetes journey. Writing operators is easier now than it has ever been. The
possibilities are endless.&lt;/p>
&lt;h2 id="what-next">What next?&lt;/h2>
&lt;p>If you want to go further, I recommend starting by deploying your controller or
operator inside a cluster. The &lt;code>Makefile&lt;/code> generated by the Operator SDK will do
most of the work.&lt;/p>
&lt;p>When deploying an operator to production, it is always a good idea to implement
robust testing. The first step in that direction is to write unit tests.
&lt;a href="https://sdk.operatorframework.io/docs/building-operators/golang/testing/">This documentation&lt;/a> will guide you in writing tests for
your operator. I wrote tests for the operator we just wrote; you can find all of
my code in &lt;a href="https://github.com/busser/label-operator">this GitHub repository&lt;/a>.&lt;/p>
&lt;h2 id="how-to-learn-more">How to learn more?&lt;/h2>
&lt;p>The &lt;a href="https://sdk.operatorframework.io/docs/">Operator SDK documentation&lt;/a> goes into detail on how you
can go further and implement more complex operators.&lt;/p>
&lt;p>When modeling a more complex use-case, a single controller acting on built-in
Kubernetes types may not be enough. You may need to build a more complex
operator with &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions (CRDs)&lt;/a>
and multiple controllers. The Operator SDK is a great tool to help you do this.&lt;/p>
&lt;p>If you want to discuss building an operator, join the &lt;a href="https://kubernetes.slack.com/messages/kubernetes-operators">#kubernetes-operator&lt;/a>
channel in the &lt;a href="https://slack.k8s.io/">Kubernetes Slack workspace&lt;/a>!&lt;/p>
&lt;!-- Links --></description></item><item><title>Blog: Using Finalizers to Control Deletion</title><link>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Aaron Alpar (Kasten)&lt;/p>
&lt;p>Deleting objects in Kubernetes can be challenging. You may think you’ve deleted something, only to find it still persists. While issuing a &lt;code>kubectl delete&lt;/code> command and hoping for the best might work for day-to-day operations, understanding how Kubernetes &lt;code>delete&lt;/code> commands operate will help you understand why some objects linger after deletion.&lt;/p>
&lt;p>In this post, I’ll look at:&lt;/p>
&lt;ul>
&lt;li>What properties of a resource govern deletion&lt;/li>
&lt;li>How finalizers and owner references impact object deletion&lt;/li>
&lt;li>How the propagation policy can be used to change the order of deletions&lt;/li>
&lt;li>How deletion works, with examples&lt;/li>
&lt;/ul>
&lt;p>For simplicity, all examples will use ConfigMaps and basic shell commands to demonstrate the process. We’ll explore how the commands work and discuss repercussions and results from using them in practice.&lt;/p>
&lt;h2 id="the-basic-delete">The basic &lt;code>delete&lt;/code>&lt;/h2>
&lt;p>Kubernetes has several different commands you can use that allow you to create, read, update, and delete objects. For the purpose of this blog post, we’ll focus on four &lt;code>kubectl&lt;/code> commands: &lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>patch&lt;/code>, and &lt;code>delete&lt;/code>.&lt;/p>
&lt;p>Here are examples of the basic &lt;code>kubectl delete&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>kubectl create configmap mymap
configmap/mymap created
&lt;/code>&lt;/pre>&lt;pre>&lt;code>kubectl get configmap/mymap
NAME DATA AGE
mymap 0 12s
&lt;/code>&lt;/pre>&lt;pre>&lt;code>kubectl delete configmap/mymap
configmap &amp;quot;mymap&amp;quot; deleted
&lt;/code>&lt;/pre>&lt;pre>&lt;code>kubectl get configmap/mymap
Error from server (NotFound): configmaps &amp;quot;mymap&amp;quot; not found
&lt;/code>&lt;/pre>&lt;p>Shell commands preceded by &lt;code>$&lt;/code> are followed by their output. You can see that we begin with a &lt;code>kubectl create configmap mymap&lt;/code>, which will create the empty configmap &lt;code>mymap&lt;/code>. Next, we need to &lt;code>get&lt;/code> the configmap to prove it exists. We can then delete that configmap. Attempting to &lt;code>get&lt;/code> it again produces an HTTP 404 error, which means the configmap is not found.&lt;/p>
&lt;p>The state diagram for the basic &lt;code>delete&lt;/code> command is very simple:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-05-14-using-finalizers-to-control-deletion/state-diagram-delete.png"
alt="State diagram for delete" width="495"/> &lt;figcaption>
&lt;p>State diagram for delete&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Although this operation is straightforward, other factors may interfere with the deletion, including finalizers and owner references.&lt;/p>
&lt;h2 id="understanding-finalizers">Understanding Finalizers&lt;/h2>
&lt;p>When it comes to understanding resource deletion in Kubernetes, knowledge of how finalizers work is helpful and can help you understand why some objects don’t get deleted.&lt;/p>
&lt;p>Finalizers are keys on resources that signal pre-delete operations. They control the garbage collection on resources, and are designed to alert controllers what cleanup operations to perform prior to removing a resource. However, they don’t necessarily name code that should be executed; finalizers on resources are basically just lists of keys much like annotations. Like annotations, they can be manipulated.&lt;/p>
&lt;p>Some common finalizers you’ve likely encountered are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubernetes.io/pv-protection&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.io/pvc-protection&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The finalizers above are used on volumes to prevent accidental deletion. Similarly, some finalizers can be used to prevent deletion of any resource but are not managed by any controller.&lt;/p>
&lt;p>Below with a custom configmap, which has no properties but contains a finalizer:&lt;/p>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
name: mymap
finalizers:
- kubernetes
EOF
&lt;/code>&lt;/pre>&lt;p>The configmap resource controller doesn't understand what to do with the &lt;code>kubernetes&lt;/code> finalizer key. I term these “dead” finalizers for configmaps as it is normally used on namespaces. Here’s what happen upon attempting to delete the configmap:&lt;/p>
&lt;pre>&lt;code>kubectl delete configmap/mymap &amp;amp;
configmap &amp;quot;mymap&amp;quot; deleted
jobs
[1]+ Running kubectl delete configmap/mymap
&lt;/code>&lt;/pre>&lt;p>Kubernetes will report back that the object has been deleted, however, it hasn’t been deleted in a traditional sense. Rather, it’s in the process of deletion. When we attempt to &lt;code>get&lt;/code> that object again, we discover the object has been modified to include the deletion timestamp.&lt;/p>
&lt;pre>&lt;code>kubectl get configmap/mymap -o yaml
apiVersion: v1
kind: ConfigMap
metadata:
creationTimestamp: &amp;quot;2020-10-22T21:30:18Z&amp;quot;
deletionGracePeriodSeconds: 0
deletionTimestamp: &amp;quot;2020-10-22T21:30:34Z&amp;quot;
finalizers:
- kubernetes
name: mymap
namespace: default
resourceVersion: &amp;quot;311456&amp;quot;
selfLink: /api/v1/namespaces/default/configmaps/mymap
uid: 93a37fed-23e3-45e8-b6ee-b2521db81638
&lt;/code>&lt;/pre>&lt;p>In short, what’s happened is that the object was updated, not deleted. That’s because Kubernetes saw that the object contained finalizers and put it into a read-only state. The deletion timestamp signals that the object can only be read, with the exception of removing the finalizer key updates. In other words, the deletion will not be complete until we edit the object and remove the finalizer.&lt;/p>
&lt;p>Here's a demonstration of using the &lt;code>patch&lt;/code> command to remove finalizers. If we want to delete an object, we can simply patch it on the command line to remove the finalizers. In this way, the deletion that was running in the background will complete and the object will be deleted. When we attempt to &lt;code>get&lt;/code> that configmap, it will be gone.&lt;/p>
&lt;pre>&lt;code>kubectl patch configmap/mymap \
--type json \
--patch='[ { &amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/metadata/finalizers&amp;quot; } ]'
configmap/mymap patched
[1]+ Done kubectl delete configmap/mymap
kubectl get configmap/mymap -o yaml
Error from server (NotFound): configmaps &amp;quot;mymap&amp;quot; not found
&lt;/code>&lt;/pre>&lt;p>Here's a state diagram for finalization:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-05-14-using-finalizers-to-control-deletion/state-diagram-finalize.png"
alt="State diagram for finalize" width="617"/> &lt;figcaption>
&lt;p>State diagram for finalize&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>So, if you attempt to delete an object that has a finalizer on it, it will remain in finalization until the controller has removed the finalizer keys or the finalizers are removed using Kubectl. Once that finalizer list is empty, the object can actually be reclaimed by Kubernetes and put into a queue to be deleted from the registry.&lt;/p>
&lt;h2 id="owner-references">Owner References&lt;/h2>
&lt;p>Owner references describe how groups of objects are related. They are properties on resources that specify the relationship to one another, so entire trees of resources can be deleted.&lt;/p>
&lt;p>Finalizer rules are processed when there are owner references. An owner reference consists of a name and a UID. Owner references link resources within the same namespace, and it also needs a UID for that reference to work. Pods typically have owner references to the owning replica set. So, when deployments or stateful sets are deleted, then the child replica sets and pods are deleted in the process.&lt;/p>
&lt;p>Here are some examples of owner references and how they work. In the first example, we create a parent object first, then the child. The result is a very simple configmap that contains an owner reference to its parent:&lt;/p>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
name: mymap-parent
EOF
CM_UID=$(kubectl get configmap mymap-parent -o jsonpath=&amp;quot;{.metadata.uid}&amp;quot;)
cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
name: mymap-child
ownerReferences:
- apiVersion: v1
kind: ConfigMap
name: mymap-parent
uid: $CM_UID
EOF
&lt;/code>&lt;/pre>&lt;p>Deleting the child object when an owner reference is involved does not delete the parent:&lt;/p>
&lt;pre>&lt;code>kubectl get configmap
NAME DATA AGE
mymap-child 0 12m4s
mymap-parent 0 12m4s
kubectl delete configmap/mymap-child
configmap &amp;quot;mymap-child&amp;quot; deleted
kubectl get configmap
NAME DATA AGE
mymap-parent 0 12m10s
&lt;/code>&lt;/pre>&lt;p>In this example, we re-created the parent-child configmaps from above. Now, when deleting from the parent (instead of the child) with an owner reference from the child to the parent, when we &lt;code>get&lt;/code> the configmaps, none are in the namespace:&lt;/p>
&lt;pre>&lt;code>kubectl get configmap
NAME DATA AGE
mymap-child 0 10m2s
mymap-parent 0 10m2s
kubectl delete configmap/mymap-parent
configmap &amp;quot;mymap-parent&amp;quot; deleted
kubectl get configmap
No resources found in default namespace.
&lt;/code>&lt;/pre>&lt;p>To sum things up, when there's an override owner reference from a child to a parent, deleting the parent deletes the children automatically. This is called &lt;code>cascade&lt;/code>. The default for cascade is &lt;code>true&lt;/code>, however, you can use the --cascade=false option for &lt;code>kubectl delete&lt;/code> to delete an object and orphan its children.&lt;/p>
&lt;p>In the following example, there is a parent and a child. Notice the owner references are still included. If I delete the parent using --cascade=false, the parent is deleted but the child still exists:&lt;/p>
&lt;pre>&lt;code>kubectl get configmap
NAME DATA AGE
mymap-child 0 13m8s
mymap-parent 0 13m8s
kubectl delete --cascade=false configmap/mymap-parent
configmap &amp;quot;mymap-parent&amp;quot; deleted
kubectl get configmap
NAME DATA AGE
mymap-child 0 13m21s
&lt;/code>&lt;/pre>&lt;p>The --cascade option links to the propagation policy in the API, which allows you to change the order in which objects are deleted within a tree. In the following example uses API access to craft a custom delete API call with the background propagation policy:&lt;/p>
&lt;pre>&lt;code>kubectl proxy --port=8080 &amp;amp;
Starting to serve on 127.0.0.1:8080
curl -X DELETE \
localhost:8080/api/v1/namespaces/default/configmaps/mymap-parent \
-d '{ &amp;quot;kind&amp;quot;:&amp;quot;DeleteOptions&amp;quot;, &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;, &amp;quot;propagationPolicy&amp;quot;:&amp;quot;Background&amp;quot; }' \
-H &amp;quot;Content-Type: application/json&amp;quot;
{
&amp;quot;kind&amp;quot;: &amp;quot;Status&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
&amp;quot;metadata&amp;quot;: {},
&amp;quot;status&amp;quot;: &amp;quot;Success&amp;quot;,
&amp;quot;details&amp;quot;: { ... }
}
&lt;/code>&lt;/pre>&lt;p>Note that the propagation policy cannot be specified on the command line using kubectl. You have to specify it using a custom API call. Simply create a proxy, so you have access to the API server from the client, and execute a &lt;code>curl&lt;/code> command with just a URL to execute that &lt;code>delete&lt;/code> command.&lt;/p>
&lt;p>There are three different options for the propagation policy:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Foreground&lt;/code>: Children are deleted before the parent (post-order)&lt;/li>
&lt;li>&lt;code>Background&lt;/code>: Parent is deleted before the children (pre-order)&lt;/li>
&lt;li>&lt;code>Orphan&lt;/code>: Owner references are ignored&lt;/li>
&lt;/ul>
&lt;p>Keep in mind that when you delete an object and owner references have been specified, finalizers will be honored in the process. This can result in trees of objects persisting, and you end up with a partial deletion. At that point, you have to look at any existing owner references on your objects, as well as any finalizers, to understand what’s happening.&lt;/p>
&lt;h2 id="forcing-a-deletion-of-a-namespace">Forcing a Deletion of a Namespace&lt;/h2>
&lt;p>There's one situation that may require forcing finalization for a namespace. If you've deleted a namespace and you've cleaned out all of the objects under it, but the namespace still exists, deletion can be forced by updating the namespace subresource, &lt;code>finalize&lt;/code>. This informs the namespace controller that it needs to remove the finalizer from the namespace and perform any cleanup:&lt;/p>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF | curl -X PUT \
localhost:8080/api/v1/namespaces/test/finalize \
-H &amp;quot;Content-Type: application/json&amp;quot; \
--data-binary @-
{
&amp;quot;kind&amp;quot;: &amp;quot;Namespace&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;test&amp;quot;
},
&amp;quot;spec&amp;quot;: {
&amp;quot;finalizers&amp;quot;: null
}
}
EOF
&lt;/code>&lt;/pre>&lt;p>This should be done with caution as it may delete the namespace only and leave orphan objects within the, now non-exiting, namespace - a confusing state for Kubernetes. If this happens, the namespace can be re-created manually and sometimes the orphaned objects will re-appear under the just-created namespace which will allow manual cleanup and recovery.&lt;/p>
&lt;h2 id="key-takeaways">Key Takeaways&lt;/h2>
&lt;p>As these examples demonstrate, finalizers can get in the way of deleting resources in Kubernetes, especially when there are parent-child relationships between objects. Often, there is a reason for adding a finalizer into the code, so you should always investigate before manually deleting it. Owner references allow you to specify and remove trees of resources, although finalizers will be honored in the process. Finally, the propagation policy can be used to specify the order of deletion via a custom API call, giving you control over how objects are deleted. Now that you know a little more about how deletions work in Kubernetes, we recommend you try it out on your own, using a test cluster.&lt;/p>
&lt;div class="youtube-quote-sm">
&lt;iframe src="https://www.youtube.com/embed/F7-ZxWwf4sY" allowfullscreen title="Clean Up Your Room! What Does It Mean to Delete Something in K8s">&lt;/iframe>
&lt;/div></description></item><item><title>Blog: Kubernetes 1.21: Metrics Stability hits GA</title><link>https://kubernetes.io/blog/2021/04/23/kubernetes-release-1.21-metrics-stability-ga/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/23/kubernetes-release-1.21-metrics-stability-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Han Kang (Google), Elana Hashman (Red Hat)&lt;/p>
&lt;p>Kubernetes 1.21 marks the graduation of the metrics stability framework and along with it, the first officially supported stable metrics. Not only do stable metrics come with supportability guarantees, the metrics stability framework brings escape hatches that you can use if you encounter problematic metrics.&lt;/p>
&lt;p>See the list of &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml">stable Kubernetes metrics here&lt;/a>&lt;/p>
&lt;h3 id="what-are-stable-metrics-and-why-do-we-need-them">What are stable metrics and why do we need them?&lt;/h3>
&lt;p>A stable metric is one which, from a consumption point of view, can be reliably consumed across a number of Kubernetes versions without risk of ingestion failure.&lt;/p>
&lt;p>Metrics stability is an ongoing community concern. Cluster monitoring infrastructure often assumes the stability of some control plane metrics, so we have introduced a mechanism for versioning metrics as a proper API, with stability guarantees around a formal metrics deprecation process.&lt;/p>
&lt;h3 id="what-are-the-stability-levels-for-metrics">What are the stability levels for metrics?&lt;/h3>
&lt;p>Metrics can currently have one of two stability levels: alpha or stable.&lt;/p>
&lt;p>&lt;em>Alpha metrics&lt;/em> have no stability guarantees; as such they can be modified or deleted at any time. At this time, all Kubernetes metrics implicitly fall into this category.&lt;/p>
&lt;p>&lt;em>Stable metrics&lt;/em> can be guaranteed to not change, except that the metric may become marked deprecated for a future Kubernetes version. By not change, we mean three things:&lt;/p>
&lt;ol>
&lt;li>the metric itself will not be deleted or renamed&lt;/li>
&lt;li>the type of metric will not be modified&lt;/li>
&lt;li>no labels can be added or removed from this metric&lt;/li>
&lt;/ol>
&lt;p>From an ingestion point of view, it is backwards-compatible to add or remove possible values for labels which already do exist, but not labels themselves. Therefore, adding or removing values from an existing label is permitted. Stable metrics can also be marked as deprecated for a future Kubernetes version, since this is tracked in a metadata field and does not actually change the metric itself.&lt;/p>
&lt;p>Removing or adding labels from stable metrics is not permitted. In order to add or remove a label from an existing stable metric, one would have to introduce a new metric and deprecate the stable one; otherwise this would violate compatibility agreements.&lt;/p>
&lt;h4 id="how-are-metrics-deprecated">How are metrics deprecated?&lt;/h4>
&lt;p>While deprecation policies only affect stability guarantees for stable metrics (and not alpha ones), deprecation information may be optionally provided on alpha metrics to help component owners inform users of future intent and assist with transition plans.&lt;/p>
&lt;p>A stable metric undergoing the deprecation process signals that the metric will eventually be deleted. The metrics deprecation lifecycle looks roughly like this (with each stage representing a Kubernetes release):&lt;/p>
&lt;p>&lt;img src="lifecycle-metric.png" alt="Stable metric → Deprecated metric → Hidden metric → Deletion">&lt;/p>
&lt;p>&lt;em>Deprecated metrics&lt;/em> have the same stability guarantees of their stable counterparts. If a stable metric is deprecated, then a deprecated stable metric is guaranteed to not change. When deprecating a stable metric, a future Kubernetes release is specified as the point from which the metric will be considered deprecated.&lt;/p>
&lt;p>Deprecated metrics will have their description text prefixed with a deprecation notice string “(Deprecated from x.y)” and a warning log will be emitted during metric registration, in the spirit of the official Kubernetes deprecation policy.&lt;/p>
&lt;p>Like their stable metric counterparts, deprecated metrics will be automatically registered to the metrics endpoint. On a subsequent release (when the metric's deprecatedVersion is equal to &lt;em>current_kubernetes_version - 4&lt;/em>)), a deprecated metric will become a &lt;em>hidden&lt;/em> metric. &lt;em>Hidden metrics&lt;/em> are not automatically registered, and hence are hidden by default from end users. These hidden metrics can be explicitly re-enabled for one release after they reach the hidden state, to provide a migration path for cluster operators.&lt;/p>
&lt;h4 id="as-an-owner-of-a-kubernetes-component-how-do-i-add-stable-metrics">As an owner of a Kubernetes component, how do I add stable metrics?&lt;/h4>
&lt;p>During metric instantiation, stability can be specified by setting the metadata field, StabilityLevel, to “Stable”. When a StabilityLevel is not explicitly set, metrics default to “Alpha” stability. Note that metrics which have fields determined at runtime cannot be marked as Stable. Stable metrics will be detected during static analysis during the pre-commit phase, and must be reviewed by sig-instrumentation.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">&lt;span style="color:#a2f;font-weight:bold">var&lt;/span> metricDefinition = kubemetrics.CounterOpts{
Name: &lt;span style="color:#b44">&amp;#34;some_metric&amp;#34;&lt;/span>,
Help: &lt;span style="color:#b44">&amp;#34;some description&amp;#34;&lt;/span>,
StabilityLevel: kubemetrics.STABLE,
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>For more examples of setting metrics stability and deprecation, see the &lt;a href="http://bit.ly/metrics-stability">Metrics Stability KEP&lt;/a>.&lt;/p>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.
We offer a huge thank you to all the contributors in Kubernetes community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p>
&lt;ul>
&lt;li>Han Kang (logicalhan)&lt;/li>
&lt;li>Frederic Branczyk (brancz)&lt;/li>
&lt;li>Marek Siarkowicz (serathius)&lt;/li>
&lt;li>Elana Hashman (ehashman)&lt;/li>
&lt;li>Solly Ross (DirectXMan12)&lt;/li>
&lt;li>Stefan Schimanski (sttts)&lt;/li>
&lt;li>David Ashpole (dashpole)&lt;/li>
&lt;li>Yuchen Zhou (yoyinzyc)&lt;/li>
&lt;li>Yu Yi (erain)&lt;/li>
&lt;/ul>
&lt;p>If you’re interested in getting involved with the design and development of instrumentation or any part of the Kubernetes metrics system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-instrumentation">Kubernetes Instrumentation Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Evolving Kubernetes networking with the Gateway API</title><link>https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/</link><pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Mark Church (Google), Harry Bagdi (Kong), Daneyon Hanson (Red Hat), Nick Young (VMware), Manuel Zapf (Traefik Labs)&lt;/p>
&lt;p>The Ingress resource is one of the many Kubernetes success stories. It created a &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">diverse ecosystem of Ingress controllers&lt;/a> which were used across hundreds of thousands of clusters in a standardized and consistent way. This standardization helped users adopt Kubernetes. However, five years after the creation of Ingress, there are signs of fragmentation into different but &lt;a href="https://dave.cheney.net/paste/ingress-is-dead-long-live-ingressroute.pdf">strikingly similar CRDs&lt;/a> and &lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">overloaded annotations&lt;/a>. The same portability that made Ingress pervasive also limited its future.&lt;/p>
&lt;p>It was at Kubecon 2019 San Diego when a passionate group of contributors gathered to discuss the &lt;a href="https://static.sched.com/hosted_files/kccncna19/a5/Kubecon%20San%20Diego%202019%20-%20Evolving%20the%20Kubernetes%20Ingress%20APIs%20to%20GA%20and%20Beyond%20%5BPUBLIC%5D.pdf">evolution of Ingress&lt;/a>. The discussion overflowed to the hotel lobby across the street and what came out of it would later be known as the &lt;a href="https://gateway-api.sigs.k8s.io">Gateway API&lt;/a>. This discussion was based on a few key assumptions:&lt;/p>
&lt;ol>
&lt;li>The API standards underlying route matching, traffic management, and service exposure are commoditized and provide little value to their implementers and users as custom APIs&lt;/li>
&lt;li>It’s possible to represent L4/L7 routing and traffic management through common core API resources&lt;/li>
&lt;li>It’s possible to provide extensibility for more complex capabilities in a way that does not sacrifice the user experience of the core API&lt;/li>
&lt;/ol>
&lt;h2 id="introducing-the-gateway-api">Introducing the Gateway API&lt;/h2>
&lt;p>This led to design principles that allow the Gateway API to improve upon Ingress:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Expressiveness&lt;/strong> - In addition to HTTP host/path matching and TLS, Gateway API can express capabilities like HTTP header manipulation, traffic weighting &amp;amp; mirroring, TCP/UDP routing, and other capabilities that were only possible in Ingress through custom annotations.&lt;/li>
&lt;li>&lt;strong>Role-oriented design&lt;/strong> - The API resource model reflects the separation of responsibilities that is common in routing and Kubernetes service networking.&lt;/li>
&lt;li>&lt;strong>Extensibility&lt;/strong> - The resources allow arbitrary configuration attachment at various layers within the API. This makes granular customization possible at the most appropriate places.&lt;/li>
&lt;li>&lt;strong>Flexible conformance&lt;/strong> - The Gateway API defines varying conformance levels - core (mandatory support), extended (portable if supported), and custom (no portability guarantee), known together as &lt;a href="https://gateway-api.sigs.k8s.io/concepts/guidelines/#conformance">flexible conformance&lt;/a>. This promotes a highly portable core API (like Ingress) that still gives flexibility for Gateway controller implementers.&lt;/li>
&lt;/ul>
&lt;h3 id="what-does-the-gateway-api-look-like">What does the Gateway API look like?&lt;/h3>
&lt;p>The Gateway API introduces a few new resource types:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.GatewayClass">GatewayClasses&lt;/a>&lt;/strong> are cluster-scoped resources that act as templates to explicitly define behavior for Gateways derived from them. This is similar in concept to StorageClasses, but for networking data-planes.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.Gateway">Gateways&lt;/a>&lt;/strong> are the deployed instances of GatewayClasses. They are the logical representation of the data-plane which performs routing, which may be in-cluster proxies, hardware LBs, or cloud LBs.&lt;/li>
&lt;li>&lt;strong>Routes&lt;/strong> are not a single resource, but represent many different protocol-specific Route resources. The &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.HTTPRoute">HTTPRoute&lt;/a> has matching, filtering, and routing rules that get applied to Gateways that can process HTTP and HTTPS traffic. Similarly, there are &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.TCPRoute">TCPRoutes&lt;/a>, &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.UDPRoute">UDPRoutes&lt;/a>, and &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.TLSRoute">TLSRoutes&lt;/a> which also have protocol-specific semantics. This model also allows the Gateway API to incrementally expand its protocol support in the future.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="gateway-api-resources.png" alt="The resources of the Gateway API">&lt;/p>
&lt;h3 id="gateway-controller-implementations">Gateway Controller Implementations&lt;/h3>
&lt;p>The good news is that although Gateway is in &lt;a href="https://github.com/kubernetes-sigs/gateway-api/releases">Alpha&lt;/a>, there are already several &lt;a href="https://gateway-api.sigs.k8s.io/references/implementations/">Gateway controller implementations&lt;/a> that you can run. Since it’s a standardized spec, the following example could be run on any of them and should function the exact same way. Check out &lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/">getting started&lt;/a> to see how to install and use one of these Gateway controllers.&lt;/p>
&lt;h2 id="getting-hands-on-with-the-gateway-api">Getting Hands-on with the Gateway API&lt;/h2>
&lt;p>In the following example, we’ll demonstrate the relationships between the different API Resources and walk you through a common use case:&lt;/p>
&lt;ul>
&lt;li>Team foo has their app deployed in the foo Namespace. They need to control the routing logic for the different pages of their app.&lt;/li>
&lt;li>Team bar is running in the bar Namespace. They want to be able to do blue-green rollouts of their application to reduce risk.&lt;/li>
&lt;li>The platform team is responsible for managing the load balancer and network security of all the apps in the Kubernetes cluster.&lt;/li>
&lt;/ul>
&lt;p>The following foo-route does path matching to various Services in the foo Namespace and also has a default route to a 404 server. This exposes foo-auth and foo-home Services via &lt;code>foo.example.com/login&lt;/code> and &lt;code>foo.example.com/home&lt;/code> respectively.:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-route&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hostnames&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;foo.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Prefix&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/login&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-auth&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Prefix&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/home&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-home&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Prefix&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-404&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The bar team, operating in the bar Namespace of the same Kubernetes cluster, also wishes to expose their application to the internet, but they also want to control their own canary and blue-green rollouts. The following HTTPRoute is configured for the following behavior:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>For traffic to &lt;code>bar.example.com&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Send 90% of the traffic to bar-v1&lt;/li>
&lt;li>Send 10% of the traffic to bar-v2&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>For traffic to &lt;code>bar.example.com&lt;/code> with the HTTP header &lt;code>env: canary&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Send all the traffic to bar-v2&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="httproute.png" alt="The routing rules configured for the bar-v1 and bar-v2 Services">&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-route&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hostnames&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;bar.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">90&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="route-and-gateway-binding">Route and Gateway Binding&lt;/h3>
&lt;p>So we have two HTTPRoutes matching and routing traffic to different Services. You might be wondering, where are these Services accessible? Through which networks or IPs are they exposed?&lt;/p>
&lt;p>How Routes are exposed to clients is governed by &lt;a href="https://gateway-api.sigs.k8s.io/concepts/api-overview/#route-binding">Route binding&lt;/a>, which describes how Routes and Gateways create a bidirectional relationship between each other. When Routes are bound to a Gateway it means their collective routing rules are configured on the underlying load balancers or proxies and the Routes are accessible through the Gateway. Thus, a Gateway is a logical representation of a networking data plane that can be configured through Routes.&lt;/p>
&lt;p>&lt;img src="route-binding.png" alt="How Routes bind with Gateways">&lt;/p>
&lt;h3 id="administrative-delegation">Administrative Delegation&lt;/h3>
&lt;p>The split between Gateway and Route resources allows the cluster administrator to delegate some of the routing configuration to individual teams while still retaining centralized control. The following Gateway resource exposes HTTPS on port 443 and terminates all traffic on the port with a certificate controlled by the cluster administrator.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Gateway&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod-web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gatewayClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>acme-lb&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">listeners&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">443&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespaces&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>All&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">tls&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">certificateRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admin-controlled-cert&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The following HTTPRoute shows how the Route can ensure it matches the Gateway's selector via it’s &lt;code>kind&lt;/code> (HTTPRoute) and resource labels (&lt;code>gateway=external-https-prod&lt;/code>).&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#080;font-style:italic"># Matches the required kind selector on the Gateway&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-route&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-ns&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Matches the required label selector on the Gateway&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">...&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="role-oriented-design">Role Oriented Design&lt;/h3>
&lt;p>When you put it all together, you have a single load balancing infrastructure that can be safely shared by multiple teams. The Gateway API is not only a more expressive API for advanced routing, but is also a role-oriented API, designed for multi-tenant infrastructure. Its extensibility ensures that it will evolve for future use-cases while preserving portability. Ultimately these characteristics will allow the Gateway API to adapt to different organizational models and implementations well into the future.&lt;/p>
&lt;h3 id="try-it-out-and-get-involved">Try it out and get involved&lt;/h3>
&lt;p>There are many resources to check out to learn more.&lt;/p>
&lt;ul>
&lt;li>Check out the &lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/">user guides&lt;/a> to see what use-cases can be addressed.&lt;/li>
&lt;li>Try out one of the &lt;a href="https://gateway-api.sigs.k8s.io/references/implementations/">existing Gateway controllers &lt;/a>&lt;/li>
&lt;li>Or &lt;a href="https://gateway-api.sigs.k8s.io/contributing/community/">get involved&lt;/a> and help design and influence the future of Kubernetes service networking!&lt;/li>
&lt;/ul></description></item><item><title>Blog: Graceful Node Shutdown Goes Beta</title><link>https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> David Porter (Google), Mrunal Patel (Red Hat), and Tim Bannister (The Scale Factory)&lt;/p>
&lt;p>Graceful node shutdown, beta in 1.21, enables kubelet to gracefully evict pods during a node shutdown.&lt;/p>
&lt;p>Kubernetes is a distributed system and as such we need to be prepared for inevitable failures — nodes will fail, containers might crash or be restarted, and - ideally - your workloads will be able to withstand these catastrophic events.&lt;/p>
&lt;p>One of the common classes of issues are workload failures on node shutdown or restart. The best practice prior to bringing your node down is to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">safely drain and cordon your node&lt;/a>. This will ensure that all pods running on this node can safely be evicted. An eviction will ensure your pods can follow the expected &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">pod termination lifecycle&lt;/a> meaning receiving a SIGTERM in your container and/or running &lt;code>preStopHooks&lt;/code>.&lt;/p>
&lt;p>Prior to Kubernetes 1.20 (when graceful node shutdown was introduced as an alpha feature), safe node draining was not easy: it required users to manually take action and drain the node beforehand. If someone or something shut down your node without draining it first, most likely your pods would not be safely evicted from your node and shutdown abruptly. Other services talking to those pods might see errors due to the pods exiting abruptly. Some examples of this situation may be caused by a reboot due to security patches or preemption of short lived cloud compute instances.&lt;/p>
&lt;p>Kubernetes 1.21 brings graceful node shutdown to beta. Graceful node shutdown gives you more control over some of those unexpected shutdown situations. With graceful node shutdown, the kubelet is aware of underlying system shutdown events and can propagate these events to pods, ensuring containers can shut down as gracefully as possible. This gives the containers a chance to checkpoint their state or release back any resources they are holding.&lt;/p>
&lt;p>Note, that for the best availability, even with graceful node shutdown, you should still design your deployments to be resilient to node failures.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>On Linux, your system can shut down in many different situations. For example:&lt;/p>
&lt;ul>
&lt;li>A user or script running &lt;code>shutdown -h now&lt;/code> or &lt;code>systemctl poweroff&lt;/code> or &lt;code>systemctl reboot&lt;/code>.&lt;/li>
&lt;li>Physically pressing a power button on the machine.&lt;/li>
&lt;li>Stopping a VM instance on a cloud provider, e.g. &lt;code>gcloud compute instances stop&lt;/code> on GCP.&lt;/li>
&lt;li>A Preemptible VM or Spot Instance that your cloud provider can terminate unexpectedly, but with a brief warning.&lt;/li>
&lt;/ul>
&lt;p>Many of these situations can be unexpected and there is no guarantee that a cluster administrator drained the node prior to these events. With the graceful node shutdown feature, kubelet uses a systemd mechanism called &lt;a href="https://www.freedesktop.org/wiki/Software/systemd/inhibit">&amp;quot;Inhibitor Locks&amp;quot;&lt;/a> to allow draining in most cases. Using Inhibitor Locks, kubelet instructs systemd to postpone system shutdown for a specified duration, giving a chance for the node to drain and evict pods on the system.&lt;/p>
&lt;p>Kubelet makes use of this mechanism to ensure your pods will be terminated cleanly. When the kubelet starts, it acquires a systemd delay-type inhibitor lock. When the system is about to shut down, the kubelet can delay that shutdown for a configurable, short duration utilizing the delay-type inhibitor lock it acquired earlier. This gives your pods extra time to terminate. As a result, even during unexpected shutdowns, your application will receive a SIGTERM, &lt;a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks">preStop hooks&lt;/a> will execute, and kubelet will properly update &lt;code>Ready&lt;/code> node condition and respective pod statuses to the api-server.&lt;/p>
&lt;p>For example, on a node with graceful node shutdown enabled, you can see that the inhibitor lock is taken by the kubelet:&lt;/p>
&lt;pre>&lt;code>kubelet-node ~ # systemd-inhibit --list
Who: kubelet (UID 0/root, PID 1515/kubelet)
What: shutdown
Why: Kubelet needs time to handle node shutdown
Mode: delay
1 inhibitors listed.
&lt;/code>&lt;/pre>&lt;p>One important consideration we took when designing this feature is that not all pods are created equal. For example, some of the pods running on a node such as a logging related daemonset should stay running as long as possible to capture important logs during the shutdown itself. As a result, pods are split into two categories: &amp;quot;regular&amp;quot; and &amp;quot;critical&amp;quot;. &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">Critical pods&lt;/a> are those that have &lt;code>priorityClassName&lt;/code> set to &lt;code>system-cluster-critical&lt;/code> or &lt;code>system-node-critical&lt;/code>; all other pods are considered regular.&lt;/p>
&lt;p>In our example, the logging DaemonSet would run as a critical pod. During the graceful node shutdown, regular pods are terminated first, followed by critical pods. As an example, this would allow a critical pod associated with a logging daemonset to continue functioning, and collecting logs during the termination of regular pods.&lt;/p>
&lt;p>We will evaluate during the beta phase if we need more flexibility for different pod priority classes and add support if needed, please let us know if you have some scenarios in mind.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>Graceful node shutdown is controlled with the &lt;code>GracefulNodeShutdown&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates">feature gate&lt;/a> and is enabled by default in Kubernetes 1.21.&lt;/p>
&lt;p>You can configure the graceful node shutdown behavior using two kubelet configuration options: &lt;code>ShutdownGracePeriod&lt;/code> and &lt;code>ShutdownGracePeriodCriticalPods&lt;/code>. To configure these options, you edit the kubelet configuration file that is passed to kubelet via the &lt;code>--config&lt;/code> flag; for more details, refer to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">Set kubelet parameters via a configuration file&lt;/a>.&lt;/p>
&lt;p>During a shutdown, kubelet terminates pods in two phases. You can configure how long each of these phases lasts.&lt;/p>
&lt;ol>
&lt;li>Terminate regular pods running on the node.&lt;/li>
&lt;li>Terminate critical pods running on the node.&lt;/li>
&lt;/ol>
&lt;p>The settings that control the duration of shutdown are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ShutdownGracePeriod&lt;/code>
&lt;ul>
&lt;li>Specifies the total duration that the node should delay the shutdown by. This is the total grace period for pod termination for both regular and critical pods.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>ShutdownGracePeriodCriticalPods&lt;/code>
&lt;ul>
&lt;li>Specifies the duration used to terminate critical pods during a node shutdown. This should be less than &lt;code>ShutdownGracePeriod&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For example, if &lt;code>ShutdownGracePeriod=30s&lt;/code>, and &lt;code>ShutdownGracePeriodCriticalPods=10s&lt;/code>, kubelet will delay the node shutdown by 30 seconds. During this time, the first 20 seconds (30-10) would be reserved for gracefully terminating normal pods, and the last 10 seconds would be reserved for terminating critical pods.&lt;/p>
&lt;p>Note that by default, both configuration options described above, &lt;code>ShutdownGracePeriod&lt;/code> and &lt;code>ShutdownGracePeriodCriticalPods&lt;/code> are set to zero, so you will need to configure them as appropriate for your environment to activate graceful node shutdown functionality.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>Read the &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown">documentation&lt;/a>&lt;/li>
&lt;li>Read the enhancement proposal, &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2000-graceful-node-shutdown">KEP 2000&lt;/a>&lt;/li>
&lt;li>View the &lt;a href="https://github.com/kubernetes/kubernetes/tree/release-1.21/pkg/kubelet/nodeshutdown">code&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Your feedback is always welcome! SIG Node meets regularly and can be reached via &lt;a href="https://slack.k8s.io">Slack&lt;/a> (channel &lt;code>#sig-node&lt;/code>), or the SIG's &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">mailing list&lt;/a>&lt;/p></description></item><item><title>Blog: Annotating Kubernetes Services for Humans</title><link>https://kubernetes.io/blog/2021/04/20/annotating-k8s-for-humans/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/20/annotating-k8s-for-humans/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Richard Li, Ambassador Labs&lt;/p>
&lt;p>Have you ever been asked to troubleshoot a failing Kubernetes service and struggled to find basic information about the service such as the source repository and owner?&lt;/p>
&lt;p>One of the problems as Kubernetes applications grow is the proliferation of services. As the number of services grows, developers start to specialize working with specific services. When it comes to troubleshooting, however, developers need to be able to find the source, understand the service and dependencies, and chat with the owning team for any service.&lt;/p>
&lt;h2 id="human-service-discovery">Human service discovery&lt;/h2>
&lt;p>Troubleshooting always begins with information gathering. While much attention has been paid to centralizing machine data (e.g., logs, metrics), much less attention has been given to the human aspect of service discovery. Who owns a particular service? What Slack channel does the team work on? Where is the source for the service? What issues are currently known and being tracked?&lt;/p>
&lt;h2 id="kubernetes-annotations">Kubernetes annotations&lt;/h2>
&lt;p>Kubernetes annotations are designed to solve exactly this problem. Oft-overlooked, Kubernetes annotations are designed to add metadata to Kubernetes objects. The Kubernetes documentation says annotations can “attach arbitrary non-identifying metadata to objects.” This means that annotations should be used for attaching metadata that is external to Kubernetes (i.e., metadata that Kubernetes won’t use to identify objects. As such, annotations can contain any type of data. This is a contrast to labels, which are designed for uses internal to Kubernetes. As such, label structure and values are &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">constrained&lt;/a> so they can be efficiently used by Kubernetes.&lt;/p>
&lt;h2 id="kubernetes-annotations-in-action">Kubernetes annotations in action&lt;/h2>
&lt;p>Here is an example. Imagine you have a Kubernetes service for quoting, called the quote service. You can do the following:&lt;/p>
&lt;pre>&lt;code>kubectl annotate service quote a8r.io/owner=”@sally”
&lt;/code>&lt;/pre>&lt;p>In this example, we've just added an annotation called &lt;code>a8r.io/owner&lt;/code> with the value of @sally. Now, we can use &lt;code>kubectl describe&lt;/code> to get the information.&lt;/p>
&lt;pre>&lt;code>Name: quote
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: a8r.io/owner: @sally
Selector: app=quote
Type: ClusterIP
IP: 10.109.142.131
Port: http 80/TCP
TargetPort: 8080/TCP
Endpoints: &amp;lt;none&amp;gt;
Session Affinity: None
Events: &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;p>If you’re practicing GitOps (and you should be!) you’ll want to code these values directly into your Kubernetes manifest, e.g.,&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quote&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">a8r.io/owner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>“@sally”&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>http&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quote&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="a-convention-for-annotations">A Convention for Annotations&lt;/h2>
&lt;p>Adopting a common convention for annotations ensures consistency and understandability. Typically, you’ll want to attach the annotation to the service object, as services are the high-level resource that maps most clearly to a team’s responsibility. Namespacing your annotations is also very important. Here is one set of conventions, documented at &lt;a href="https://a8r.io">a8r.io&lt;/a>, and reproduced below:&lt;/p>
&lt;table>&lt;caption style="display: none;">Annotation convention for human-readable services&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th>Annotation&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>a8r.io/description&lt;/code>&lt;/td>
&lt;td>Unstructured text description of the service for humans.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/owner&lt;/code>&lt;/td>
&lt;td>SSO username (GitHub), email address (linked to GitHub account), or unstructured owner description.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/chat&lt;/code>&lt;/td>
&lt;td>Slack channel, or link to external chat system.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/bugs&lt;/code>&lt;/td>
&lt;td>Link to external bug tracker.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/logs&lt;/code>&lt;/td>
&lt;td>Link to external log viewer.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/documentation&lt;/code>&lt;/td>
&lt;td>Link to external project documentation.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/repository&lt;/code>&lt;/td>
&lt;td>Link to external VCS repository.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/support&lt;/code>&lt;/td>
&lt;td>Link to external support center.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/runbook&lt;/code>&lt;/td>
&lt;td>Link to external project runbook.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/incidents&lt;/code>&lt;/td>
&lt;td>Link to external incident dashboard.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/uptime&lt;/code>&lt;/td>
&lt;td>Link to external uptime dashboard.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/performance&lt;/code>&lt;/td>
&lt;td>Link to external performance dashboard.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/dependencies&lt;/code>&lt;/td>
&lt;td>Unstructured text describing the service dependencies for humans.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="visualizing-annotations-service-catalogs">Visualizing annotations: Service Catalogs&lt;/h2>
&lt;p>As the number of microservices and annotations proliferate, running &lt;code>kubectl describe&lt;/code> can get tedious. Moreover, using &lt;code>kubectl describe&lt;/code> requires every developer to have some direct access to the Kubernetes cluster. Over the past few years, service catalogs have gained greater visibility in the Kubernetes ecosystem. Popularized by tools such as &lt;a href="https://shopify.engineering/scaling-mobile-development-by-treating-apps-as-services">Shopify's ServicesDB&lt;/a> and &lt;a href="https://dzone.com/articles/modeling-microservices-at-spotify-with-petter-mari">Spotify's System Z&lt;/a>, service catalogs are internally-facing developer portals that present critical information about microservices.&lt;/p>
&lt;p>Note that these service catalogs should not be confused with the &lt;a href="https://svc-cat.io/">Kubernetes Service Catalog project&lt;/a>. Built on the Open Service Broker API, the Kubernetes Service Catalog enables Kubernetes operators to plug in different services (e.g., databases) to their cluster.&lt;/p>
&lt;h2 id="annotate-your-services-now-and-thank-yourself-later">Annotate your services now and thank yourself later&lt;/h2>
&lt;p>Much like implementing observability within microservice systems, you often don’t realize that you need human service discovery until it’s too late. Don't wait until something is on fire in production to start wishing you had implemented better metrics and also documented how to get in touch with the part of your organization that looks after it.&lt;/p>
&lt;p>There's enormous benefits to building an effective “version 0” service: a &lt;a href="https://containerjournal.com/topics/container-management/dancing-skeleton-apis-and-microservices/">&lt;em>dancing skeleton&lt;/em>&lt;/a> application with a thin slice of complete functionality that can be deployed to production with a minimal yet effective continuous delivery pipeline.&lt;/p>
&lt;p>Adding service annotations should be an essential part of your “version 0” for all of your services. Add them now, and you’ll thank yourself later.&lt;/p></description></item><item><title>Blog: Defining Network Policy Conformance for Container Network Interface (CNI) providers</title><link>https://kubernetes.io/blog/2021/04/20/defining-networkpolicy-conformance-cni-providers/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/20/defining-networkpolicy-conformance-cni-providers/</guid><description>
&lt;p>Authors: Matt Fenwick (Synopsys), Jay Vyas (VMWare), Ricardo Katz, Amim Knabben (Loadsmart), Douglas Schilling Landgraf (Red Hat), Christopher Tomkins (Tigera)&lt;/p>
&lt;p>Special thanks to Tim Hockin and Bowie Du (Google), Dan Winship and Antonio Ojea (Red Hat),
Casey Davenport and Shaun Crampton (Tigera), and Abhishek Raut and Antonin Bas (VMware) for
being supportive of this work, and working with us to resolve issues in different Container Network Interfaces (CNIs) over time.&lt;/p>
&lt;p>A brief conversation around &amp;quot;node local&amp;quot; Network Policies in April of 2020 inspired the creation of a NetworkPolicy subproject from SIG Network. It became clear that as a community,
we need a rock-solid story around how to do pod network security on Kubernetes, and this story needed a community around it, so as to grow the cultural adoption of enterprise security patterns in K8s.&lt;/p>
&lt;p>In this post we'll discuss:&lt;/p>
&lt;ul>
&lt;li>Why we created a subproject for &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Network Policies&lt;/a>&lt;/li>
&lt;li>How we changed the Kubernetes e2e framework to &lt;code>visualize&lt;/code> NetworkPolicy implementation of your CNI provider&lt;/li>
&lt;li>The initial results of our comprehensive NetworkPolicy conformance validator, &lt;em>Cyclonus&lt;/em>, built around these principles&lt;/li>
&lt;li>Improvements subproject contributors have made to the NetworkPolicy user experience&lt;/li>
&lt;/ul>
&lt;h2 id="why-we-created-a-subproject-for-networkpolicies">Why we created a subproject for NetworkPolicies&lt;/h2>
&lt;p>In April of 2020 it was becoming clear that many CNIs were emerging, and many vendors
implement these CNIs in subtly different ways. Users were beginning to express a little bit
of confusion around how to implement policies for different scenarios, and asking for new features.
It was clear that we needed to begin unifying the way we think about Network Policies
in Kubernetes, to avoid API fragmentation and unnecessary complexity.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;ul>
&lt;li>In order to be flexible to the user’s environment, Calico as a CNI provider can be run using IPIP or VXLAN mode, or without encapsulation overhead. CNIs such as Antrea
and Cilium offer similar configuration options as well.&lt;/li>
&lt;li>Some CNI plugins offer iptables for NetworkPolicies amongst other options, whereas other CNIs use a completely
different technology stack (for example, the Antrea project uses Open vSwitch rules).&lt;/li>
&lt;li>Some CNI plugins only implement a subset of the Kubernetes NetworkPolicy API, and some a superset. For example, certain plugins don't support the
ability to target a named port; others don't work with certain IP address types, and there are diverging semantics for similar policy types.&lt;/li>
&lt;li>Some CNI plugins combine with OTHER CNI plugins in order to implement NetworkPolicies (canal), some CNI's might mix implementations (multus), and some clouds do routing separately from NetworkPolicy implementation.&lt;/li>
&lt;/ul>
&lt;p>Although this complexity is to some extent necessary to support different environments, end-users find that they need to follow a multistep process to implement Network Policies to secure their applications:&lt;/p>
&lt;ul>
&lt;li>Confirm that their network plugin supports NetworkPolicies (some don't, such as Flannel)&lt;/li>
&lt;li>Confirm that their cluster's network plugin supports the specific NetworkPolicy features that they are interested in (again, the named port or port range examples come to mind here)&lt;/li>
&lt;li>Confirm that their application's Network Policy definitions are doing the right thing&lt;/li>
&lt;li>Find out the nuances of a vendor's implementation of policy, and check whether or not that implementation has a CNI neutral implementation (which is sometimes adequate for users)&lt;/li>
&lt;/ul>
&lt;p>The NetworkPolicy project in upstream Kubernetes aims at providing a community where
people can learn about, and contribute to, the Kubernetes NetworkPolicy API and the surrounding ecosystem.&lt;/p>
&lt;h2 id="the-first-step-a-validation-framework-for-networkpolicies-that-was-intuitive-to-use-and-understand">The First step: A validation framework for NetworkPolicies that was intuitive to use and understand&lt;/h2>
&lt;p>The Kubernetes end to end suite has always had NetworkPolicy tests, but these weren't
run in CI, and the way they were implemented didn't provide holistic, easily consumable
information about how a policy was working in a cluster.
This is because the original tests didn't provide any kind of visual summary of connectivity
across a cluster. We thus initially set out to make it easy to confirm CNI support for NetworkPolicies by
making the end to end tests (which are often used by administrators or users to diagnose cluster conformance) easy to interpret.&lt;/p>
&lt;p>To solve the problem of confirming that CNIs support the basic features most users care about
for a policy, we built a new NetworkPolicy validation tool into the Kubernetes e2e
framework which allows for visual inspection of policies and their effect on a standard set of pods in a cluster.
For example, take the following test output. We found a bug in
&lt;a href="https://github.com/ovn-org/ovn-kubernetes/issues/1782">OVN Kubernetes&lt;/a>. This bug has now been resolved. With this tool the bug was really
easy to characterize, wherein certain policies caused a state-modification that,
later on, caused traffic to incorrectly be blocked (even after all Network Policies were deleted from the cluster).&lt;/p>
&lt;p>This is the network policy for the test in question:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">creationTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">null&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>allow-ingress-port-80&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>serve-80-tcp&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">podSelector&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{}&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>These are the expected connectivity results. The test setup is 9 pods (3 namespaces: x, y, and z;
and 3 pods in each namespace: a, b, and c); each pod runs a server on the same port and protocol
that can be reached through HTTP calls in the absence of network policies. Connectivity is verified
by using the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/test/images/agnhost">agnhost&lt;/a> network utility to issue HTTP calls on a port and protocol that other pods are
expected to be serving. A test scenario first
runs a connectivity check to ensure that each pod can reach each other pod, for 81 (= 9 x 9) data
points. This is the &amp;quot;control&amp;quot;. Then perturbations are applied, depending on the test scenario:
policies are created, updated, and deleted; labels are added and removed from pods and namespaces,
and so on. After each change, the connectivity matrix is recollected and compared to the expected
connectivity.&lt;/p>
&lt;p>These results give a visual indication of connectivity in a simple matrix. Going down the leftmost column is the &amp;quot;source&amp;quot;
pod, or the pod issuing the request; going across the topmost row is the &amp;quot;destination&amp;quot; pod, or the pod
receiving the request. A &lt;code>.&lt;/code> means that the connection was allowed; an &lt;code>X&lt;/code> means the connection was
blocked. For example:&lt;/p>
&lt;pre>&lt;code>Nov 4 16:58:43.449: INFO: expected:
- x/a x/b x/c y/a y/b y/c z/a z/b z/c
x/a . . . . . . . . .
x/b . . . . . . . . .
x/c . . . . . . . . .
y/a . . . . . . . . .
y/b . . . . . . . . .
y/c . . . . . . . . .
z/a . . . . . . . . .
z/b . . . . . . . . .
z/c . . . . . . . . .
&lt;/code>&lt;/pre>&lt;p>Below are the observed connectivity results in the case of the OVN Kubernetes bug. Notice how the top three rows indicate that
all requests from namespace x regardless of pod and destination were blocked. Since these
experimental results do not match the expected results, a failure will be reported. Note
how the specific pattern of failure provides clear insight into the nature of the problem --
since all requests from a specific namespace fail, we have a clear clue to start our
investigation.&lt;/p>
&lt;pre>&lt;code>Nov 4 16:58:43.449: INFO: observed:
- x/a x/b x/c y/a y/b y/c z/a z/b z/c
x/a X X X X X X X X X
x/b X X X X X X X X X
x/c X X X X X X X X X
y/a . . . . . . . . .
y/b . . . . . . . . .
y/c . . . . . . . . .
z/a . . . . . . . . .
z/b . . . . . . . . .
z/c . . . . . . . . .
&lt;/code>&lt;/pre>&lt;p>This was one of our earliest wins in the Network Policy group, as we were able to
identify and work with the OVN Kubernetes group to fix a bug in egress policy processing.&lt;/p>
&lt;p>However, even though this tool has made it easy to validate roughly 30 common scenarios,
it doesn't validate &lt;em>all&lt;/em> Network Policy scenarios - because there are an enormous number of possible
permutations that one might create (technically, we might say this number is
infinite given that there's an infinite number of possible namespace/pod/port/protocol variations one can create).&lt;/p>
&lt;p>Once these tests were in play, we worked with the Upstream SIG Network and SIG Testing communities
(thanks to Antonio Ojea and Ben Elder) to put a testgrid Network Policy job in place. This job
continuously runs the entire suite of Network Policy tests against
&lt;a href="https://testgrid.k8s.io/sig-network-gce#presubmit-network-policies,%20google-gce">GCE with Calico as a Network Policy provider&lt;/a>.&lt;/p>
&lt;p>Part of our role as a subproject is to help make sure that, when these tests break, we can help triage them effectively.&lt;/p>
&lt;h2 id="cyclonus">Cyclonus: The next step towards Network Policy conformance&lt;/h2>
&lt;p>Around the time that we were finishing the validation work, it became clear from the community that,
in general, we needed to solve the overall problem of testing ALL possible Network Policy implementations.
For example, a KEP was recently written which introduced the concept of micro versioning to
Network Policies to accommodate &lt;a href="https://github.com/kubernetes/enhancements/pull/2137/files">describing this at the API level&lt;/a>, by Dan Winship.&lt;/p>
&lt;p>In response to this increasingly obvious need to comprehensively evaluate Network
Policy implementations from all vendors, Matt Fenwick decided to evolve our approach to Network Policy validation again by creating Cyclonus.&lt;/p>
&lt;p>Cyclonus is a comprehensive Network Policy fuzzing tool which verifies a CNI provider
against hundreds of different Network Policy scenarios, by defining similar truth table/policy
combinations as demonstrated in the end to end tests, while also providing a hierarchical
representation of policy &amp;quot;categories&amp;quot;. We've found some interesting nuances and issues
in almost every CNI we've tested so far, and have even contributed some fixes back.&lt;/p>
&lt;p>To perform a Cyclonus validation run, you create a Job manifest similar to:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ./cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- generate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --perturbation-wait-seconds=15&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --server-protocol=tcp,udp&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>IfNotPresent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mfenwick100/cyclonus:latest&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">serviceAccount&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Cyclonus outputs a report of all the test cases it will run:&lt;/p>
&lt;pre>&lt;code>test cases to run by tag:
- target: 6
- peer-ipblock: 4
- udp: 16
- delete-pod: 1
- conflict: 16
- multi-port/protocol: 14
- ingress: 51
- all-pods: 14
- egress: 51
- all-namespaces: 10
- sctp: 10
- port: 56
- miscellaneous: 22
- direction: 100
- multi-peer: 0
- any-port-protocol: 2
- set-namespace-labels: 1
- upstream-e2e: 0
- allow-all: 6
- namespaces-by-label: 6
- deny-all: 10
- pathological: 6
- action: 6
- rule: 30
- policy-namespace: 4
- example: 0
- tcp: 16
- target-namespace: 3
- named-port: 24
- update-policy: 1
- any-peer: 2
- target-pod-selector: 3
- IP-block-with-except: 2
- pods-by-label: 6
- numbered-port: 28
- protocol: 42
- peer-pods: 20
- create-policy: 2
- policy-stack: 0
- any-port: 14
- delete-namespace: 1
- delete-policy: 1
- create-pod: 1
- IP-block-no-except: 2
- create-namespace: 1
- set-pod-labels: 1
testing 112 cases
&lt;/code>&lt;/pre>&lt;p>Note that Cyclonus tags its tests based on the type of policy being created, because
the policies themselves are auto-generated, and thus have no meaningful names to be recognized by.&lt;/p>
&lt;p>For each test, Cyclonus outputs a truth table, which is again similar to that of the
E2E tests, along with the policy being validated:&lt;/p>
&lt;pre>&lt;code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
creationTimestamp: null
name: base
namespace: x
spec:
egress:
- ports:
- port: 81
to:
- namespaceSelector:
matchExpressions:
- key: ns
operator: In
values:
- &amp;quot;y&amp;quot;
- z
podSelector:
matchExpressions:
- key: pod
operator: In
values:
- a
- b
- ports:
- port: 53
protocol: UDP
ingress:
- from:
- namespaceSelector:
matchExpressions:
- key: ns
operator: In
values:
- x
- &amp;quot;y&amp;quot;
podSelector:
matchExpressions:
- key: pod
operator: In
values:
- b
- c
ports:
- port: 80
protocol: TCP
podSelector:
matchLabels:
pod: a
policyTypes:
- Ingress
- Egress
0 wrong, 0 ignored, 81 correct
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| TCP/80 | X/A | X/B | X/C | Y/A | Y/B | Y/C | Z/A | Z/B | Z/C |
| TCP/81 | | | | | | | | | |
| UDP/80 | | | | | | | | | |
| UDP/81 | | | | | | | | | |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/a | X | X | X | X | X | X | X | X | X |
| | X | X | X | . | . | X | . | . | X |
| | X | X | X | X | X | X | X | X | X |
| | X | X | X | X | X | X | X | X | X |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/b | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/c | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/a | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/b | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/c | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/a | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/b | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/c | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
&lt;/code>&lt;/pre>&lt;p>Both Cyclonus and the e2e tests use the same strategy to validate a Network Policy - probing pods over TCP or UDP, with
SCTP support available as well for CNIs that support it (such as Calico).&lt;/p>
&lt;p>As examples of how we use Cyclonus to help make CNI implementations better from a Network Policy perspective, you can see the following issues:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/vmware-tanzu/antrea/issues/1764">Antrea: NetworkPolicy: unable to allow ingress by CIDR&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/projectcalico/libcalico-go/pull/1373">Calico: default missing protocol to TCP; don't let single port overwrite all ports&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cilium/cilium/issues/14678">Cilium: Egress Network Policy allows traffic that should be denied&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The good news is that Antrea and Calico have already merged fixes for all the issues found and other CNI providers are working on it,
with the support of SIG Network and the Network Policy subproject.&lt;/p>
&lt;p>Are you interested in verifying NetworkPolicy functionality on your cluster?
(if you care about security or offer multi-tenant SaaS, you should be)
If so, you can run the upstream end to end tests, or Cyclonus, or both.&lt;/p>
&lt;ul>
&lt;li>If you're just getting started with NetworkPolicies and want to simply
verify the &amp;quot;common&amp;quot; NetworkPolicy cases that most CNIs should be
implementing correctly, in a way that is quick to diagnose, then you're
better off running the e2e tests only.&lt;/li>
&lt;li>If you are deeply curious about your CNI provider's NetworkPolicy
implementation, and want to verify it: use Cyclonus.&lt;/li>
&lt;li>If you want to test &lt;em>hundreds&lt;/em> of policies, and evaluate your CNI plugin
for comprehensive functionality, for deep discovery of potential security
holes: use Cyclonus, and also consider running end-to-end cluster tests.&lt;/li>
&lt;li>If you're thinking of getting involved with the upstream NetworkPolicy efforts:
use Cyclonus, and read at least an outline of which e2e tests are relevant.&lt;/li>
&lt;/ul>
&lt;h2 id="where-to-start-with-networkpolicy-testing">Where to start with NetworkPolicy testing?&lt;/h2>
&lt;ul>
&lt;li>Cyclonus is easy to run on your cluster, check out the &lt;a href="https://github.com/mattfenwick/cyclonus#run-as-a-kubernetes-job">instructions on github&lt;/a>,
and determine whether &lt;em>your&lt;/em> specific CNI configuration is fully conformant to the hundreds of different
Kubernetes Network Policy API constructs.&lt;/li>
&lt;li>Alternatively, you can use a tool like &lt;a href="https://github.com/vmware-tanzu/sonobuoy">sonobuoy&lt;/a>
to run the existing E2E tests in Kubernetes, with the &lt;code>--ginkgo.focus=NetworkPolicy&lt;/code> flag.
Make sure that you use the K8s conformance image for K8s 1.21 or above (for example, by using the &lt;code>--kube-conformance-image-version v1.21.0&lt;/code> flag),
as older images will not have the &lt;em>new&lt;/em> Network Policy tests in them.&lt;/li>
&lt;/ul>
&lt;h2 id="improvements-to-the-networkpolicy-api-and-user-experience">Improvements to the NetworkPolicy API and user experience&lt;/h2>
&lt;p>In addition to cleaning up the validation story for CNI plugins that implement NetworkPolicies,
subproject contributors have also spent some time improving the Kubernetes NetworkPolicy API for a few commonly requested features.
After months of deliberation, we eventually settled on a few core areas for improvement:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Port Range policies: We now allow you to specify a &lt;em>range&lt;/em> of ports for a policy.
This allows users interested in scenarios like FTP or virtualization to enable advanced policies.
The port range option for network policies will be available to use in Kubernetes 1.21.
Read more in &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports">targeting a range of ports&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Namespace as name policies: Allowing users in Kubernetes &amp;gt;= 1.21 to target namespaces using names,
when building Network Policy objects. This was done in collaboration with Jordan Liggitt and Tim Hockin on the API Machinery side.
This change allowed us to improve the Network Policy user experience without actually
changing the API! For more details, you can read
&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#automatic-labelling">Automatic labelling&lt;/a> in the page about Namespaces.
The TL,DR; is that for Kubernetes 1.21 and later, &lt;strong>all namespaces&lt;/strong> have the following label added by default:&lt;/p>
&lt;pre>&lt;code>kubernetes.io/metadata.name: &amp;lt;name-of-namespace&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>This means you can write a namespace policy against this namespace, even if you can't edit its labels.
For example, this policy, will 'just work', without needing to run a command such as &lt;code>kubectl edit namespace&lt;/code>.
In fact, it will even work if you can't edit or view this namespace's data at all, because of the magic of API server defaulting.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NetworkPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-network-policy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">podSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">role&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>db&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">policyTypes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- Ingress&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Allow inbound traffic to Pods labelled role=db, in the namespace &amp;#39;default&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># provided that the source is a Pod in the namespace &amp;#39;my-namespace&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubernetes.io/metadata.name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-namespace&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="results">Results&lt;/h2>
&lt;p>In our tests, we found that:&lt;/p>
&lt;ul>
&lt;li>Antrea and Calico are at a point where they support all of cyclonus's scenarios, modulo a few very minor tweaks which we've made.&lt;/li>
&lt;li>Cilium also conformed to the majority of the policies, outside known features that aren't fully supported (for example, related to the way Cilium deals with pod CIDR policies).&lt;/li>
&lt;/ul>
&lt;p>If you are a CNI provider and interested in helping us to do a better job curating large tests of network policies, please reach out! We are continuing to curate the Network Policy conformance results from Cyclonus &lt;a href="https://raw.githubusercontent.com/K8sbykeshed/cyclonus-artifacts/">here&lt;/a>, but
we are not capable of maintaining all of the subtleties in NetworkPolicy testing data on our own. For now, we use github actions and Kind to test in CI.&lt;/p>
&lt;h2 id="the-future">The Future&lt;/h2>
&lt;p>We're also working on some improvements for the future of Network Policies, including:&lt;/p>
&lt;ul>
&lt;li>Fully qualified Domain policies: The Google Cloud team created a prototype (which
we are really excited about) of &lt;a href="https://github.com/GoogleCloudPlatform/gke-fqdnnetworkpolicies-golang">FQDN policies&lt;/a>.
This tool uses the Network Policy API to enforce policies against L7 URLs, by finding
their IPs and blocking them proactively when requests are made.&lt;/li>
&lt;li>Cluster Administrative policies: We're working hard at enabling &lt;em>administrative&lt;/em> or
&lt;em>cluster scoped&lt;/em> Network Policies for the future. These are being presented iteratively to the NetworkPolicy subproject.
You can read about them here in &lt;a href="https://docs.google.com/presentation/d/1Jk86jtS3TcGAugVSM_I4Yds5ukXFJ4F1ZCvxN5v2BaY/">Cluster Scoped Network Policy&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The Network Policy subproject meets on mondays at 4PM EST. For details, check out the
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network community repo&lt;/a>. We'd love
to hang out with you, hack on stuff, and help you adopt K8s Network Policies for your cluster wherever possible.&lt;/p>
&lt;h3 id="a-quick-note-on-user-feedback">A quick note on User Feedback&lt;/h3>
&lt;p>We've gotten a lot of ideas and feedback from users on Network Policies. A lot of people have interesting ideas about Network Policies,
but we've found that as a subproject, very few people were deeply interested in implementing these ideas to the full extent.&lt;/p>
&lt;p>Almost every change to the NetworkPolicy API includes weeks or months of discussion to cover different cases, and ensure no CVEs are being introduced. Thus, long term ownership
is the biggest impediment in improving the NetworkPolicy user experience for us, over time.&lt;/p>
&lt;ul>
&lt;li>We've documented a lot of the history of the Network Policy dialogue &lt;a href="https://github.com/jayunit100/network-policy-subproject/blob/master/history.md">here&lt;/a>.&lt;/li>
&lt;li>We've also taken a poll of users, for what they'd like to see in the Network Policy API &lt;a href="https://github.com/jayunit100/network-policy-subproject/blob/master/p0_user_stories.md">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>We encourage anyone to provide us with feedback, but our most pressing issues right now
involve finding &lt;em>long term owners to help us drive changes&lt;/em>.&lt;/p>
&lt;p>This doesn't require a lot of technical knowledge, but rather, just a long term commitment to helping us stay organized, do paperwork,
and iterate through the many stages of the K8s feature process. If you want to help us and get involved, please reach out on the SIG Network mailing list, or in the SIG Network room in the k8s.io slack channel!&lt;/p>
&lt;p>Anyone can put an oar in the water and help make NetworkPolices better!&lt;/p></description></item><item><title>Blog: Introducing Indexed Jobs</title><link>https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Aldo Culquicondor (Google)&lt;/p>
&lt;p>Once you have containerized a non-parallel &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job&lt;/a>,
it is quite easy to get it up and running on Kubernetes without modifications to
the binary. In most cases, when running parallel distributed Jobs, you had
to set a separate system to partition the work among the workers. For
example, you could set up a task queue to &lt;a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">assign one work item to each
Pod&lt;/a> or &lt;a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">multiple items
to each Pod until the queue is emptied&lt;/a>.&lt;/p>
&lt;p>The Kubernetes 1.21 release introduces a new field to control Job &lt;em>completion mode&lt;/em>,
a configuration option that allows you to control how Pod completions affect the
overall progress of a Job, with two possible options (for now):&lt;/p>
&lt;ul>
&lt;li>&lt;code>NonIndexed&lt;/code> (default): the Job is considered complete when there has been
a number of successfully completed Pods equal to the specified number in
&lt;code>.spec.completions&lt;/code>. In other words, each Pod completion is homologous to
each other. Any Job you might have created before the introduction of
completion modes is implicitly NonIndexed.&lt;/li>
&lt;li>&lt;code>Indexed&lt;/code>: the Job is considered complete when there is one successfully
completed Pod associated with each index from 0 to &lt;code>.spec.completions-1&lt;/code>. The
index is exposed to each Pod in the &lt;code>batch.kubernetes.io/job-completion-index&lt;/code>
annotation and the &lt;code>JOB_COMPLETION_INDEX&lt;/code> environment variable.&lt;/li>
&lt;/ul>
&lt;p>You can start using Jobs with Indexed completion mode, or Indexed Jobs, for
short, to easily start parallel Jobs. Then, each worker Pod can have a statically
assigned partition of the data based on the index. This saves you from having to
set up a queuing system or even having to modify your binary!&lt;/p>
&lt;h2 id="creating-an-indexed-job">Creating an Indexed Job&lt;/h2>
&lt;p>To create an Indexed Job, you just have to add &lt;code>completionMode: Indexed&lt;/code> to the
Job spec and make use of the &lt;code>JOB_COMPLETION_INDEX&lt;/code> environment variable.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;sample-job&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parallelism&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completionMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Indexed&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#39;bash&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#39;-c&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#39;echo &amp;#34;My partition: ${JOB_COMPLETION_INDEX}&amp;#34;&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;docker.io/library/bash&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;sample-load&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that completion mode is an alpha feature in the 1.21 release. To be able to
use it in your cluster, make sure to enable the &lt;code>IndexedJob&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature
gate&lt;/a> on the
&lt;a href="docs/reference/command-line-tools-reference/kube-apiserver/">API server&lt;/a> and
the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">controller manager&lt;/a>.&lt;/p>
&lt;p>When you run the example, you will see that each of the three created Pods gets a
different completion index. For the user's convenience, the control plane sets the
&lt;code>JOB_COMPLETION_INDEX&lt;/code> environment variable, but you can choose to &lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">set your
own&lt;/a>
or &lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">expose the index as a file&lt;/a>.&lt;/p>
&lt;p>See &lt;a href="https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job for parallel processing with static work
assignment&lt;/a> for a
step-by-step guide, and a few more examples.&lt;/p>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>SIG Apps envisions that there might be more completion modes that enable more
use cases for the Job API. We welcome you to open issues in
&lt;a href="https://github.com/kubernetes/kubernetes">kubernetes/kubernetes&lt;/a> with your
suggestions.&lt;/p>
&lt;p>In particular, we are considering an &lt;code>IndexedAndUnique&lt;/code> mode where the indexes
are not just available as annotation, but they are part of the Pod names,
similar to &lt;a class='glossary-tooltip' title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet&lt;/a>.
This should facilitate inter-Pod communication for tightly coupled Pods.
You can join the discussion in the &lt;a href="https://github.com/kubernetes/kubernetes/issues/99497">open issue&lt;/a>.&lt;/p>
&lt;h2 id="wrap-up">Wrap-up&lt;/h2>
&lt;p>Indexed Jobs allows you to statically partition work among the workers of your
parallel Jobs. SIG Apps hopes that this feature facilitates the migration of
more batch workloads to Kubernetes.&lt;/p></description></item><item><title>Blog: Volume Health Monitoring Alpha Update</title><link>https://kubernetes.io/blog/2021/04/16/volume-health-monitoring-alpha-update/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/16/volume-health-monitoring-alpha-update/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Xing Yang (VMware)&lt;/p>
&lt;p>The CSI Volume Health Monitoring feature, originally introduced in 1.19 has undergone a large update for the 1.21 release.&lt;/p>
&lt;h2 id="why-add-volume-health-monitoring-to-kubernetes">Why add Volume Health Monitoring to Kubernetes?&lt;/h2>
&lt;p>Without Volume Health Monitoring, Kubernetes has no knowledge of the state of the underlying volumes of a storage system after a PVC is provisioned and used by a Pod. Many things could happen to the underlying storage system after a volume is provisioned in Kubernetes. For example, the volume could be deleted by accident outside of Kubernetes, the disk that the volume resides on could fail, it could be out of capacity, the disk may be degraded which affects its performance, and so on. Even when the volume is mounted on a pod and used by an application, there could be problems later on such as read/write I/O errors, file system corruption, accidental unmounting of the volume outside of Kubernetes, etc. It is very hard to debug and detect root causes when something happened like this.&lt;/p>
&lt;p>Volume health monitoring can be very beneficial to Kubernetes users. It can communicate with the CSI driver to retrieve errors detected by the underlying storage system. PVC events can be reported up to the user to take action. For example, if the volume is out of capacity, they could request a volume expansion to get more space.&lt;/p>
&lt;h2 id="what-is-volume-health-monitoring">What is Volume Health Monitoring?&lt;/h2>
&lt;p>CSI Volume Health Monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on PVCs or Pods.&lt;/p>
&lt;p>The Kubernetes components that monitor the volumes and report events with volume health information include the following:&lt;/p>
&lt;ul>
&lt;li>Kubelet, in addition to gathering the existing volume stats will watch the volume health of the PVCs on that node. If a PVC has an abnormal health condition, an event will be reported on the pod object using the PVC. If multiple pods are using the same PVC, events will be reported on all pods using that PVC.&lt;/li>
&lt;li>An &lt;a href="https://github.com/kubernetes-csi/external-health-monitor">External Volume Health Monitor Controller&lt;/a> watches volume health of the PVCs and reports events on the PVCs.&lt;/li>
&lt;/ul>
&lt;p>Note that the node side volume health monitoring logic was an external agent when this feature was first introduced in the Kubernetes 1.19 release. In Kubernetes 1.21, the node side volume health monitoring logic was moved from the external agent into the Kubelet, to avoid making duplicate CSI function calls. With this change in 1.21, a new alpha &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> &lt;code>CSIVolumeHealth&lt;/code> was introduced for the volume health monitoring logic in Kubelet.&lt;/p>
&lt;p>Currently the Volume Health Monitoring feature is informational only as it only reports abnormal volume health events on PVCs or Pods. Users will need to check these events and manually fix the problems. This feature serves as a stepping stone towards programmatic detection and resolution of volume health issues by Kubernetes in the future.&lt;/p>
&lt;h2 id="how-do-i-use-volume-health-on-my-kubernetes-cluster">How do I use Volume Health on my Kubernetes Cluster?&lt;/h2>
&lt;p>To use the Volume Health feature, first make sure the CSI driver you are using supports this feature. Refer to this &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI drivers doc&lt;/a> to find out which CSI drivers support this feature.&lt;/p>
&lt;p>To enable Volume Health Monitoring from the node side, the alpha feature gate &lt;code>CSIVolumeHealth&lt;/code> needs to be enabled.&lt;/p>
&lt;p>If a CSI driver supports the Volume Health Monitoring feature from the controller side, events regarding abnormal volume conditions will be recorded on PVCs.&lt;/p>
&lt;p>If a CSI driver supports the Volume Health Monitoring feature from the controller side, user can also get events regarding node failures if the &lt;code>enable-node-watcher&lt;/code> flag is set to true when deploying the External Health Monitor Controller. When a node failure event is detected, an event will be reported on the PVC to indicate that pods using this PVC are on a failed node.&lt;/p>
&lt;p>If a CSI driver supports the Volume Health Monitoring feature from the node side, events regarding abnormal volume conditions will be recorded on pods using the PVCs.&lt;/p>
&lt;h2 id="as-a-storage-vendor-how-do-i-add-support-for-volume-health-to-my-csi-driver">As a storage vendor, how do I add support for volume health to my CSI driver?&lt;/h2>
&lt;p>Volume Health Monitoring includes two parts:&lt;/p>
&lt;ul>
&lt;li>An External Volume Health Monitoring Controller monitors volume health from the controller side.&lt;/li>
&lt;li>Kubelet monitors volume health from the node side.&lt;/li>
&lt;/ul>
&lt;p>For details, see the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI spec&lt;/a> and the &lt;a href="https://kubernetes-csi.github.io/docs/volume-health-monitor.html">Kubernetes-CSI Driver Developer Guide&lt;/a>.&lt;/p>
&lt;p>There is a sample implementation for volume health in &lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path">CSI host path driver&lt;/a>.&lt;/p>
&lt;h3 id="controller-side-volume-health-monitoring">Controller Side Volume Health Monitoring&lt;/h3>
&lt;p>To learn how to deploy the External Volume Health Monitoring controller, see &lt;a href="https://kubernetes-csi.github.io/docs/external-health-monitor-controller.html">CSI external-health-monitor-controller&lt;/a> in the CSI documentation.&lt;/p>
&lt;p>The External Health Monitor Controller calls either &lt;code>ListVolumes&lt;/code> or &lt;code>ControllerGetVolume&lt;/code> CSI RPC and reports VolumeConditionAbnormal events with messages on PVCs if abnormal volume conditions are detected. Only CSI drivers with &lt;code>LIST_VOLUMES&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capability or &lt;code>GET_VOLUME&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capability support Volume Health Monitoring in the external controller.&lt;/p>
&lt;p>To implement the volume health feature from the controller side, a CSI driver &lt;strong>must&lt;/strong> add support for the new controller capabilities.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>LIST_VOLUMES&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capabilities, it &lt;strong>must&lt;/strong> implement controller RPC &lt;code>ListVolumes&lt;/code> and report the volume condition in the response.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>GET_VOLUME&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capability, it &lt;strong>must&lt;/strong> implement controller PRC &lt;code>ControllerGetVolume&lt;/code> and report the volume condition in the response.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>LIST_VOLUMES&lt;/code>, &lt;code>GET_VOLUME&lt;/code>, and &lt;code>VOLUME_CONDITION&lt;/code> controller capabilities, only &lt;code>ListVolumes&lt;/code> CSI RPC will be invoked by the External Health Monitor Controller.&lt;/p>
&lt;h3 id="node-side-volume-health-monitoring">Node Side Volume Health Monitoring&lt;/h3>
&lt;p>Kubelet calls &lt;code>NodeGetVolumeStats&lt;/code> CSI RPC and reports VolumeConditionAbnormal events with messages on Pods if abnormal volume conditions are detected. Only CSI drivers with &lt;code>VOLUME_CONDITION&lt;/code> node capability support Volume Health Monitoring in Kubelet.&lt;/p>
&lt;p>To implement the volume health feature from the node side, a CSI driver &lt;strong>must&lt;/strong> add support for the new node capabilities.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>VOLUME_CONDITION&lt;/code> node capability, it &lt;strong>must&lt;/strong> report the volume condition in node RPC &lt;code>NodeGetVoumeStats&lt;/code>.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI volume health implementation to beta in either 1.22 or 1.23.&lt;/p>
&lt;p>We are also exploring how to use volume health information for programmatic detection and automatic reconcile in Kubernetes.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>To learn the design details for Volume Health Monitoring, read the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor">Volume Health Monitor&lt;/a> enhancement proposal.&lt;/p>
&lt;p>The Volume Health Monitor controller source code is at &lt;a href="https://github.com/kubernetes-csi/external-health-monitor">https://github.com/kubernetes-csi/external-health-monitor&lt;/a>.&lt;/p>
&lt;p>There are also more details about volume health checks in the &lt;a href="https://kubernetes-csi.github.io/docs/">Container Storage Interface Documentation&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The &lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes Slack channel #csi&lt;/a> and any of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">standard SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and the CSI team.&lt;/p>
&lt;p>We offer a huge thank you to the contributors who helped release this feature in 1.21. We want to thank Yuquan Ren (&lt;a href="https://github.com/nickrenren">NickrenREN&lt;/a>) who implemented the initial volume health monitor controller and agent in the external health monitor repo, thank Ran Xu (&lt;a href="https://github.com/fengzixu">fengzixu&lt;/a>) who moved the volume health monitoring logic from the external agent to Kubelet in 1.21, and we offer special thanks to the following people for their insightful reviews: David Ashpole (&lt;a href="https://github.com/dashpole">dashpole&lt;/a>), Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>), David Eads (&lt;a href="https://github.com/deads2k">deads2k&lt;/a>), Elana Hashman (&lt;a href="https://github.com/ehashman">ehashman&lt;/a>), Seth Jennings (&lt;a href="https://github.com/sjenning">sjenning&lt;/a>), and Jiawei Wang (&lt;a href="https://github.com/Jiawei0227">Jiawei0227&lt;/a>).&lt;/p>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Three Tenancy Models For Kubernetes</title><link>https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ryan Bezdicek (Medtronic), Jim Bugwadia (Nirmata), Tasha Drew (VMware), Fei Guo (Alibaba), Adrian Ludwin (Google)&lt;/p>
&lt;p>Kubernetes clusters are typically used by several teams in an organization. In other cases, Kubernetes may be used to deliver applications to end users requiring segmentation and isolation of resources across users from different organizations. Secure sharing of Kubernetes control plane and worker node resources allows maximizing productivity and saving costs in both cases.&lt;/p>
&lt;p>The Kubernetes Multi-Tenancy Working Group is chartered with defining tenancy models for Kubernetes and making it easier to operationalize tenancy related use cases. This blog post, from the working group members, describes three common tenancy models and introduces related working group projects.&lt;/p>
&lt;p>We will also be presenting on this content and discussing different use cases at our Kubecon EU 2021 panel session, &lt;a href="https://sched.co/iE66">Multi-tenancy vs. Multi-cluster: When Should you Use What?&lt;/a>.&lt;/p>
&lt;h2 id="namespaces-as-a-service">Namespaces as a Service&lt;/h2>
&lt;p>With the &lt;em>namespaces-as-a-service&lt;/em> model, tenants share a cluster and tenant workloads are restricted to a set of Namespaces assigned to the tenant. The cluster control plane resources like the API server and scheduler, and worker node resources like CPU, memory, etc. are available for use across all tenants.&lt;/p>
&lt;p>To isolate tenant workloads, each namespace must also contain:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding">role bindings&lt;/a>:&lt;/strong> for controlling access to the namespace&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies&lt;/a>:&lt;/strong> to prevent network traffic across tenants&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quotas&lt;/a>:&lt;/strong> to limit usage and ensure fairness across tenants&lt;/li>
&lt;/ul>
&lt;p>With this model, tenants share cluster-wide resources like ClusterRoles and CustomResourceDefinitions (CRDs) and hence cannot create or update these cluster-wide resources.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/">Hierarchical Namespace Controller (HNC)&lt;/a> project makes it easier to manage namespace based tenancy by allowing users to create additional namespaces under a namespace, and propagating resources within the namespace hierarchy. This allows self-service namespaces for tenants, without requiring cluster-wide permissions.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/benchmarks">Multi-Tenancy Benchmarks (MTB)&lt;/a> project provides benchmarks and a command-line tool that performs several configuration and runtime checks to report if tenant namespaces are properly isolated and the necessary security controls are implemented.&lt;/p>
&lt;h2 id="clusters-as-a-service">Clusters as a Service&lt;/h2>
&lt;p>With the &lt;em>clusters-as-a-service&lt;/em> usage model, each tenant gets their own cluster. This model allows tenants to have different versions of cluster-wide resources such as CRDs, and provides full isolation of the Kubernetes control plane.&lt;/p>
&lt;p>The tenant clusters may be provisioned using projects like &lt;a href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)&lt;/a> where a management cluster is used to provision multiple workload clusters. A workload cluster is assigned to a tenant and tenants have full control over cluster resources. Note that in most enterprises a central platform team may be responsible for managing required add-on services such as security and monitoring services, and for providing cluster lifecycle management services such as patching and upgrades. A tenant administrator may be restricted from modifying the centrally managed services and other critical cluster information.&lt;/p>
&lt;h2 id="control-planes-as-a-service">Control planes as a Service&lt;/h2>
&lt;p>In a variation of the &lt;em>clusters-as-a-service&lt;/em> model, the tenant cluster may be a &lt;strong>virtual cluster&lt;/strong> where each tenant gets their own dedicated Kubernetes control plane but share worker node resources. As with other forms of virtualization, users of a virtual cluster see no significant differences between a virtual cluster and other Kubernetes clusters. This is sometimes referred to as &lt;code>Control Planes as a Service&lt;/code> (CPaaS).&lt;/p>
&lt;p>A virtual cluster of this type shares worker node resources and workload state independent control plane components, like the scheduler. Other workload aware control-plane components, like the API server, are created on a per-tenant basis to allow overlaps, and additional components are used to synchronize and manage state across the per-tenant control plane and the underlying shared cluster resources. With this model users can manage their own cluster-wide resources.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/virtualcluster">Virtual Cluster&lt;/a> project implements this model, where a &lt;code>supercluster&lt;/code> is shared by multiple &lt;code>virtual clusters&lt;/code>. The &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested">Cluster API Nested&lt;/a> project is extending this work to conform to the CAPI model, allowing use of familiar API resources to provision and manage virtual clusters.&lt;/p>
&lt;h2 id="security-considerations">Security considerations&lt;/h2>
&lt;p>Cloud native security involves different system layers and lifecycle phases as described in the &lt;a href="https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters">Cloud Native Security Whitepaper&lt;/a> from CNCF SIG Security. Without proper security measures implemented across all layers and phases, Kubernetes tenant isolation can be compromised and a security breach with one tenant can threaten other tenants.&lt;/p>
&lt;p>It is important for any new user to Kubernetes to realize that the default installation of a new upstream Kubernetes cluster is not secure, and you are going to need to invest in hardening it in order to avoid security issues.&lt;/p>
&lt;p>At a minimum, the following security measures are required:&lt;/p>
&lt;ul>
&lt;li>image scanning: container image vulnerabilities can be exploited to execute commands and access additional resources.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a>: for &lt;em>namespaces-as-a-service&lt;/em> user roles and permissions must be properly configured at a per-namespace level; for other models tenants may need to be restricted from accessing centrally managed add-on services and other cluster-wide resources.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies&lt;/a>: for &lt;em>namespaces-as-a-service&lt;/em> default network policies that deny all ingress and egress traffic are recommended to prevent cross-tenant network traffic and may also be used as a best practice for other tenancy models.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Kubernetes Pod Security Standards&lt;/a>: to enforce Pod hardening best practices the &lt;code>Restricted&lt;/code> policy is recommended as the default for tenant workloads with exclusions configured only as needed.&lt;/li>
&lt;li>&lt;a href="https://www.cisecurity.org/benchmark/kubernetes/">CIS Benchmarks for Kubernetes&lt;/a>: the CIS Benchmarks for Kubernetes guidelines should be used to properly configure Kubernetes control-plane and worker node components.&lt;/li>
&lt;/ul>
&lt;p>Additional recommendations include using:&lt;/p>
&lt;ul>
&lt;li>policy engines: for configuration security best practices, such as only allowing trusted registries.&lt;/li>
&lt;li>runtime scanners: to detect and report runtime security events.&lt;/li>
&lt;li>VM-based container sandboxing: for stronger data plane isolation.&lt;/li>
&lt;/ul>
&lt;p>While proper security is required independently of tenancy models, not having essential security controls like &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">pod security&lt;/a> in a shared cluster provides attackers with means to compromise tenancy models and possibly access sensitive information across tenants increasing the overall risk profile.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>A 2020 CNCF survey showed that production Kubernetes usage has increased by over 300% since 2016. As an increasing number of Kubernetes workloads move to production, organizations are looking for ways to share Kubernetes resources across teams for agility and cost savings.&lt;/p>
&lt;p>The &lt;strong>namespaces as a service&lt;/strong> tenancy model allows sharing clusters and hence enables resource efficiencies. However, it requires proper security configurations and has limitations as all tenants share the same cluster-wide resources.&lt;/p>
&lt;p>The &lt;strong>clusters as a service&lt;/strong> tenancy model addresses these limitations, but with higher management and resource overhead.&lt;/p>
&lt;p>The &lt;strong>control planes as a service&lt;/strong> model provides a way to share resources of a single Kubernetes cluster and also let tenants manage their own cluster-wide resources. Sharing worker node resources increases resource effeciencies, but also exposes cross tenant security and isolation concerns that exist for shared clusters.&lt;/p>
&lt;p>In many cases, organizations will use multiple tenancy models to address different use cases and as different product and development teams will have varying needs. Following security and management best practices, such as applying &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> and not using the &lt;code>default&lt;/code> namespace, makes it easer to switch from one model to another.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">Kubernetes Multi-Tenancy Working Group&lt;/a> has created several projects like &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc">Hierarchical Namespaces Controller&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/virtualcluster">Virtual Cluster&lt;/a> / &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested">CAPI Nested&lt;/a>, and &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/benchmarks">Multi-Tenancy Benchmarks&lt;/a> to make it easier to provision and manage multi-tenancy models.&lt;/p>
&lt;p>If you are interested in multi-tenancy topics, or would like to share your use cases, please join us in an upcoming &lt;a href="https://github.com/kubernetes/community/blob/master/wg-multitenancy/README.md">community meeting&lt;/a> or reach out on the &lt;em>wg-multitenancy channel&lt;/em> on the &lt;a href="https://slack.k8s.io/">Kubernetes slack&lt;/a>.&lt;/p></description></item><item><title>Blog: Local Storage: Storage Capacity Tracking, Distributed Provisioning and Generic Ephemeral Volumes hit Beta</title><link>https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/</link><pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">&amp;quot;generic ephemeral
volumes&amp;quot;&lt;/a>
and &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">&amp;quot;storage capacity
tracking&amp;quot;&lt;/a>
features in Kubernetes are getting promoted to beta in Kubernetes
1.21. Together with the &lt;a href="https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node">distributed provisioning
support&lt;/a>
in the CSI external-provisioner, development and deployment of
Container Storage Interface (CSI) drivers which manage storage locally
on a node become a lot easier.&lt;/p>
&lt;p>This blog post explains how such drivers worked before and how these
features can be used to make drivers simpler.&lt;/p>
&lt;h2 id="problems-we-are-solving">Problems we are solving&lt;/h2>
&lt;p>There are drivers for local storage, like
&lt;a href="https://github.com/cybozu-go/topolvm">TopoLVM&lt;/a> for traditional disks
and &lt;a href="https://intel.github.io/pmem-csi/latest/README.html">PMEM-CSI&lt;/a>
for &lt;a href="https://pmem.io/">persistent memory&lt;/a>. They work and are ready for
usage today also on older Kubernetes releases, but making that possible
was not trivial.&lt;/p>
&lt;h3 id="central-component-required">Central component required&lt;/h3>
&lt;p>The first problem is volume provisioning: it is handled through the
Kubernetes control plane. Some component must react to
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims&lt;/a>
(PVCs)
and create volumes. Usually, that is handled by a central deployment
of the &lt;a href="https://kubernetes-csi.github.io/docs/external-provisioner.html">CSI
external-provisioner&lt;/a>
and a CSI driver component that then connects to the storage
backplane. But for local storage, there is no such backplane.&lt;/p>
&lt;p>TopoLVM solved this by having its different components communicate
with each other through the Kubernetes API server by creating and
reacting to custom resources. So although TopoLVM is based on CSI, a
standard that is independent of a particular container orchestrator,
TopoLVM only works on Kubernetes.&lt;/p>
&lt;p>PMEM-CSI created its own storage backplane with communication through
gRPC calls. Securing that communication depends on TLS certificates,
which made driver deployment more complicated.&lt;/p>
&lt;h3 id="informing-pod-scheduler-about-capacity">Informing Pod scheduler about capacity&lt;/h3>
&lt;p>The next problem is scheduling. When volumes get created independently
of pods (&amp;quot;immediate binding&amp;quot;), the CSI driver must pick a node without
knowing anything about the pod(s) that are going to use it. Topology
information then forces those pods to run on the node where the volume
was created. If other resources like RAM or CPU are exhausted there,
the pod cannot start. This can be avoided by configuring in the
StorageClass that volume creation is meant to wait for the first pod
that uses a volume (&lt;code>volumeBinding: WaitForFirstConsumer&lt;/code>). In that
mode, the Kubernetes scheduler tentatively picks a node based on other
constraints and then the external-provisioner is asked to create a
volume such that it is usable there. If local storage is exhausted,
the provisioner &lt;a href="https://github.com/kubernetes-csi/external-provisioner/blob/master/doc/design.md">can
ask&lt;/a>
for another scheduling round. But without information about available
capacity, the scheduler might always pick the same unsuitable node.&lt;/p>
&lt;p>Both TopoLVM and PMEM-CSI solved this with scheduler extenders. This
works, but it is hard to configure when deploying the driver because
communication between kube-scheduler and the driver is very dependent
on how the cluster was set up.&lt;/p>
&lt;h3 id="rescheduling">Rescheduling&lt;/h3>
&lt;p>A common use case for local storage is scratch space. A better fit for
that use case than persistent volumes are ephemeral volumes that get
created for a pod and destroyed together with it. The initial API for
supporting ephemeral volumes with CSI drivers (hence called &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">&amp;quot;&lt;em>CSI&lt;/em>
ephemeral
volumes&amp;quot;&lt;/a>)
was &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md">designed for light-weight
volumes&lt;/a>
where volume creation is unlikely to fail. Volume creation happens
after pods have been permanently scheduled onto a node, in contrast to
the traditional provisioning where volume creation is tried before
scheduling a pod onto a node. CSI drivers must be modified to support
&amp;quot;CSI ephemeral volumes&amp;quot;, which was done for TopoLVM and PMEM-CSI. But
due to the design of the feature in Kubernetes, pods can get stuck
permanently if storage capacity runs out on a node. The scheduler
extenders try to avoid that, but cannot be 100% reliable.&lt;/p>
&lt;h2 id="enhancements-in-kubernetes-1-21">Enhancements in Kubernetes 1.21&lt;/h2>
&lt;h3 id="distributed-provisioning">Distributed provisioning&lt;/h3>
&lt;p>Starting with &lt;a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v2.1.0">external-provisioner
v2.1.0&lt;/a>,
released for Kubernetes 1.20, provisioning can be handled by
external-provisioner instances that get &lt;a href="https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node">deployed together with the
CSI driver on each
node&lt;/a>
and then cooperate to provision volumes (&amp;quot;distributed
provisioning&amp;quot;). There is no need any more to have a central component
and thus no need for communication between nodes, at least not for
provisioning.&lt;/p>
&lt;h3 id="storage-capacity-tracking">Storage capacity tracking&lt;/h3>
&lt;p>A scheduler extender still needs some way to find out about capacity
on each node. When PMEM-CSI switched to distributed provisioning in
v0.9.0, this was done by querying the metrics data exposed by the
local driver containers. But it is better also for users to eliminate
the need for a scheduler extender completely because the driver
deployment becomes simpler. &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">Storage capacity
tracking&lt;/a>, &lt;a href="https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/">introduced in
1.19&lt;/a>
and promoted to beta in Kubernetes 1.21, achieves that. It works by
publishing information about capacity in &lt;code>CSIStorageCapacity&lt;/code>
objects. The scheduler itself then uses that information to filter out
unsuitable nodes. Because information might be not quite up-to-date,
pods may still get assigned to nodes with insufficient storage, it's
just less likely and the next scheduling attempt for a pod should work
better once the information got refreshed.&lt;/p>
&lt;h3 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h3>
&lt;p>So CSI drivers still need the ability to recover from a bad scheduling
decision, something that turned out to be impossible to implement for
&amp;quot;CSI ephemeral volumes&amp;quot;. &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">&amp;quot;&lt;em>Generic&lt;/em> ephemeral
volumes&amp;quot;&lt;/a>,
another feature that got promoted to beta in 1.21, don't have that
limitation. This feature adds a controller that will create and manage
PVCs with the lifetime of the Pod and therefore the normal recovery
mechanism also works for them. Existing storage drivers will be able
to process these PVCs without any new logic to handle this new
scenario.&lt;/p>
&lt;h2 id="known-limitations">Known limitations&lt;/h2>
&lt;p>Both generic ephemeral volumes and storage capacity tracking increase
the load on the API server. Whether that is a problem depends a lot on
the kind of workload, in particular how many pods have volumes and how
often those need to be created and destroyed.&lt;/p>
&lt;p>No attempt was made to model how scheduling decisions affect storage
capacity. That's because the effect can vary considerably depending on
how the storage system handles storage. The effect is that multiple
pods with unbound volumes might get assigned to the same node even
though there is only sufficient capacity for one pod. Scheduling
should recover, but it would be more efficient if the scheduler knew
more about storage.&lt;/p>
&lt;p>Because storage capacity gets published by a running CSI driver and
the cluster autoscaler needs information about a node that hasn't been
created yet, it will currently not scale up a cluster for pods that
need volumes. There is an &lt;a href="https://github.com/kubernetes/autoscaler/pull/3887">idea how to provide that
information&lt;/a>, but
more work is needed in that area.&lt;/p>
&lt;p>Distributed snapshotting and resizing are not currently supported. It
should be doable to adapt the respective sidecar and there are
tracking issues for external-snapshotter and external-resizer open
already, they just need some volunteer.&lt;/p>
&lt;p>The recovery from a bad scheduling decising can fail for pods with
multiple volumes, in particular when those volumes are local to nodes:
if one volume can be created and then storage is insufficient for
another volume, the first volume continues to exist and forces the
scheduler to put the pod onto the node of that volume. There is an
idea how do deal with this, &lt;a href="https://github.com/kubernetes/enhancements/pull/1703">rolling back the provision of the
volume&lt;/a>, but
this is only in the very early stages of brainstorming and not even a
merged KEP yet. For now it is better to avoid creating pods with more
than one persistent volume.&lt;/p>
&lt;h2 id="enabling-the-new-features-and-next-steps">Enabling the new features and next steps&lt;/h2>
&lt;p>With the feature entering beta in the 1.21 release, no additional actions are needed to enable it. Generic
ephemeral volumes also work without changes in CSI drivers. For more
information, see the
&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">documentation&lt;/a>
and the &lt;a href="https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/">previous blog
post&lt;/a>
about it. The API has not changed at all between alpha and beta.&lt;/p>
&lt;p>For the other two features, the external-provisioner documentation
explains how CSI driver developers must change how their driver gets
deployed to support &lt;a href="https://github.com/kubernetes-csi/external-provisioner#capacity-support">storage capacity
tracking&lt;/a>
and &lt;a href="https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node">distributed
provisioning&lt;/a>.
These two features are independent, therefore it is okay to enable
only one of them.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG
Storage&lt;/a>
would like to hear from you if you are using these new features. We
can be reached through
&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-storage">email&lt;/a>,
&lt;a href="https://slack.k8s.io/">Slack&lt;/a> (channel &lt;a href="https://kubernetes.slack.com/messages/sig-storage">&lt;code>#sig-storage&lt;/code>&lt;/a>) and in the
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meeting">regular SIG
meeting&lt;/a>.
A description of your workload would be very useful to validate design
decisions, set up performance tests and eventually promote these
features to GA.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Thanks a lot to the members of the community who have contributed to these
features or given feedback including members of SIG Scheduling, SIG Auth,
￼and of course SIG Storage!&lt;/p></description></item><item><title>Blog: kube-state-metrics goes v2.0</title><link>https://kubernetes.io/blog/2021/04/13/kube-state-metrics-v-2-0/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/13/kube-state-metrics-v-2-0/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Lili Cosic (Red Hat), Frederic Branczyk (Polar Signals), Manuel Rüger (Sony Interactive Entertainment), Tariq Ibrahim (Salesforce)&lt;/p>
&lt;h2 id="what">What?&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a>, a project under the Kubernetes organization, generates Prometheus format metrics based on the current state of the Kubernetes native resources. It does this by listening to the Kubernetes API and gathering information about resources and objects, e.g. Deployments, Pods, Services, and StatefulSets. A full list of resources is available in the &lt;a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs">documentation&lt;/a> of kube-state-metrics.&lt;/p>
&lt;h2 id="why">Why?&lt;/h2>
&lt;p>There are numerous useful metrics and insights provided by &lt;code>kube-state-metrics&lt;/code> right out of the box! These metrics can be used to serve as an insight into your cluster: Either through metrics alone, in the form of dashboards, or through an alerting pipeline. To provide a few examples:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kube_pod_container_status_restarts_total&lt;/code> can be used to alert on a crashing pod.&lt;/li>
&lt;li>&lt;code>kube_deployment_status_replicas&lt;/code> which together with &lt;code>kube_deployment_status_replicas_available&lt;/code> can be used to alert on whether a deployment is rolled out successfully or stuck.&lt;/li>
&lt;li>&lt;code>kube_pod_container_resource_requests&lt;/code> and &lt;code>kube_pod_container_resource_limits&lt;/code> can be used in capacity planning dashboards.&lt;/li>
&lt;/ul>
&lt;p>And there are many more metrics available! To learn more about the other metrics and their details, please check out the &lt;a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs#readme">documentation&lt;/a>.&lt;/p>
&lt;h2 id="what-is-new-in-v2-0">What is new in v2.0?&lt;/h2>
&lt;p>So now that we know what kube-state-metrics is, we are excited to announce the next release: kube-state-metrics v2.0! This release was long-awaited and started with an alpha release in September 2020. To ease maintenance we removed tech debt and also adjusted some confusing wording around user-facing flags and APIs. We also removed some metrics that caused unnecessarily high cardinality in Prometheus! For the 2.0 release, we took the time to set up scale and performance testing. This allows us to better understand if we hit any issues in large clusters and also to document resource request recommendations for your clusters. In this release (and v1.9.8) container builds providing support for multiple architectures were introduced allowing you to run kube-state-metrics on ARM, ARM64, PPC64 and S390x as well!&lt;/p>
&lt;p>So without further ado, here is the list of more noteworthy user-facing breaking changes. A full list of changes, features and bug fixes is available in the changelog at the end of this post.&lt;/p>
&lt;ul>
&lt;li>Flag &lt;code>--namespace&lt;/code> was renamed to &lt;code>--namespaces&lt;/code>. If you are using the former, please make sure to update the flag before deploying the latest release.&lt;/li>
&lt;li>Flag &lt;code>--collectors&lt;/code> was renamed to &lt;code>--resources&lt;/code>.&lt;/li>
&lt;li>Flags &lt;code>--metric-blacklist&lt;/code> and &lt;code>--metric-whitelist&lt;/code> were renamed to &lt;code>--metric-denylist&lt;/code> and &lt;code>--metric-allowlist&lt;/code>.&lt;/li>
&lt;li>Flag &lt;code>--metric-labels-allowlist&lt;/code> allows you to specify a list of Kubernetes labels that get turned into the dimensions of the &lt;code>kube_&amp;lt;resource-name&amp;gt;_labels&lt;/code> metrics. By default, the metric contains only name and namespace labels.&lt;/li>
&lt;li>All metrics with a prefix of &lt;code>kube_hpa_*&lt;/code> were renamed to &lt;code>kube_horizontalpodautoscaler_*&lt;/code>.&lt;/li>
&lt;li>Metric labels that relate to Kubernetes were converted to snake_case.&lt;/li>
&lt;li>If you are importing kube-state-metrics as a library, we have updated our go module path to &lt;code>k8s.io/kube-state-metrics/v2&lt;/code>&lt;/li>
&lt;li>All deprecated stable metrics were removed as per the &lt;a href="https://github.com/kubernetes/kube-state-metrics/tree/release-1.9/docs#metrics-deprecation">notice in the v1.9 release&lt;/a>.&lt;/li>
&lt;li>&lt;code>quay.io/coreos/kube-state-metrics&lt;/code> images will no longer be updated. &lt;code>k8s.gcr.io/kube-state-metrics/kube-state-metrics&lt;/code> is the new canonical location.&lt;/li>
&lt;li>The helm chart that is part of the kubernetes/kube-state-metrics repository is deprecated. &lt;a href="https://github.com/prometheus-community/helm-charts">https://github.com/prometheus-community/helm-charts&lt;/a> will be its new location.&lt;/li>
&lt;/ul>
&lt;p>For the full list of v2.0 release changes includes features, bug fixes and other breaking changes see the full &lt;a href="https://github.com/kubernetes/kube-state-metrics/blob/master/CHANGELOG.md">CHANGELOG&lt;/a>.&lt;/p>
&lt;h2 id="found-a-problem">Found a problem?&lt;/h2>
&lt;p>Thanks to all our users for testing so far and thank you to all our contributors for your issue reports as well as code and documentation changes! If you find any problems, we the &lt;a href="https://github.com/kubernetes/kube-state-metrics/blob/master/OWNERS">maintainers&lt;/a> are more than happy to look into them, so please report them by opening a &lt;a href="https://github.com/kubernetes/kube-state-metrics/issues/new/choose">GitHub issue&lt;/a>.&lt;/p></description></item><item><title>Blog: Introducing Suspended Jobs</title><link>https://kubernetes.io/blog/2021/04/12/introducing-suspended-jobs/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/12/introducing-suspended-jobs/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Adhityaa Chandrasekar (Google)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Jobs&lt;/a> are a crucial part of
Kubernetes' API. While other kinds of workloads such as &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments&lt;/a>,
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSets&lt;/a>,
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a>, and
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSets&lt;/a>
solve use-cases that require Pods to run forever, Jobs are useful when Pods need
to run to completion. Commonly used in parallel batch processing, Jobs can be
used in a variety of applications ranging from video rendering and database
maintenance to sending bulk emails and scientific computing.&lt;/p>
&lt;p>While the amount of parallelism and the conditions for Job completion are
configurable, the Kubernetes API lacked the ability to suspend and resume Jobs.
This is often desired when cluster resources are limited and a higher priority
Job needs to execute in the place of another Job. Deleting the lower priority
Job is a poor workaround as Pod completion history and other metrics associated
with the Job will be lost.&lt;/p>
&lt;p>With the recent Kubernetes 1.21 release, you will be able to suspend a Job by
updating its spec. The feature is currently in &lt;strong>alpha&lt;/strong> and requires you to
enable the &lt;code>SuspendJob&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>
on the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">API server&lt;/a>
and the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">controller manager&lt;/a>
in order to use it.&lt;/p>
&lt;h2 id="api-changes">API changes&lt;/h2>
&lt;p>We introduced a new boolean field &lt;code>suspend&lt;/code> into the &lt;code>.spec&lt;/code> of Jobs. Let's say
I create the following Job:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">suspend&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parallelism&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-container&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>busybox&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;sleep&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;5&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Jobs are not suspended by default, so I'm explicitly setting the &lt;code>suspend&lt;/code> field
to &lt;em>true&lt;/em> in the &lt;code>.spec&lt;/code> of the above Job manifest. In the above example, the
Job controller will refrain from creating Pods until I'm ready to start the Job,
which I can do by updating &lt;code>suspend&lt;/code> to false.&lt;/p>
&lt;p>As another example, consider a Job that was created with the &lt;code>suspend&lt;/code> field
omitted. The Job controller will happily create Pods to work towards Job
completion. However, before the Job completes, if I explicitly set the field to
true with a Job update, the Job controller will terminate all active Pods that
are running and will wait indefinitely for the flag to be flipped back to false.
Typically, Pod termination is done by sending a SIGTERM signal to all container
processes in the Pod; the &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">graceful termination period&lt;/a>
defined in the Pod spec will be honoured. Pods terminated this way will not be
counted as failures by the Job controller.&lt;/p>
&lt;p>It is important to understand that succeeded and failed Pods from the past will
continue to exist after you suspend a Job. That is, that they will count towards
Job completion once you resume it. You can verify this by looking at Job's
status before and after suspension.&lt;/p>
&lt;p>Read the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job#suspending-a-job">documentation&lt;/a>
for a full overview of this new feature.&lt;/p>
&lt;h2 id="where-is-this-useful">Where is this useful?&lt;/h2>
&lt;p>Let's say I'm the operator of a large cluster. I have many users submitting Jobs
to the cluster, but not all Jobs are created equal — some Jobs are more
important than others. Cluster resources aren't infinite either, so all users
must share resources. If all Jobs were created in the suspended state and placed
in a pending queue, I can achieve priority-based Job scheduling by resuming Jobs
in the right order.&lt;/p>
&lt;p>As another motivational use-case, consider a cloud provider where compute
resources are cheaper at night than in the morning. If I have a long-running Job
that takes multiple days to complete, being able to suspend the Job in the
morning and then resume it in the evening every day can reduce costs.&lt;/p>
&lt;p>Since this field is a part of the Job spec, &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJobs&lt;/a>
automatically get this feature for free too.&lt;/p>
&lt;h2 id="references-and-next-steps">References and next steps&lt;/h2>
&lt;p>If you're interested in a deeper dive into the rationale behind this feature and
the decisions we have taken, consider reading the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2232-suspend-jobs">enhancement proposal&lt;/a>.
There's more detail on suspending and resuming jobs in the documentation for &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job#suspending-a-job">Job&lt;/a>.&lt;/p>
&lt;p>As previously mentioned, this feature is currently in alpha and is available
only if you explicitly opt-in through the &lt;code>SuspendJob&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>.
If this is a feature you're interested in, please consider testing suspended
Jobs in your cluster and providing feedback. You can discuss this enhancement &lt;a href="https://github.com/kubernetes/enhancements/issues/2232">on GitHub&lt;/a>.
The SIG Apps community also &lt;a href="https://github.com/kubernetes/community/tree/master/sig-apps#meetings">meets regularly&lt;/a>
and can be reached through &lt;a href="https://github.com/kubernetes/community/tree/master/sig-apps#contact">Slack or the mailing list&lt;/a>.
Barring any unexpected changes to the API, we intend to graduate the feature to
beta in Kubernetes 1.22, so that the feature becomes available by default.&lt;/p></description></item><item><title>Blog: Kubernetes 1.21: CronJob Reaches GA</title><link>https://kubernetes.io/blog/2021/04/09/kubernetes-release-1.21-cronjob-ga/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/09/kubernetes-release-1.21-cronjob-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alay Patel (Red Hat), and Maciej Szulik (Red Hat)&lt;/p>
&lt;p>In Kubernetes v1.21, the
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a> resource
reached general availability (GA). We've also substantially improved the
performance of CronJobs since Kubernetes v1.19, by implementing a new
controller.&lt;/p>
&lt;p>In Kubernetes v1.20 we launched a revised v2 controller for CronJobs,
initially as an alpha feature. Kubernetes 1.21 uses the newer controller by
default, and the CronJob resource itself is now GA (group version: &lt;code>batch/v1&lt;/code>).&lt;/p>
&lt;p>In this article, we'll take you through the driving forces behind this new
development, give you a brief description of controller design for core
Kubernetes, and we'll outline what you will gain from this improved controller.&lt;/p>
&lt;p>The driving force behind promoting the API was Kubernetes' policy choice to
&lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/">ensure APIs move beyond beta&lt;/a>.
That policy aims to prevent APIs from being stuck in a “permanent beta” state.
Over the years the old CronJob controller implementation had received healthy
feedback from the community, with reports of several widely recognized
&lt;a href="https://github.com/kubernetes/kubernetes/issues/82659">issues&lt;/a>.&lt;/p>
&lt;p>If the beta API for CronJob was to be supported as GA, the existing controller
code would need substantial rework. Instead, the SIG Apps community decided
to introduce a new controller and gradually replace the old one.&lt;/p>
&lt;h2 id="how-do-controllers-work">How do controllers work?&lt;/h2>
&lt;p>Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">controllers&lt;/a> are control
loops that watch the state of resource(s) in your cluster, then make or
request changes where needed. Each controller tries to move part of the
current cluster state closer to the desired state.&lt;/p>
&lt;p>The v1 CronJob controller works by performing a periodic poll and sweep of all
the CronJob objects in your cluster, in order to act on them. It is a single
worker implementation that gets all CronJobs every 10 seconds, iterates over
each one of them, and syncs them to their desired state. This was the default
way of doing things almost 5 years ago when the controller was initially
written. In hindsight, we can certainly say that such an approach can
overload the API server at scale.&lt;/p>
&lt;p>These days, every core controller in kubernetes must follow the guidelines
described in &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md#readme">Writing Controllers&lt;/a>.
Among many details, that document prescribes using
&lt;a href="https://www.cncf.io/blog/2019/10/15/extend-kubernetes-via-a-shared-informer/">shared informers&lt;/a>
to “receive notifications of adds, updates, and deletes for a particular
resource”. Upon any such events, the related object(s) is placed in a queue.
Workers pull items from the queue and process them one at a time. This
approach ensures consistency and scalability.&lt;/p>
&lt;p>The picture below shows the flow of information from kubernetes API server,
through shared informers and queue, to the main part of a controller - a
reconciliation loop which is responsible for performing the core functionality.&lt;/p>
&lt;p>&lt;img src="controller-flowchart.svg" alt="Controller flowchart">&lt;/p>
&lt;p>The CronJob controller V2 uses a queue that implements the DelayingInterface to
handle the scheduling aspect. This queue allows processing an element after a
specific time interval. Every time there is a change in a CronJob or its related
Jobs, the key that represents the CronJob is pushed to the queue. The main
handler pops the key, processes the CronJob, and after completion
pushes the key back into the queue for the next scheduled time interval. This is
immediately a more performant implementation, as it no longer requires a linear
scan of all the CronJobs. On top of that, this controller can be scaled by
increasing the number of workers processing the CronJobs in parallel.&lt;/p>
&lt;h2 id="performance-impact">Performance impact of the new controller&lt;/h2>
&lt;p>In order to test the performance difference of the two controllers a VM instance
with 128 GiB RAM and 64 vCPUs was used to set up a single node Kubernetes cluster.
Initially, a sample workload was created with 20 CronJob instances with a schedule
to run every minute, and 2100 CronJobs running every 20 hours. Additionally,
over the next few minutes we added 1000 CronJobs with a schedule to run every
20 hours, until we reached a total of 5120 CronJobs.&lt;/p>
&lt;p>&lt;img src="performance-impact-graph.svg" alt="Visualization of performance">&lt;/p>
&lt;p>We observed that for every 1000 CronJobs added, the old controller used
around 90 to 120 seconds more wall-clock time to schedule 20 Jobs every cycle.
That is, at 5120 CronJobs, the old controller took approximately 9 minutes
to create 20 Jobs. Hence, during each cycle, about 8 schedules were missed.
The new controller, implemented with architectural change explained above,
created 20 Jobs without any delay, even when we created an additional batch
of 1000 CronJobs reaching a total of 6120.&lt;/p>
&lt;p>As a closing remark, the new controller exposes a histogram metric
&lt;code>cronjob_controller_cronjob_job_creation_skew_duration_seconds&lt;/code> which helps
monitor the time difference between when a CronJob is meant to run and when
the actual Job is created.&lt;/p>
&lt;p>Hopefully the above description is a sufficient argument to follow the
guidelines and standards set in the Kubernetes project, even for your own
controllers. As mentioned before, the new controller is on by default starting
from Kubernetes v1.21; if you want to check it out in the previous release (1.20),
you can enable the &lt;code>CronJobControllerV2&lt;/code>
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>
for the kube-controller-manager: &lt;code>--feature-gate=&amp;quot;CronJobControllerV2=true&amp;quot;&lt;/code>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.21: Power to the Community</title><link>https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.21/release-team.md">Kubernetes 1.21 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.21, our first release of 2021! This release consists of 51 enhancements: 13 enhancements have graduated to stable, 16 enhancements are moving to beta, 20 enhancements are entering alpha, and 2 features have been deprecated.&lt;/p>
&lt;p>This release cycle, we saw a major shift in ownership of processes around the release team. We moved from a synchronous mode of communication, where we periodically asked the community for inputs, to a mode where the community opts-in to contribute features and/or blogs to the release. These changes have resulted in an increase in collaboration and teamwork across the community. The result of all that is reflected in Kubernetes 1.21 having the most number of features in the recent times.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="cronjobs-graduate-to-stable">CronJobs Graduate to Stable!&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJobs&lt;/a> (previously ScheduledJobs) has been a beta feature since Kubernetes 1.8! With 1.21 we get to finally see this widely used API graduate to stable.&lt;/p>
&lt;p>CronJobs are meant for performing regular scheduled actions such as backups, report generation, and so on. Each of those tasks should be configured to recur indefinitely (for example: once a day / week / month); you can define the point in time within that interval when the job should start.&lt;/p>
&lt;h3 id="immutable-secrets-and-configmaps">Immutable Secrets and ConfigMaps&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable">Immutable Secrets&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable">ConfigMaps&lt;/a> add a new field to those resource types that will reject changes to those objects if set. Secrets and ConfigMaps by default are mutable which is beneficial for pods that are able to consume changes. Mutating Secrets and ConfigMaps can also cause problems if a bad configuration is pushed for pods that use them.&lt;/p>
&lt;p>By marking Secrets and ConfigMaps as immutable you can be sure your application configuration won't change. If you want to make changes you'll need to create a new, uniquly named Secret or ConfigMap and deploy a new pod to consume that resource. Immutable resources also have scaling benefits because controllers do not need to poll the API server to watch for changes.&lt;/p>
&lt;p>This feature has graduated to stable in Kubernetes 1.21.&lt;/p>
&lt;h3 id="ipv4-ipv6-dual-stack-support">IPv4/IPv6 dual-stack support&lt;/h3>
&lt;p>IP addresses are a consumable resource that cluster operators and administrators need to make sure are not exhausted. In particular, public IPv4 addresses are now scarce. Having dual-stack support enables native IPv6 routing to pods and services, whilst still allowing your cluster to talk IPv4 where needed. Dual-stack cluster networking also improves a possible scaling limitation for workloads.&lt;/p>
&lt;p>Dual-stack support in Kubernetes means that pods, services, and nodes can get IPv4 addresses and IPv6 addresses. In Kubernetes 1.21 &lt;a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/">dual-stack networking&lt;/a> has graduated from alpha to beta, and is now enabled by default.&lt;/p>
&lt;h3 id="graceful-node-shutdown">Graceful Node Shutdown&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown">Graceful Node shutdown&lt;/a> also graduated to beta with this release (and will now be available to a much larger group of users)! This is a hugely beneficial feature that allows the kubelet to be aware of node shutdown, and gracefully terminate pods that are scheduled to that node.&lt;/p>
&lt;p>Currently, when a node shuts down, pods do not follow the expected termination lifecycle and are not shut down gracefully. This can introduce problems with a lot of different workloads. Going forward, the kubelet will be able to detect imminent system shutdown through systemd, then inform running pods so they can terminate as gracefully as possible.&lt;/p>
&lt;h3 id="persistentvolume-health-monitor">PersistentVolume Health Monitor&lt;/h3>
&lt;p>Persistent Volumes (PV) are commonly used in applications to get local, file-based storage. They can be used in many different ways and help users migrate applications without needing to re-write storage backends.&lt;/p>
&lt;p>Kubernetes 1.21 has a new alpha feature which allows PVs to be monitored for health of the volume and marked accordingly if the volume becomes unhealthy. Workloads will be able to react to the health state to protect data from being written or read from a volume that is unhealthy.&lt;/p>
&lt;h3 id="reducing-kubernetes-build-maintenance">Reducing Kubernetes Build Maintenance&lt;/h3>
&lt;p>Previously Kubernetes has maintained multiple build systems. This has often been a source of friction and complexity for new and current contributors.&lt;/p>
&lt;p>Over the last release cycle, a lot of work has been put in to simplify the build process, and standardize on the native Golang build tools. This should empower broader community maintenance, and lower the barrier to entry for new contributors.&lt;/p>
&lt;h2 id="major-changes">Major Changes&lt;/h2>
&lt;h3 id="podsecuritypolicy-deprecation">PodSecurityPolicy Deprecation&lt;/h3>
&lt;p>In Kubernetes 1.21, PodSecurityPolicy is deprecated. As with all Kubernetes feature deprecations, PodSecurityPolicy will continue to be available and fully-functional for several more releases. PodSecurityPolicy, previously in the beta stage, is planned for removal in Kubernetes 1.25.&lt;/p>
&lt;p>What's next? We're developing a new built-in mechanism to help limit Pod privileges, with a working title of “PSP Replacement Policy.” Our plan is for this new mechanism to cover the key PodSecurityPolicy use cases, with greatly improved ergonomics and maintainability. To learn more, read &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>.&lt;/p>
&lt;h3 id="topologykeys-deprecation">TopologyKeys Deprecation&lt;/h3>
&lt;p>The Service field &lt;code>topologyKeys&lt;/code> is now deprecated; all the component features that used this field were previously alpha, and are now also deprecated.
We've replaced &lt;code>topologyKeys&lt;/code> with a way to implement topology-aware routing, called topology-aware hints. Topology-aware hints are an alpha feature in Kubernetes 1.21. You can read more details about the replacement feature in &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service-topology/">Topology Aware Hints&lt;/a>; the related &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/2433-topology-aware-hints/README.md">KEP&lt;/a> explains the context for why we switched.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/34">Add sysctl support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/85">PodDisruptionBudgets&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="notable-feature-updates">Notable Feature Updates&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/541">External client-go credential providers&lt;/a> - beta in 1.21&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1602">Structured logging&lt;/a> - graduating to beta in 1.22&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/592">TTL after finish cleanup for Jobs and Pods&lt;/a> - graduated to beta&lt;/li>
&lt;/ul>
&lt;h1 id="release-notes">Release notes&lt;/h1>
&lt;p>You can check out the full details of the 1.21 release in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md">release notes&lt;/a>.&lt;/p>
&lt;h1 id="availability-of-release">Availability of release&lt;/h1>
&lt;p>Kubernetes 1.21 is available for &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.21.0">download on GitHub&lt;/a>. There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a>. If you’d like to try building a cluster from scratch, check out the &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> tutorial by Kelsey Hightower.&lt;/p>
&lt;h1 id="release-team">Release Team&lt;/h1>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Nabarun Pal, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.21 release for the community.&lt;/p>
&lt;h1 id="release-logo">Release Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-04-08-kubernetes-release-1.21/globe_250px.png" alt="Kubernetes 1.21 Release Logo">&lt;/p>
&lt;p>The Kubernetes 1.21 Release Logo portrays the global nature of the Release Team, with release team members residing in timezones from UTC+8 all the way to UTC-8. The diversity of the release team brought in a lot of challenges, but the team tackled them all by adopting more asynchronous communication practices. The heptagonal globe in the release logo signifies the sheer determination of the community to overcome the challenges as they come. It celebrates the amazing teamwork of the release team over the last 3 months to bring in a fun packed Kubernetes release!&lt;/p>
&lt;p>The logo is designed by &lt;a href="https://www.behance.net/noblebatman">Aravind Sekar&lt;/a>, an independent designer based out of India. Aravind helps open source communities like PyCon India in their design efforts.&lt;/p>
&lt;h1 id="user-highlights">User Highlights&lt;/h1>
&lt;ul>
&lt;li>CNCF welcomes 47 new organizations across the globe as members to advance Cloud Native technology further at the start of 2021! These &lt;a href="https://www.cncf.io/announcements/2021/02/24/cloud-native-computing-foundation-welcomes-47-new-members-at-the-start-of-2021/">new members&lt;/a> will join CNCF at the upcoming 2021 KubeCon + CloudNativeCon events, including &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">KubeCon + CloudNativeCom EU – Virtual&lt;/a> from May 4 – 7, 2021, and &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">KubeCon + CloudNativeCon NA in Los Angeles&lt;/a> from October 12 – 15, 2021.&lt;/li>
&lt;/ul>
&lt;h1 id="project-velocity">Project Velocity&lt;/h1>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/">CNCF K8s DevStats project&lt;/a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.21 release cycle, which ran for 12 weeks (January 11 to April 8), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.20.0%20-%20now&amp;amp;var-metric=contributions">999 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.20.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1279 individuals&lt;/a>.&lt;/p>
&lt;h1 id="ecosystem-updates">Ecosystem Updates&lt;/h1>
&lt;ul>
&lt;li>In the wake of rising racism &amp;amp; attacks on global Asian communities, read the statement from CNCF General Priyanka Sharma on the &lt;a href="https://www.cncf.io/blog/2021/03/18/statement-from-cncf-general-manager-priyanka-sharma-on-the-unacceptable-attacks-against-aapi-and-asian-communities/">CNCF blog&lt;/a> reinstating the community's commitment towards inclusive values &amp;amp; diversity-powered resilience.&lt;/li>
&lt;li>We now have a process in place for migration of the default branch from master → main. Learn more about the guidelines &lt;a href="k8s.dev/rename">here&lt;/a>&lt;/li>
&lt;li>CNCF and the Linux Foundation have announced the availability of their new training course, &lt;a href="https://training.linuxfoundation.org/training/kubernetes-security-essentials-lfs260/">LFS260 – Kubernetes Security Essentials&lt;/a>. In addition to providing skills and knowledge on a broad range of best practices for securing container-based applications and Kubernetes platforms, the course is also a great way to prepare for the recently launched &lt;a href="https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/">Certified Kubernetes Security Specialist&lt;/a> certification exam.&lt;/li>
&lt;/ul>
&lt;h1 id="event-updates">Event Updates&lt;/h1>
&lt;ul>
&lt;li>KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! You can find more information about the conference &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">here&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://kubernetescommunitydays.org/">Kubernetes Community Days&lt;/a> are being relaunched! Q2 2021 will start with Africa and Bengaluru.&lt;/li>
&lt;/ul>
&lt;h1 id="upcoming-release-webinar">Upcoming release webinar&lt;/h1>
&lt;p>Join the members of the Kubernetes 1.21 release team on May 13th, 2021 to learn about the major features in this release including IPv4/IPv6 dual-stack support, PersistentVolume Health Monitor, Immutable Secrets and ConfigMaps, and many more. Register here: &lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-121-release/">https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-121-release/&lt;/a>&lt;/p>
&lt;h1 id="get-involved">Get Involved&lt;/h1>
&lt;p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://github.com/cncf/foundation/blob/master/case-study-guidelines.md">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: PodSecurityPolicy Deprecation: Past, Present, and Future</title><link>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Tabitha Sable (Kubernetes SIG Security)&lt;/p>
&lt;p>PodSecurityPolicy (PSP) is being deprecated in Kubernetes 1.21, to be released later this week. This starts the countdown to its removal, but doesn’t change anything else. PodSecurityPolicy will continue to be fully functional for several more releases before being removed completely. In the meantime, we are developing a replacement for PSP that covers key use cases more easily and sustainably.&lt;/p>
&lt;p>What are Pod Security Policies? Why did we need them? Why are they going away, and what’s next? How does this affect you? These key questions come to mind as we prepare to say goodbye to PSP, so let’s walk through them together. We’ll start with an overview of how features get removed from Kubernetes.&lt;/p>
&lt;h2 id="what-does-deprecation-mean-in-kubernetes">What does deprecation mean in Kubernetes?&lt;/h2>
&lt;p>Whenever a Kubernetes feature is set to go away, our &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a> is our guide. First the feature is marked as deprecated, then after enough time has passed, it can finally be removed.&lt;/p>
&lt;p>Kubernetes 1.21 starts the deprecation process for PodSecurityPolicy. As with all feature deprecations, PodSecurityPolicy will continue to be fully functional for several more releases. The current plan is to remove PSP from Kubernetes in the 1.25 release.&lt;/p>
&lt;p>Until then, PSP is still PSP. There will be at least a year during which the newest Kubernetes releases will still support PSP, and nearly two years until PSP will pass fully out of all supported Kubernetes versions.&lt;/p>
&lt;h2 id="what-is-podsecuritypolicy">What is PodSecurityPolicy?&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a> is a built-in &lt;a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/">admission controller&lt;/a> that allows a cluster administrator to control security-sensitive aspects of the Pod specification.&lt;/p>
&lt;p>First, one or more PodSecurityPolicy resources are created in a cluster to define the requirements Pods must meet. Then, RBAC rules are created to control which PodSecurityPolicy applies to a given pod. If a pod meets the requirements of its PSP, it will be admitted to the cluster as usual. In some cases, PSP can also modify Pod fields, effectively creating new defaults for those fields. If a Pod does not meet the PSP requirements, it is rejected, and cannot run.&lt;/p>
&lt;p>One more important thing to know about PodSecurityPolicy: it’s not the same as &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context">PodSecurityContext&lt;/a>.&lt;/p>
&lt;p>A part of the Pod specification, PodSecurityContext (and its per-container counterpart &lt;code>SecurityContext&lt;/code>) is the collection of fields that specify many of the security-relevant settings for a Pod. The security context dictates to the kubelet and container runtime how the Pod should actually be run. In contrast, the PodSecurityPolicy only constrains (or defaults) the values that may be set on the security context.&lt;/p>
&lt;p>The deprecation of PSP does not affect PodSecurityContext in any way.&lt;/p>
&lt;h2 id="why-did-we-need-podsecuritypolicy">Why did we need PodSecurityPolicy?&lt;/h2>
&lt;p>In Kubernetes, we define resources such as Deployments, StatefulSets, and Services that represent the building blocks of software applications. The various controllers inside a Kubernetes cluster react to these resources, creating further Kubernetes resources or configuring some software or hardware to accomplish our goals.&lt;/p>
&lt;p>In most Kubernetes clusters, RBAC (Role-Based Access Control) &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">rules&lt;/a> control access to these resources. &lt;code>list&lt;/code>, &lt;code>get&lt;/code>, &lt;code>create&lt;/code>, &lt;code>edit&lt;/code>, and &lt;code>delete&lt;/code> are the sorts of API operations that RBAC cares about, but &lt;em>RBAC does not consider what settings are being put into the resources it controls&lt;/em>. For example, a Pod can be almost anything from a simple webserver to a privileged command prompt offering full access to the underlying server node and all the data. It’s all the same to RBAC: a Pod is a Pod is a Pod.&lt;/p>
&lt;p>To control what sorts of settings are allowed in the resources defined in your cluster, you need Admission Control in addition to RBAC. Since Kubernetes 1.3, PodSecurityPolicy has been the built-in way to do that for security-related Pod fields. Using PodSecurityPolicy, you can prevent “create Pod” from automatically meaning “root on every cluster node,” without needing to deploy additional external admission controllers.&lt;/p>
&lt;h2 id="why-is-podsecuritypolicy-going-away">Why is PodSecurityPolicy going away?&lt;/h2>
&lt;p>In the years since PodSecurityPolicy was first introduced, we have realized that PSP has some serious usability problems that can’t be addressed without making breaking changes.&lt;/p>
&lt;p>The way PSPs are applied to Pods has proven confusing to nearly everyone that has attempted to use them. It is easy to accidentally grant broader permissions than intended, and difficult to inspect which PSP(s) apply in a given situation. The “changing Pod defaults” feature can be handy, but is only supported for certain Pod settings and it’s not obvious when they will or will not apply to your Pod. Without a “dry run” or audit mode, it’s impractical to retrofit PSP to existing clusters safely, and it’s impossible for PSP to ever be enabled by default.&lt;/p>
&lt;p>For more information about these and other PSP difficulties, check out SIG Auth’s KubeCon NA 2019 Maintainer Track session video:
&lt;div class="youtube-quote-sm">
&lt;iframe src="https://www.youtube.com/embed/SFtHRmPuhEw?start=953" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>Today, you’re not limited only to deploying PSP or writing your own custom admission controller. Several external admission controllers are available that incorporate lessons learned from PSP to provide a better user experience. &lt;a href="https://github.com/cruise-automation/k-rail">K-Rail&lt;/a>, &lt;a href="https://github.com/kyverno/kyverno/">Kyverno&lt;/a>, and &lt;a href="https://github.com/open-policy-agent/gatekeeper/">OPA/Gatekeeper&lt;/a> are all well-known, and each has its fans.&lt;/p>
&lt;p>Although there are other good options available now, we believe there is still value in having a built-in admission controller available as a choice for users. With this in mind, we turn toward building what’s next, inspired by the lessons learned from PSP.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Kubernetes SIG Security, SIG Auth, and a diverse collection of other community members have been working together for months to ensure that what’s coming next is going to be awesome. We have developed a Kubernetes Enhancement Proposal (&lt;a href="https://github.com/kubernetes/enhancements/issues/2579">KEP 2579&lt;/a>) and a prototype for a new feature, currently being called by the temporary name &amp;quot;PSP Replacement Policy.&amp;quot; We are targeting an Alpha release in Kubernetes 1.22.&lt;/p>
&lt;p>PSP Replacement Policy starts with the realization that since there is a robust ecosystem of external admission controllers already available, PSP’s replacement doesn’t need to be all things to all people. Simplicity of deployment and adoption is the key advantage a built-in admission controller has compared to an external webhook, so we have focused on how to best utilize that advantage.&lt;/p>
&lt;p>PSP Replacement Policy is designed to be as simple as practically possible while providing enough flexibility to really be useful in production at scale. It has soft rollout features to enable retrofitting it to existing clusters, and is configurable enough that it can eventually be active by default. It can be deactivated partially or entirely, to coexist with external admission controllers for advanced use cases.&lt;/p>
&lt;h2 id="what-does-this-mean-for-you">What does this mean for you?&lt;/h2>
&lt;p>What this all means for you depends on your current PSP situation. If you’re already using PSP, there’s plenty of time to plan your next move. Please review the PSP Replacement Policy KEP and think about how well it will suit your use case.&lt;/p>
&lt;p>If you’re making extensive use of the flexibility of PSP with numerous PSPs and complex binding rules, you will likely find the simplicity of PSP Replacement Policy too limiting. Use the next year to evaluate the other admission controller choices in the ecosystem. There are resources available to ease this transition, such as the &lt;a href="https://github.com/open-policy-agent/gatekeeper-library">Gatekeeper Policy Library&lt;/a>.&lt;/p>
&lt;p>If your use of PSP is relatively simple, with a few policies and straightforward binding to service accounts in each namespace, you will likely find PSP Replacement Policy to be a good match for your needs. Evaluate your PSPs compared to the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> to get a feel for where you’ll be able to use the Restricted, Baseline, and Privileged policies. Please follow along with or contribute to the KEP and subsequent development, and try out the Alpha release of PSP Replacement Policy when it becomes available.&lt;/p>
&lt;p>If you’re just beginning your PSP journey, you will save time and effort by keeping it simple. You can approximate the functionality of PSP Replacement Policy today by using the Pod Security Standards’ PSPs. If you set the cluster default by binding a Baseline or Restricted policy to the &lt;code>system:serviceaccounts&lt;/code> group, and then make a more-permissive policy available as needed in certain Namespaces &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod">using ServiceAccount bindings&lt;/a>, you will avoid many of the PSP pitfalls and have an easy migration to PSP Replacement Policy. If your needs are much more complex than this, your effort is probably better spent adopting one of the more fully-featured external admission controllers mentioned above.&lt;/p>
&lt;p>We’re dedicated to making Kubernetes the best container orchestration tool we can, and sometimes that means we need to remove longstanding features to make space for better things to come. When that happens, the Kubernetes deprecation policy ensures you have plenty of time to plan your next move. In the case of PodSecurityPolicy, several options are available to suit a range of needs and use cases. Start planning ahead now for PSP’s eventual removal, and please consider contributing to its replacement! Happy securing!&lt;/p>
&lt;p>&lt;strong>Acknowledgment:&lt;/strong> It takes a wonderful group to make wonderful software. Thanks are due to everyone who has contributed to the PSP replacement effort, especially (in alphabetical order) Tim Allclair, Ian Coldwater, and Jordan Liggitt. It’s been a joy to work with y’all on this.&lt;/p></description></item><item><title>Blog: The Evolution of Kubernetes Dashboard</title><link>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</guid><description>
&lt;p>Authors: Marcin Maciaszczyk, Kubermatic &amp;amp; Sebastian Florek, Kubermatic&lt;/p>
&lt;p>In October 2020, the Kubernetes Dashboard officially turned five. As main project maintainers, we can barely believe that so much time has passed since our very first commits to the project. However, looking back with a bit of nostalgia, we realize that quite a lot has happened since then. Now it’s due time to celebrate “our baby” with a short recap.&lt;/p>
&lt;h2 id="how-it-all-began">How It All Began&lt;/h2>
&lt;p>The initial idea behind the Kubernetes Dashboard project was to provide a web interface for Kubernetes. We wanted to reflect the kubectl functionality through an intuitive web UI. The main benefit from using the UI is to be able to quickly see things that do not work as expected (monitoring and troubleshooting). Also, the Kubernetes Dashboard is a great starting point for users that are new to the Kubernetes ecosystem.&lt;/p>
&lt;p>The very &lt;a href="https://github.com/kubernetes/dashboard/commit/5861187fa807ac1cc2d9b2ac786afeced065076c">first commit&lt;/a> to the Kubernetes Dashboard was made by Filip Grządkowski from Google on 16th October 2015 – just a few months from the initial commit to the Kubernetes repository. Our initial commits go back to November 2015 (&lt;a href="https://github.com/kubernetes/dashboard/commit/09e65b6bb08c49b926253de3621a73da05e400fd">Sebastian committed on 16 November 2015&lt;/a>; &lt;a href="https://github.com/kubernetes/dashboard/commit/1da4b1c25ef040818072c734f71333f9b4733f55">Marcin committed on 23 November 2015&lt;/a>). Since that time, we’ve become regular contributors to the project. For the next two years, we worked closely with the Googlers, eventually becoming main project maintainers ourselves.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/first-ui.png"
alt="The First Version of the User Interface"/> &lt;figcaption>
&lt;p>The First Version of the User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/along-the-way-ui.png"
alt="Prototype of the New User Interface"/> &lt;figcaption>
&lt;p>Prototype of the New User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/current-ui.png"
alt="The Current User Interface"/> &lt;figcaption>
&lt;p>The Current User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>As you can see, the initial look and feel of the project were completely different from the current one. We have changed the design multiple times. The same has happened with the code itself.&lt;/p>
&lt;h2 id="growing-up-the-big-migration">Growing Up - The Big Migration&lt;/h2>
&lt;p>At &lt;a href="https://github.com/kubernetes/dashboard/pull/2727">the beginning of 2018&lt;/a>, we reached a point where AngularJS was getting closer to the end of its life, while the new Angular versions were published quite often. A lot of the libraries and the modules that we were using were following the trend. That forced us to spend a lot of the time rewriting the frontend part of the project to make it work with newer technologies.&lt;/p>
&lt;p>The migration came with many benefits like being able to refactor a lot of the code, introduce design patterns, reduce code complexity, and benefit from the new modules. However, you can imagine that the scale of the migration was huge. Luckily, there were a number of contributions from the community helping us with the resource support, new Kubernetes version support, i18n, and much more. After many long days and nights, we finally released the &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.0.0-beta1">first beta version&lt;/a> in July 2019, followed by the &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.0.0">2.0 release&lt;/a> in April 2020 — our baby had grown up.&lt;/p>
&lt;h2 id="where-are-we-standing-in-2021">Where Are We Standing in 2021?&lt;/h2>
&lt;p>Due to limited resources, unfortunately, we were not able to offer extensive support for many different Kubernetes versions. So, we’ve decided to always try and support the latest Kubernetes version available at the time of the Kubernetes Dashboard release. The latest release, &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.2.0">Dashboard v2.2.0&lt;/a> provides support for Kubernetes v1.20.&lt;/p>
&lt;p>On top of that, we put in a great deal of effort into &lt;a href="https://github.com/kubernetes/dashboard/issues/5232">improving resource support&lt;/a>. Meanwhile, we do offer support for most of the Kubernetes resources. Also, the Kubernetes Dashboard supports multiple languages: English, German, French, Japanese, Korean, Chinese (Traditional, Simplified, Traditional Hong Kong). Persian and Russian localizations are currently in progress. Moreover, we are working on the support for 3rd party themes and the design of the app in general. As you can see, quite a lot of things are going on.&lt;/p>
&lt;p>Luckily, we do have regular contributors with domain knowledge who are taking care of the project, updating the Helm charts, translations, Go modules, and more. But as always, there could be many more hands on deck. So if you are thinking about contributing to Kubernetes, keep us in mind ;)&lt;/p>
&lt;h2 id="what-s-next">What’s Next&lt;/h2>
&lt;p>The Kubernetes Dashboard has been growing and prospering for more than 5 years now. It provides the community with an intuitive Web UI, thereby decreasing the complexity of Kubernetes and increasing its accessibility to new community members. We are proud of what the project has achieved so far, but this is by far not the end. These are our priorities for the future:&lt;/p>
&lt;ul>
&lt;li>Keep providing support for the new Kubernetes versions&lt;/li>
&lt;li>Keep improving the support for the existing resources&lt;/li>
&lt;li>Keep working on auth system improvements&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/pull/5449">Rewrite the API to use gRPC and shared informers&lt;/a>: This will allow us to improve the performance of the application but, most importantly, to support live updates coming from the Kubernetes project. It is one of the most requested features from the community.&lt;/li>
&lt;li>Split the application into two containers, one with the UI and the second with the API running inside.&lt;/li>
&lt;/ul>
&lt;h2 id="the-kubernetes-dashboard-in-numbers">The Kubernetes Dashboard in Numbers&lt;/h2>
&lt;ul>
&lt;li>Initial commit made on October 16, 2015&lt;/li>
&lt;li>Over 100 million pulls from Dockerhub since the v2 release&lt;/li>
&lt;li>8 supported languages and the next 2 in progress&lt;/li>
&lt;li>Over 3360 closed PRs&lt;/li>
&lt;li>Over 2260 closed issues&lt;/li>
&lt;li>100% coverage of the supported core Kubernetes resources&lt;/li>
&lt;li>Over 9000 stars on GitHub&lt;/li>
&lt;li>Over 237 000 lines of code&lt;/li>
&lt;/ul>
&lt;h2 id="join-us">Join Us&lt;/h2>
&lt;p>As mentioned earlier, we are currently looking for more people to help us further develop and grow the project. We are open to contributions in multiple areas, i.e., &lt;a href="https://github.com/kubernetes/dashboard/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">issues with help wanted label&lt;/a>. Please feel free to reach out via GitHub or the #sig-ui channel in the &lt;a href="https://slack.k8s.io/">Kubernetes Slack&lt;/a>.&lt;/p></description></item><item><title>Blog: A Custom Kubernetes Scheduler to Orchestrate Highly Available Applications</title><link>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Chris Seto (Cockroach Labs)&lt;/p>
&lt;p>As long as you're willing to follow the rules, deploying on Kubernetes and air travel can be quite pleasant. More often than not, things will &amp;quot;just work&amp;quot;. However, if one is interested in travelling with an alligator that must remain alive or scaling a database that must remain available, the situation is likely to become a bit more complicated. It may even be easier to build one's own plane or database for that matter. Travelling with reptiles aside, scaling a highly available stateful system is no trivial task.&lt;/p>
&lt;p>Scaling any system has two main components:&lt;/p>
&lt;ol>
&lt;li>Adding or removing infrastructure that the system will run on, and&lt;/li>
&lt;li>Ensuring that the system knows how to handle additional instances of itself being added and removed.&lt;/li>
&lt;/ol>
&lt;p>Most stateless systems, web servers for example, are created without the need to be aware of peers. Stateful systems, which includes databases like CockroachDB, have to coordinate with their peer instances and shuffle around data. As luck would have it, CockroachDB handles data redistribution and replication. The tricky part is being able to tolerate failures during these operations by ensuring that data and instances are distributed across many failure domains (availability zones).&lt;/p>
&lt;p>One of Kubernetes' responsibilities is to place &amp;quot;resources&amp;quot; (e.g, a disk or container) into the cluster and satisfy the constraints they request. For example: &amp;quot;I must be in availability zone &lt;em>A&lt;/em>&amp;quot; (see &lt;a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/#nodes-are-labeled">Running in multiple zones&lt;/a>), or &amp;quot;I can't be placed onto the same node as this other Pod&amp;quot; (see &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">Affinity and anti-affinity&lt;/a>).&lt;/p>
&lt;p>As an addition to those constraints, Kubernetes offers &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">Statefulsets&lt;/a> that provide identity to Pods as well as persistent storage that &amp;quot;follows&amp;quot; these identified pods. Identity in a StatefulSet is handled by an increasing integer at the end of a pod's name. It's important to note that this integer must always be contiguous: in a StatefulSet, if pods 1 and 3 exist then pod 2 must also exist.&lt;/p>
&lt;p>Under the hood, CockroachCloud deploys each region of CockroachDB as a StatefulSet in its own Kubernetes cluster - see &lt;a href="https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html">Orchestrate CockroachDB in a Single Kubernetes Cluster&lt;/a>.
In this article, I'll be looking at an individual region, one StatefulSet and one Kubernetes cluster which is distributed across at least three availability zones.&lt;/p>
&lt;p>A three-node CockroachCloud cluster would look something like this:&lt;/p>
&lt;p>&lt;img src="image01.png" alt="3-node, multi-zone cockroachdb cluster">&lt;/p>
&lt;p>When adding additional resources to the cluster we also distribute them across zones. For the speediest user experience, we add all Kubernetes nodes at the same time and then scale up the StatefulSet.&lt;/p>
&lt;p>&lt;img src="image02.png" alt="illustration of phases: adding Kubernetes nodes to the multi-zone cockroachdb cluster">&lt;/p>
&lt;p>Note that anti-affinities are satisfied no matter the order in which pods are assigned to Kubernetes nodes. In the example, pods 0, 1 and 2 were assigned to zones A, B, and C respectively, but pods 3 and 4 were assigned in a different order, to zones B and A respectively. The anti-affinity is still satisfied because the pods are still placed in different zones.&lt;/p>
&lt;p>To remove resources from a cluster, we perform these operations in reverse order.&lt;/p>
&lt;p>We first scale down the StatefulSet and then remove from the cluster any nodes lacking a CockroachDB pod.&lt;/p>
&lt;p>&lt;img src="image03.png" alt="illustration of phases: scaling down pods in a multi-zone cockroachdb cluster in Kubernetes">&lt;/p>
&lt;p>Now, remember that pods in a StatefulSet of size &lt;em>n&lt;/em> must have ids in the range &lt;code>[0,n)&lt;/code>. When scaling down a StatefulSet by &lt;em>m&lt;/em>, Kubernetes removes &lt;em>m&lt;/em> pods, starting from the highest ordinals and moving towards the lowest, &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">the reverse in which they were added&lt;/a>.
Consider the cluster topology below:&lt;/p>
&lt;p>&lt;img src="image04.png" alt="illustration: cockroachdb cluster: 6 nodes distributed across 3 availability zones">&lt;/p>
&lt;p>As ordinals 5 through 3 are removed from this cluster, the statefulset continues to have a presence across all 3 availability zones.&lt;/p>
&lt;p>&lt;img src="image05.png" alt="illustration: removing 3 nodes from a 6-node, 3-zone cockroachdb cluster">&lt;/p>
&lt;p>However, Kubernetes' scheduler doesn't &lt;em>guarantee&lt;/em> the placement above as we expected at first.&lt;/p>
&lt;p>Our combined knowledge of the following is what lead to this misconception.&lt;/p>
&lt;ul>
&lt;li>Kubernetes' ability to &lt;a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/#pods-are-spread-across-zones">automatically spread Pods across zone&lt;/a>&lt;/li>
&lt;li>The behavior that a StatefulSet with &lt;em>n&lt;/em> replicas, when Pods are being deployed, they are created sequentially, in order from &lt;code>{0..n-1}&lt;/code>. See &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">StatefulSet&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;p>Consider the following topology:&lt;/p>
&lt;p>&lt;img src="image06.png" alt="illustration: 6-node cockroachdb cluster distributed across 3 availability zones">&lt;/p>
&lt;p>These pods were created in order and they are spread across all availability zones in the cluster. When ordinals 5 through 3 are terminated, this cluster will lose its presence in zone C!&lt;/p>
&lt;p>&lt;img src="image07.png" alt="illustration: terminating 3 nodes in 6-node cluster spread across 3 availability zones, where 2/2 nodes in the same availability zone are terminated, knocking out that AZ">&lt;/p>
&lt;p>Worse yet, our automation, at the time, would remove Nodes A-2, B-2, and C-2. Leaving CRDB-1 in an unscheduled state as persistent volumes are only available in the zone they are initially created in.&lt;/p>
&lt;p>To correct the latter issue, we now employ a &amp;quot;hunt and peck&amp;quot; approach to removing machines from a cluster. Rather than blindly removing Kubernetes nodes from the cluster, only nodes without a CockroachDB pod would be removed. The much more daunting task was to wrangle the Kubernetes scheduler.&lt;/p>
&lt;h2 id="a-session-of-brainstorming-left-us-with-3-options">A session of brainstorming left us with 3 options:&lt;/h2>
&lt;h3 id="1-upgrade-to-kubernetes-1-18-and-make-use-of-pod-topology-spread-constraints">1. Upgrade to kubernetes 1.18 and make use of Pod Topology Spread Constraints&lt;/h3>
&lt;p>While this seems like it could have been the perfect solution, at the time of writing Kubernetes 1.18 was unavailable on the two most common managed Kubernetes services in public cloud, EKS and GKE.
Furthermore, &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">pod topology spread constraints&lt;/a> were still a &lt;a href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">beta feature in 1.18&lt;/a> which meant that it &lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#kubernetes_feature_choices">wasn't guaranteed to be available in managed clusters&lt;/a> even when v1.18 became available.
The entire endeavour was concerningly reminiscent of checking &lt;a href="https://caniuse.com/">caniuse.com&lt;/a> when Internet Explorer 8 was still around.&lt;/p>
&lt;h3 id="2-deploy-a-statefulset-per-zone">2. Deploy a statefulset &lt;em>per zone&lt;/em>.&lt;/h3>
&lt;p>Rather than having one StatefulSet distributed across all availability zones, a single StatefulSet with node affinities per zone would allow manual control over our zonal topology.
Our team had considered this as an option in the past which made it particularly appealing.
Ultimately, we decided to forego this option as it would have required a massive overhaul to our codebase and performing the migration on existing customer clusters would have been an equally large undertaking.&lt;/p>
&lt;h3 id="3-write-a-custom-kubernetes-scheduler">3. Write a custom Kubernetes scheduler.&lt;/h3>
&lt;p>Thanks to an example from &lt;a href="https://github.com/kelseyhightower/scheduler">Kelsey Hightower&lt;/a> and a blog post from &lt;a href="https://banzaicloud.com/blog/k8s-custom-scheduler/">Banzai Cloud&lt;/a>, we decided to dive in head first and write our own &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">custom Kubernetes scheduler&lt;/a>.
Once our proof-of-concept was deployed and running, we quickly discovered that the Kubernetes' scheduler is also responsible for mapping persistent volumes to the Pods that it schedules.
The output of &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/#verifying-that-the-pods-were-scheduled-using-the-desired-schedulers">&lt;code>kubectl get events&lt;/code>&lt;/a> had led us to believe there was another system at play.
In our journey to find the component responsible for storage claim mapping, we discovered the &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">kube-scheduler plugin system&lt;/a>. Our next POC was a &lt;code>Filter&lt;/code> plugin that determined the appropriate availability zone by pod ordinal, and it worked flawlessly!&lt;/p>
&lt;p>Our &lt;a href="https://github.com/cockroachlabs/crl-scheduler">custom scheduler plugin&lt;/a> is open source and runs in all of our CockroachCloud clusters.
Having control over how our StatefulSet pods are being scheduled has let us scale out with confidence.
We may look into retiring our plugin once pod topology spread constraints are available in GKE and EKS, but the maintenance overhead has been surprisingly low.
Better still: the plugin's implementation is orthogonal to our business logic. Deploying it, or retiring it for that matter, is as simple as changing the &lt;code>schedulerName&lt;/code> field in our StatefulSet definitions.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/_ostriches">Chris Seto&lt;/a> is a software engineer at Cockroach Labs and works on their Kubernetes automation for &lt;a href="https://cockroachlabs.cloud">CockroachCloud&lt;/a>, CockroachDB.&lt;/em>&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers</title><link>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Shihang Zhang (Google)&lt;/p>
&lt;p>Typically when a &lt;a href="https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md">CSI&lt;/a> driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.&lt;/p>
&lt;p>Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.&lt;/p>
&lt;p>Both of them exhibit the following drawbacks:&lt;/p>
&lt;ul>
&lt;li>Violating the principle of least privilege&lt;/li>
&lt;li>Every CSI driver needs to re-implement the logic of getting the pod’s service account token&lt;/li>
&lt;/ul>
&lt;p>The second approach is more problematic due to:&lt;/p>
&lt;ul>
&lt;li>The audience of the token defaults to the kube-apiserver&lt;/li>
&lt;li>The token is not guaranteed to be available (e.g. &lt;code>AutomountServiceAccountToken=false&lt;/code>)&lt;/li>
&lt;li>The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See &lt;a href="https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission">file permission section for service account token&lt;/a>&lt;/li>
&lt;li>The token might be legacy Kubernetes service account token which doesn’t expire if &lt;code>BoundServiceAccountTokenVolume=false&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Kubernetes 1.20 introduces an alpha feature, &lt;code>CSIServiceAccountToken&lt;/code>, to improve the security posture. The new feature allows CSI drivers to receive pods' &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md">bound service account tokens&lt;/a>.&lt;/p>
&lt;p>This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.&lt;/p>
&lt;h2 id="pod-impersonation">Pod Impersonation&lt;/h2>
&lt;h3 id="using-gcp-apis">Using GCP APIs&lt;/h3>
&lt;p>Using &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity&lt;/a>, a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to &lt;a href="https://cloud.google.com/iam/docs/reference/sts/rest">exchange for GCP tokens&lt;/a>. The pod's service account token is plumbed through the volume context in &lt;code>NodePublishVolume&lt;/code> RPC calls when the feature &lt;code>CSIServiceAccountToken&lt;/code> is enabled. For example: accessing &lt;a href="https://cloud.google.com/secret-manager/">Google Secret Manager&lt;/a> via a &lt;a href="https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp">secret store CSI driver&lt;/a>.&lt;/p>
&lt;h3 id="using-vault">Using Vault&lt;/h3>
&lt;p>If users configure &lt;a href="https://www.vaultproject.io/docs/auth/kubernetes">Kubernetes as an auth method&lt;/a>, Vault uses the &lt;code>TokenReview&lt;/code> API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, &lt;a href="https://github.com/hashicorp/secrets-store-csi-driver-provider-vault">secrets store CSI driver&lt;/a> and &lt;a href="https://github.com/jetstack/cert-manager-csi">cert manager CSI driver&lt;/a>.&lt;/p>
&lt;h2 id="short-lived-volumes">Short-lived Volumes&lt;/h2>
&lt;p>To keep short-lived volumes such as certificates effective, CSI drivers can specify &lt;code>RequiresRepublish=true&lt;/code> in their&lt;code>CSIDriver&lt;/code> object to have the kubelet periodically call &lt;code>NodePublishVolume&lt;/code> on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md">KEP-1855: Service Account Token for CSI Driver&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/token-requests.html">Token Requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Your feedback is always welcome!&lt;/p>
&lt;ul>
&lt;li>SIG-Auth &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#meetings">meets regularly&lt;/a> and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#contact">Slack and the mailing list&lt;/a>&lt;/li>
&lt;li>SIG-Storage &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets regularly&lt;/a> and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and the mailing list&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Third Party Device Metrics Reaches GA</title><link>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Renaud Gaubert (NVIDIA), David Ashpole (Google), and Pramod Ramarao (NVIDIA)&lt;/p>
&lt;p>With Kubernetes 1.20, infrastructure teams who manage large scale Kubernetes clusters, are seeing the graduation of two exciting and long awaited features:&lt;/p>
&lt;ul>
&lt;li>The Pod Resources API (introduced in 1.13) is finally graduating to GA. This allows Kubernetes plugins to obtain information about the node’s resource usage and assignment; for example: which pod/container consumes which device.&lt;/li>
&lt;li>The &lt;code>DisableAcceleratorMetrics&lt;/code> feature (introduced in 1.19) is graduating to beta and will be enabled by default. This removes device metrics reported by the kubelet in favor of the new plugin architecture.&lt;/li>
&lt;/ul>
&lt;p>Many of the features related to fundamental device support (device discovery, plugin, and monitoring) are reaching a strong level of stability.
Kubernetes users should see these features as stepping stones to enable more complex use cases (networking, scheduling, storage, etc.)!&lt;/p>
&lt;p>One such example is Non Uniform Memory Access (NUMA) placement where, when selecting a device, an application typically wants to ensure that data transfer between CPU Memory and Device Memory is as fast as possible. In some cases, incorrect NUMA placement can nullify the benefit of offloading compute to an external device.&lt;/p>
&lt;p>If these are topics of interest to you, consider joining the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">Kubernetes Node Special Insterest Group&lt;/a> (SIG) for all topics related to the Kubernetes node, the COD (container orchestrated device) workgroup for topics related to runtimes, or the resource management forum for topics related to resource management!&lt;/p>
&lt;h2 id="the-pod-resources-api-why-does-it-need-to-exist">The Pod Resources API - Why does it need to exist?&lt;/h2>
&lt;p>Kubernetes is a vendor neutral platform. If we want it to support device monitoring, adding vendor-specific code in the Kubernetes code base is not an ideal solution. Ultimately, devices are a domain where deep expertise is needed and the best people to add and maintain code in that area are the device vendors themselves.&lt;/p>
&lt;p>The Pod Resources API was built as a solution to this issue. Each vendor can build and maintain their own out-of-tree monitoring plugin. This monitoring plugin, often deployed as a separate pod within a cluster, can then associate the metrics a device emits with the associated pod that's using it.&lt;/p>
&lt;p>For example, use the NVIDIA GPU dcgm-exporter to scrape metrics in Prometheus format:&lt;/p>
&lt;pre>&lt;code>$ curl -sL http://127.0.01:8080/metrics
# HELP DCGM_FI_DEV_SM_CLOCK SM clock frequency (in MHz).
# TYPE DCGM_FI_DEV_SM_CLOCK gauge
# HELP DCGM_FI_DEV_MEM_CLOCK Memory clock frequency (in MHz).
# TYPE DCGM_FI_DEV_MEM_CLOCK gauge
# HELP DCGM_FI_DEV_MEMORY_TEMP Memory temperature (in C).
# TYPE DCGM_FI_DEV_MEMORY_TEMP gauge
...
DCGM_FI_DEV_SM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 139
DCGM_FI_DEV_MEM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 405
DCGM_FI_DEV_MEMORY_TEMP{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 9223372036854775794
&lt;/code>&lt;/pre>&lt;p>Each agent is expected to adhere to the node monitoring guidelines. In other words, plugins are expected to generate metrics in Prometheus format, and new metrics should not have any dependency on the Kubernetes base directly.&lt;/p>
&lt;p>This allows consumers of the metrics to use a compatible monitoring pipeline to collect and analyze metrics from a variety of agents, even if they are maintained by different vendors.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/metrics-chart.png" alt="Device metrics flowchart">&lt;/p>
&lt;h2 id="nvidia-gpu-metrics-deprecated">Disabling the NVIDIA GPU metrics - Warning&lt;/h2>
&lt;p>With the graduation of the plugin monitoring system, Kubernetes is deprecating the NVIDIA GPU metrics that are being reported by the kubelet.&lt;/p>
&lt;p>With the &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/#disable-accelerator-metrics">DisableAcceleratorMetrics&lt;/a> feature being enabled by default in Kubernetes 1.20, NVIDIA GPUs are no longer special citizens in Kubernetes. This is a good thing in the spirit of being vendor-neutral, and enables the most suited people to maintain their plugin on their own release schedule!&lt;/p>
&lt;p>Users will now need to either install the &lt;a href="https://github.com/NVIDIA/gpu-monitoring-tools">NVIDIA GDGM exporter&lt;/a> or use &lt;a href="https://github.com/nvidia/go-nvml">bindings&lt;/a> to gather more accurate and complete metrics about NVIDIA GPUs. This deprecation means that you can no longer rely on metrics that were reported by kubelet, such as &lt;code>container_accelerator_duty_cycle&lt;/code> or &lt;code>container_accelerator_memory_used_bytes&lt;/code> which were used to gather NVIDIA GPU memory utilization.&lt;/p>
&lt;p>This means that users who used to rely on the NVIDIA GPU metrics reported by the kubelet, will need to update their reference and deploy the NVIDIA plugin. Namely the different metrics reported by Kubernetes map to the following metrics:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Kubernetes Metrics&lt;/th>
&lt;th>NVIDIA dcgm-exporter metric&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>container_accelerator_duty_cycle&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_GPU_UTIL&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>container_accelerator_memory_used_bytes&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_FB_USED&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>container_accelerator_memory_total_bytes&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_FB_FREE + DCGM_FI_DEV_FB_USED&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>You might also be interested in other metrics such as &lt;code>DCGM_FI_DEV_GPU_TEMP&lt;/code> (the GPU temperature) or DCGM_FI_DEV_POWER_USAGE (the power usage). The &lt;a href="https://github.com/NVIDIA/gpu-monitoring-tools/blob/d5c9bb55b4d1529ca07068b7f81e690921ce2b59/etc/dcgm-exporter/default-counters.csv">default set&lt;/a> is available in Nvidia's &lt;a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/group__dcgmFieldIdentifiers.html">Data Center GPU Manager documentation&lt;/a>.&lt;/p>
&lt;p>Note that for this release you can still set the &lt;code>DisableAcceleratorMetrics&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> to &lt;em>false&lt;/em>, effectively re-enabling the ability for the kubelet to report NVIDIA GPU metrics.&lt;/p>
&lt;p>Paired with the graduation of the Pod Resources API, these tools can be used to generate GPU telemetry &lt;a href="https://grafana.com/grafana/dashboards/12239">that can be used in visualization dashboards&lt;/a>, below is an example:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/grafana.png" alt="Grafana visualization of device metrics">&lt;/p>
&lt;h2 id="the-pod-resources-api-what-can-i-go-on-to-do-with-this">The Pod Resources API - What can I go on to do with this?&lt;/h2>
&lt;p>As soon as this interface was introduced, many vendors started using it for widely different use cases! To list a few examples:&lt;/p>
&lt;p>The &lt;a href="https://github.com/openstack/kuryr-kubernetes">kuryr-kubernetes&lt;/a> CNI plugin in tandem with &lt;a href="https://github.com/intel/sriov-network-device-plugin">intel-sriov-device-plugin&lt;/a>. This allowed the CNI plugin to know which allocation of SR-IOV Virtual Functions (VFs) the kubelet made and use that information to correctly setup the container network namespace and use a device with the appropriate NUMA node. We also expect this interface to be used to track the allocated and available resources with information about the NUMA topology of the worker node.&lt;/p>
&lt;p>Another use-case is GPU telemetry, where GPU metrics can be associated with the containers and pods that the GPU is assigned to. One such example is the NVIDIA &lt;code>dcgm-exporter&lt;/code>, but others can be easily built in the same paradigm.&lt;/p>
&lt;p>The Pod Resources API is a simple gRPC service which informs clients of the pods the kubelet knows. The information concerns the devices assignment the kubelet made and the assignment of CPUs. This information is obtained from the internal state of the kubelet's Device Manager and CPU Manager respectively.&lt;/p>
&lt;p>You can see below a sample example of the API and how a go client could use that information in a few lines:&lt;/p>
&lt;pre>&lt;code>service PodResourcesLister {
rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}
// Kubernetes 1.21
rpc Watch(WatchPodResourcesRequest) returns (stream WatchPodResourcesResponse) {}
}
&lt;/code>&lt;/pre>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
ctx, cancel &lt;span style="color:#666">:=&lt;/span> context.&lt;span style="color:#00a000">WithTimeout&lt;/span>(context.&lt;span style="color:#00a000">Background&lt;/span>(), connectionTimeout)
&lt;span style="color:#a2f;font-weight:bold">defer&lt;/span> &lt;span style="color:#00a000">cancel&lt;/span>()
socket &lt;span style="color:#666">:=&lt;/span> &lt;span style="color:#b44">&amp;#34;/var/lib/kubelet/pod-resources/kubelet.sock&amp;#34;&lt;/span>
conn, err &lt;span style="color:#666">:=&lt;/span> grpc.&lt;span style="color:#00a000">DialContext&lt;/span>(ctx, socket, grpc.&lt;span style="color:#00a000">WithInsecure&lt;/span>(), grpc.&lt;span style="color:#00a000">WithBlock&lt;/span>(),
grpc.&lt;span style="color:#00a000">WithDialer&lt;/span>(&lt;span style="color:#a2f;font-weight:bold">func&lt;/span>(addr &lt;span style="color:#0b0;font-weight:bold">string&lt;/span>, timeout time.Duration) (net.Conn, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> net.&lt;span style="color:#00a000">DialTimeout&lt;/span>(&lt;span style="color:#b44">&amp;#34;unix&amp;#34;&lt;/span>, addr, timeout)
}),
)
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f">panic&lt;/span>(err)
}
client &lt;span style="color:#666">:=&lt;/span> podresourcesapi.&lt;span style="color:#00a000">NewPodResourcesListerClient&lt;/span>(conn)
resp, err &lt;span style="color:#666">:=&lt;/span> client.&lt;span style="color:#00a000">List&lt;/span>(ctx, &lt;span style="color:#666">&amp;amp;&lt;/span>podresourcesapi.ListPodResourcesRequest{})
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f">panic&lt;/span>(err)
}
net.&lt;span style="color:#00a000">Printf&lt;/span>(&lt;span style="color:#b44">&amp;#34;%+v\n&amp;#34;&lt;/span>, resp)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, note that you can watch the number of requests made to the Pod Resources endpoint by watching the new kubelet metric called &lt;code>pod_resources_endpoint_requests_total&lt;/code> on the kubelet's &lt;code>/metrics&lt;/code> endpoint.&lt;/p>
&lt;h2 id="is-device-monitoring-suitable-for-production-can-i-extend-it-can-i-contribute">Is device monitoring suitable for production? Can I extend it? Can I contribute?&lt;/h2>
&lt;p>Yes! This feature released in 1.13, almost 2 years ago, has seen broad adoption, is already used by different cloud managed services, and with its graduation to G.A in Kubernetes 1.20 is production ready!&lt;/p>
&lt;p>If you are a device vendor, you can start using it today! If you just want to monitor the devices in your cluster, go get the latest version of your monitoring plugin!&lt;/p>
&lt;p>If you feel passionate about that area, join the kubernetes community, help improve the API or contribute the device monitoring plugins!&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>We thank the members of the community who have contributed to this feature or given feedback including members of WG-Resource-Management, SIG-Node and the Resource management forum!&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Granular Control of Volume Permission Changes</title><link>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</link><pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Hemant Kumar, Red Hat &amp;amp; Christian Huffman, Red Hat&lt;/p>
&lt;p>Kubernetes 1.20 brings two important beta features, allowing Kubernetes admins and users alike to have more adequate control over how volume permissions are applied when a volume is mounted inside a Pod.&lt;/p>
&lt;h3 id="allow-users-to-skip-recursive-permission-changes-on-mount">Allow users to skip recursive permission changes on mount&lt;/h3>
&lt;p>Traditionally if your pod is running as a non-root user (&lt;a href="https://twitter.com/thockin/status/1333892204490735617">which you should&lt;/a>), you must specify a &lt;code>fsGroup&lt;/code> inside the pod’s security context so that the volume can be readable and writable by the Pod. This requirement is covered in more detail in &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">here&lt;/a>.&lt;/p>
&lt;p>But one side-effect of setting &lt;code>fsGroup&lt;/code> is that, each time a volume is mounted, Kubernetes must recursively &lt;code>chown()&lt;/code> and &lt;code>chmod()&lt;/code> all the files and directories inside the volume - with a few exceptions noted below. This happens even if group ownership of the volume already matches the requested &lt;code>fsGroup&lt;/code>, and can be pretty expensive for larger volumes with lots of small files, which causes pod startup to take a long time. This scenario has been a &lt;a href="https://github.com/kubernetes/kubernetes/issues/69699">known problem&lt;/a> for a while, and in Kubernetes 1.20 we are providing knobs to opt-out of recursive permission changes if the volume already has the correct permissions.&lt;/p>
&lt;p>When configuring a pod’s security context, set &lt;code>fsGroupChangePolicy&lt;/code> to &amp;quot;OnRootMismatch&amp;quot; so if the root of the volume already has the correct permissions, the recursive permission change can be skipped. Kubernetes ensures that permissions of the top-level directory are changed last the first time it applies permissions.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsUser&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroupChangePolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;OnRootMismatch&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can learn more about this in &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods">Configure volume permission and ownership change policy for Pods&lt;/a>.&lt;/p>
&lt;h3 id="allow-csi-drivers-to-declare-support-for-fsgroup-based-permissions">Allow CSI Drivers to declare support for fsGroup based permissions&lt;/h3>
&lt;p>Although the previous section implied that Kubernetes &lt;em>always&lt;/em> recursively changes permissions of a volume if a Pod has a &lt;code>fsGroup&lt;/code>, this is not strictly true. For certain multi-writer volume types, such as NFS or Gluster, the cluster doesn’t perform recursive permission changes even if the pod has a &lt;code>fsGroup&lt;/code>. Other volume types may not even support &lt;code>chown()&lt;/code>/&lt;code>chmod()&lt;/code>, which rely on Unix-style permission control primitives.&lt;/p>
&lt;p>So how do we know when to apply recursive permission changes and when we shouldn't? For in-tree storage drivers, this was relatively simple. For &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html#introduction">CSI&lt;/a> drivers that could span a multitude of platforms and storage types, this problem can be a bigger challenge.&lt;/p>
&lt;p>Previously, whenever a CSI volume was mounted to a Pod, Kubernetes would attempt to automatically determine if the permissions and ownership should be modified. These methods were imprecise and could cause issues as we already mentioned, depending on the storage type.&lt;/p>
&lt;p>The CSIDriver custom resource now has a &lt;code>.spec.fsGroupPolicy&lt;/code> field, allowing storage drivers to explicitly opt in or out of these recursive modifications. By having the CSI driver specify a policy for the backing volumes, Kubernetes can avoid needless modification attempts. This optimization helps to reduce volume mount time and also cuts own reporting errors about modifications that would never succeed.&lt;/p>
&lt;h4 id="csidriver-fsgrouppolicy-api">CSIDriver FSGroupPolicy API&lt;/h4>
&lt;p>Three FSGroupPolicy values are available as of Kubernetes 1.20, with more planned for future releases.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ReadWriteOnceWithFSType&lt;/strong> - This is the default policy, applied if no &lt;code>fsGroupPolicy&lt;/code> is defined; this preserves the behavior from previous Kubernetes releases. Each volume is examined at mount time to determine if permissions should be recursively applied.&lt;/li>
&lt;li>&lt;strong>File&lt;/strong> - Always attempt to apply permission modifications, regardless of the filesystem type or PersistentVolumeClaim’s access mode.&lt;/li>
&lt;li>&lt;strong>None&lt;/strong> - Never apply permission modifications.&lt;/li>
&lt;/ul>
&lt;h4 id="how-do-i-use-it">How do I use it?&lt;/h4>
&lt;p>The only configuration needed is defining &lt;code>fsGroupPolicy&lt;/code> inside of the &lt;code>.spec&lt;/code> for a CSIDriver. Once that element is defined, any subsequently mounted volumes will automatically use the defined policy. There’s no additional deployment required!&lt;/p>
&lt;h4 id="what-s-next">What’s next?&lt;/h4>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push these implementations to GA in either 1.21 or 1.22.&lt;/p>
&lt;h3 id="how-can-i-learn-more">How can I learn more?&lt;/h3>
&lt;p>This feature is explained in more detail in Kubernetes project documentation: &lt;a href="https://kubernetes-csi.github.io/docs/support-fsgroup.html">CSI Driver fsGroup Support&lt;/a> and &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods">Configure volume permission and ownership change policy for Pods &lt;/a>.&lt;/p>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes Slack channel #csi&lt;/a> and any of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">standard SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and the CSI team.&lt;/p>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Kubernetes Volume Snapshot Moves to GA</title><link>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Xing Yang, VMware &amp;amp; Xiangqian Yu, Google&lt;/p>
&lt;p>The Kubernetes Volume Snapshot feature is now GA in Kubernetes v1.20. It was introduced as &lt;a href="https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/">alpha&lt;/a> in Kubernetes v1.12, followed by a &lt;a href="https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/">second alpha&lt;/a> with breaking changes in Kubernetes v1.13, and promotion to &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">beta&lt;/a> in Kubernetes 1.17. This blog post summarizes the changes releasing the feature from beta to GA.&lt;/p>
&lt;h2 id="what-is-a-volume-snapshot">What is a volume snapshot?&lt;/h2>
&lt;p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to rehydrate a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p>
&lt;h2 id="why-add-volume-snapshots-to-kubernetes">Why add volume snapshots to Kubernetes?&lt;/h2>
&lt;p>Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster-specific” knowledge.&lt;/p>
&lt;p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database’s volumes before starting a database operation.&lt;/p>
&lt;p>By providing a standard way to trigger volume snapshot operations in Kubernetes, this feature allows Kubernetes users to incorporate snapshot operations in a portable manner on any Kubernetes environment regardless of the underlying storage.&lt;/p>
&lt;p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced enterprise-grade storage administration features for Kubernetes, including application or cluster level backup solutions.&lt;/p>
&lt;h2 id="what-s-new-since-beta">What’s new since beta?&lt;/h2>
&lt;p>With the promotion of Volume Snapshot to GA, the feature is enabled by default on standard Kubernetes deployments and cannot be turned off.&lt;/p>
&lt;p>Many enhancements have been made to improve the quality of this feature and to make it production-grade.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The Volume Snapshot APIs and client library were moved to a separate Go module.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A snapshot validation webhook has been added to perform necessary validation on volume snapshot objects. More details can be found in the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1900-volume-snapshot-validation-webhook">Volume Snapshot Validation Webhook Kubernetes Enhancement Proposal&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Along with the validation webhook, the volume snapshot controller will start labeling invalid snapshot objects that already existed. This allows users to identify, remove any invalid objects, and correct their workflows. Once the API is switched to the v1 type, those invalid objects will not be deletable from the system.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To provide better insights into how the snapshot feature is performing, an initial set of operation metrics has been added to the volume snapshot controller.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are more end-to-end tests, running on GCP, that validate the feature in a real Kubernetes cluster. Stress tests (based on Google Persistent Disk and &lt;code>hostPath&lt;/code> CSI Drivers) have been introduced to test the robustness of the system.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Other than introducing tightening validation, there is no difference between the v1beta1 and v1 Kubernetes volume snapshot API. In this release (with Kubernetes 1.20), both v1 and v1beta1 are served while the stored API version is still v1beta1. Future releases will switch the stored version to v1 and gradually remove v1beta1 support.&lt;/p>
&lt;h2 id="which-csi-drivers-support-volume-snapshots">Which CSI drivers support volume snapshots?&lt;/h2>
&lt;p>Snapshots are only supported for CSI drivers, not for in-tree or FlexVolume drivers. Ensure the deployed CSI driver on your cluster has implemented the snapshot interfaces. For more information, see &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI) for Kubernetes GA&lt;/a>.&lt;/p>
&lt;p>Currently more than &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">50 CSI drivers&lt;/a> support the Volume Snapshot feature. The &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE Persistent Disk CSI Driver&lt;/a> has gone through the tests for upgrading from volume snapshots beta to GA. GA level support for other CSI drivers should be available soon.&lt;/p>
&lt;h2 id="who-builds-products-using-volume-snapshots">Who builds products using volume snapshots?&lt;/h2>
&lt;p>As of the publishing of this blog, the following participants from the &lt;a href="https://github.com/kubernetes/community/tree/master/wg-data-protection">Kubernetes Data Protection Working Group&lt;/a> are building products or have already built products using Kubernetes volume snapshots.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.delltechnologies.com/en-us/data-protection/powerprotect-data-manager.htm">Dell-EMC: PowerProtect&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.druva.com/">Druva&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kasten.io/">Kasten K10&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cloud.netapp.com/project-astra">NetApp: Project Astra&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://portworx.com/products/px-backup/">Portworx (PX-Backup)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/purestorage/pso-csi">Pure Storage (Pure Service Orchestrator)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage">Red Hat OpenShift Container Storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://robin.io/storage/">Robin Cloud Native Storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.trilio.io/kubernetes/">TrilioVault for Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-csi">Velero plugin for CSI&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-deploy-volume-snapshots">How to deploy volume snapshots?&lt;/h2>
&lt;p>Volume Snapshot feature contains the following components:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/client/config/crd">Kubernetes Volume Snapshot CRDs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller">Volume snapshot controller&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/validation-webhook">Snapshot validation webhook&lt;/a>&lt;/li>
&lt;li>CSI Driver along with &lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/sidecar-controller">CSI Snapshotter sidecar&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>It is strongly recommended that Kubernetes distributors bundle and deploy the volume snapshot controller, CRDs, and validation webhook as part of their Kubernetes cluster management process (independent of any CSI Driver).&lt;/p>
&lt;blockquote class="warning callout">
&lt;div>&lt;strong>Warning:&lt;/strong> The snapshot validation webhook serves as a critical component to transition smoothly from using v1beta1 to v1 API. Not installing the snapshot validation webhook makes prevention of invalid volume snapshot objects from creation/updating impossible, which in turn will block deletion of invalid volume snapshot objects in coming upgrades.&lt;/div>
&lt;/blockquote>
&lt;p>If your cluster does not come pre-installed with the correct components, you may manually install them. See the &lt;a href="https://github.com/kubernetes-csi/external-snapshotter#readme">CSI Snapshotter&lt;/a> README for details.&lt;/p>
&lt;h2 id="how-to-use-volume-snapshots">How to use volume snapshots?&lt;/h2>
&lt;p>Assuming all the required components (including CSI driver) have been already deployed and running on your cluster, you can create volume snapshots using the &lt;code>VolumeSnapshot&lt;/code> API object, or use an existing &lt;code>VolumeSnapshot&lt;/code> to restore a PVC by specifying the VolumeSnapshot data source on it. For more details, see the &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">volume snapshot documentation&lt;/a>.&lt;/p>
&lt;blockquote class="note callout">
&lt;div>&lt;strong>Note:&lt;/strong> The Kubernetes Snapshot API does not provide any application consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using higher level APIs/controllers.&lt;/div>
&lt;/blockquote>
&lt;h3 id="dynamically-provision-a-volume-snapshot">Dynamically provision a volume snapshot&lt;/h3>
&lt;p>To dynamically provision a volume snapshot, create a &lt;code>VolumeSnapshotClass&lt;/code> API object first.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecret&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecretnamespace&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create a &lt;code>VolumeSnapshot&lt;/code> API object from a PVC by specifying the volume snapshot class.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="importing-an-existing-volume-snapshot-with-kubernetes">Importing an existing volume snapshot with Kubernetes&lt;/h3>
&lt;p>To import a pre-existing volume snapshot into Kubernetes, manually create a &lt;code>VolumeSnapshotContent&lt;/code> object first.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-content&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>7bdd0de3-xxx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create a &lt;code>VolumeSnapshot&lt;/code> object pointing to the &lt;code>VolumeSnapshotContent&lt;/code> object.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotContentName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-content&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="rehydrate-volume-from-snapshot">Rehydrate volume from snapshot&lt;/h3>
&lt;p>A bound and ready &lt;code>VolumeSnapshot&lt;/code> object can be used to rehydrate a new volume with data pre-populated from snapshotted data as shown here:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-restore&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>demo-namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-storageclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSource&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="how-to-add-support-for-snapshots-in-a-csi-driver">How to add support for snapshots in a CSI driver?&lt;/h2>
&lt;p>See the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI spec&lt;/a> and the &lt;a href="https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html">Kubernetes-CSI Driver Developer Guide&lt;/a> for more details on how to implement the snapshot feature in a CSI driver.&lt;/p>
&lt;h2 id="what-are-the-limitations">What are the limitations?&lt;/h2>
&lt;p>The GA implementation of volume snapshots for Kubernetes has the following limitations:&lt;/p>
&lt;ul>
&lt;li>Does not support reverting an existing PVC to an earlier state represented by a snapshot (only supports provisioning a new volume from a snapshot).&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-learn-more">How to learn more?&lt;/h3>
&lt;p>The code repository for snapshot APIs and controller is here: &lt;a href="https://github.com/kubernetes-csi/external-snapshotter">https://github.com/kubernetes-csi/external-snapshotter&lt;/a>&lt;/p>
&lt;p>Check out additional documentation on the snapshot feature here: &lt;a href="http://k8s.io/docs/concepts/storage/volume-snapshots">http://k8s.io/docs/concepts/storage/volume-snapshots&lt;/a> and &lt;a href="https://kubernetes-csi.github.io/docs/">https://kubernetes-csi.github.io/docs/&lt;/a>&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p>
&lt;p>We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach GA. We want to thank Saad Ali, Michelle Au, Tim Hockin, and Jordan Liggitt for their insightful reviews and thorough consideration with the design, thank Andi Li for his work on adding the support of the snapshot validation webhook, thank Grant Griffiths on implementing metrics support in the snapshot controller and handling password rotation in the validation webhook, thank Chris Henzie, Raunak Shah, and Manohar Reddy for writing critical e2e tests to meet the scalability and stability requirements for graduation, thank Kartik Sharma for moving snapshot APIs and client lib to a separate go module, and thank Raunak Shah and Prafull Ladha for their help with upgrade testing from beta to GA.&lt;/p>
&lt;p>There are many more people who have helped to move the snapshot feature from beta to GA. We want to thank everyone who has contributed to this effort:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/AndiLi99">Andi Li&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/bswartz">Ben Swartzlander&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/chrishenzie">Chris Henzie&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/huffmanca">Christian Huffman&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ggriffiths">Grant Griffiths&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/humblec">Humble Devassy Chirammal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jsafrane">Jan Šafránek&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Jiawei0227">Jiawei Wang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jingxu97">Jing Xu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/liggitt">Jordan Liggitt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Kartik494">Kartik Sharma&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Madhu-1">Madhu Rajanna&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/boddumanohar">Manohar Reddy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/msau42">Michelle Au&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/pohly">Patrick Ohly&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prafull01">Prafull Ladha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prateekpandey14">Prateek Pandey&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/RaunakShah">Raunak Shah&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saad-ali">Saad Ali&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saikat-royc">Saikat Roychowdhury&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/thockin">Tim Hockin&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/yuxiangqian">Xiangqian Yu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/xing-yang">Xing Yang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/zhucan">Zhu Can&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>For those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>We also hold regular &lt;a href="https://docs.google.com/document/d/15tLCV3csvjHbKb16DVk-mfUmFry_Rlwo-2uG6KNGsfw/edit#">Data Protection Working Group meetings&lt;/a>. New attendees are welcome to join in discussions.&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: The Raddest Release</title><link>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md">Kubernetes 1.20 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha.&lt;/p>
&lt;p>The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="volume-snapshot-operations-goes-stable">Volume Snapshot Operations Goes Stable&lt;/h3>
&lt;p>This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers.&lt;/p>
&lt;p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions.&lt;/p>
&lt;p>Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster.&lt;/p>
&lt;h3 id="kubectl-debug-graduates-to-beta">Kubectl Debug Graduates to Beta&lt;/h3>
&lt;p>The &lt;code>kubectl alpha debug&lt;/code> features graduates to beta in 1.20, becoming &lt;code>kubectl debug&lt;/code>. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include:&lt;/p>
&lt;ul>
&lt;li>Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.&lt;/li>
&lt;li>Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)&lt;/li>
&lt;li>Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem.&lt;/li>
&lt;/ul>
&lt;p>Note that as a new built-in command, &lt;code>kubectl debug&lt;/code> takes priority over any kubectl plugin named “debug”. You must rename the affected plugin.&lt;/p>
&lt;p>Invocations using &lt;code>kubectl alpha debug&lt;/code> are now deprecated and will be removed in a subsequent release. Update your scripts to use &lt;code>kubectl debug&lt;/code>. For more information about &lt;code>kubectl debug&lt;/code>, see &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/">Debugging Running Pods&lt;/a>.&lt;/p>
&lt;h3 id="beta-api-priority-and-fairness">Beta: API Priority and Fairness&lt;/h3>
&lt;p>Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows &lt;code>kube-apiserver&lt;/code> to categorize incoming requests by priority levels.&lt;/p>
&lt;h3 id="alpha-with-updates-ipv4-ipv6">Alpha with updates: IPV4/IPV6&lt;/h3>
&lt;p>The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa.&lt;/p>
&lt;h3 id="ga-process-pid-limiting-for-stability">GA: Process PID Limiting for Stability&lt;/h3>
&lt;p>Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine.&lt;/p>
&lt;p>Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node.
After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both &lt;code>SupportNodePidsLimit&lt;/code> (node-to-pod PID isolation) and &lt;code>SupportPodPidsLimit&lt;/code> (ability to limit PIDs per pod).&lt;/p>
&lt;h3 id="alpha-graceful-node-shutdown">Alpha: Graceful node shutdown&lt;/h3>
&lt;p>Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The &lt;code>GracefulNodeShutdown&lt;/code> feature is now in Alpha. &lt;code>GracefulNodeShutdown&lt;/code> makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown.&lt;/p>
&lt;h2 id="major-changes">Major Changes&lt;/h2>
&lt;h3 id="dockershim-deprecation">Dockershim Deprecation&lt;/h3>
&lt;p>Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a &lt;a href="https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/">detailed blog post about deprecation&lt;/a> with &lt;a href="https://blog.k8s.io/2020/12/02/dockershim-faq/">a dedicated FAQ page for it&lt;/a>.&lt;/p>
&lt;h3 id="exec-probe-timeout-handling">Exec Probe Timeout Handling&lt;/h3>
&lt;p>A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field &lt;code>timeoutSeconds&lt;/code> was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of &lt;code>1 second&lt;/code> will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called &lt;code>ExecProbeTimeout&lt;/code>, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to &lt;code>false&lt;/code>.&lt;/p>
&lt;p>Please review the updated documentation regarding &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">configuring probes&lt;/a> for more details.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/585">RuntimeClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1929">Built-in API Types Defaults&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/950">Add Pod-Startup Liveness-Probe Holdoff&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1001">Support CRI-ContainerD On Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/614">SCTP Support for Services&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">Adding AppProtocol To Services And Endpoints&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="notable-feature-updates">Notable Feature Updates&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="release-notes">Release notes&lt;/h1>
&lt;p>You can check out the full details of the 1.20 release in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">release notes&lt;/a>.&lt;/p>
&lt;h1 id="availability-of-release">Availability of release&lt;/h1>
&lt;p>Kubernetes 1.20 is available for &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0">download on GitHub&lt;/a>. There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a>. If you’d like to try building a cluster from scratch, check out the &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> tutorial by Kelsey Hightower.&lt;/p>
&lt;h1 id="release-team">Release Team&lt;/h1>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community.&lt;/p>
&lt;h1 id="release-logo">Release Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png" alt="Kubernetes 1.20 Release Logo">&lt;/p>
&lt;p>&lt;a href="https://www.dictionary.com/browse/rad">raddest&lt;/a>: &lt;em>adjective&lt;/em>, Slang. excellent; wonderful; cool:&lt;/p>
&lt;blockquote>
&lt;p>The Kubernetes 1.20 Release has been the raddest release yet.&lt;/p>
&lt;/blockquote>
&lt;p>2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14">Kubernetes 1.14 - Caturnetes&lt;/a> with a &amp;quot;rad&amp;quot; cat named Humphrey.&lt;/p>
&lt;p>Humphrey is the release lead's cat and has a permanent &lt;a href="https://www.inverse.com/article/42316-why-do-cats-blep-science-explains">&lt;code>blep&lt;/code>&lt;/a>. &lt;em>Rad&lt;/em> was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his &lt;em>blep&lt;/em> bring you a little joy at the end of 2020!&lt;/p>
&lt;p>The release logo was created by &lt;a href="https://www.instagram.com/robotdancebattle/">Henry Hsu - @robotdancebattle&lt;/a>.&lt;/p>
&lt;h1 id="user-highlights">User Highlights&lt;/h1>
&lt;ul>
&lt;li>Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch &lt;a href="https://youtu.be/Tx8qXC-U3KM">Alena Prokharchyk's KubeCon NA Keynote&lt;/a> to learn more about their cloud native journey.&lt;/li>
&lt;/ul>
&lt;h1 id="project-velocity">Project Velocity&lt;/h1>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/">CNCF K8s DevStats project&lt;/a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions">967 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1335 individuals&lt;/a> (&lt;a href="https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-repogroup_name=Kubernetes">44 of whom&lt;/a> made their first Kubernetes contribution) from &lt;a href="https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-period_name=Quarter&amp;amp;var-countries=All&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-metric=rcommitters&amp;amp;var-cum=countries">26 countries&lt;/a>.&lt;/p>
&lt;h1 id="ecosystem-updates">Ecosystem Updates&lt;/h1>
&lt;ul>
&lt;li>KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut">now available to all on-demand&lt;/a> for anyone still needing to catch up!&lt;/li>
&lt;li>In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming's goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given &lt;a href="https://sched.co/eukp">at KubeCon 2020 North America&lt;/a>, and the initial impact of this labor &lt;a href="https://github.com/kubernetes/enhancements/issues/2067">can actually be seen in the v1.20 release&lt;/a>.&lt;/li>
&lt;li>Previously announced this summer, &lt;a href="https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/">The Certified Kubernetes Security Specialist (CKS) Certification&lt;/a> was released during Kubecon NA for immediate scheduling! Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains. This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?).&lt;/li>
&lt;/ul>
&lt;h1 id="event-updates">Event Updates&lt;/h1>
&lt;p>KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">here&lt;/a>. Remember that &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/">the CFP&lt;/a> closes on Sunday, December 13, 11:59pm PST!&lt;/p>
&lt;h1 id="upcoming-release-webinar">Upcoming release webinar&lt;/h1>
&lt;p>Stay tuned for the upcoming release webinar happening this January.&lt;/p>
&lt;h1 id="get-involved">Get Involved&lt;/h1>
&lt;p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the new &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: GSoD 2020: Improving the API Reference Experience</title><link>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</link><pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://github.com/feloy">Philippe Martin&lt;/a>&lt;/p>
&lt;p>&lt;em>Editor's note: Better API references have been my goal since I joined Kubernetes docs three and a half years ago. Philippe has succeeded fantastically. More than a better API reference, though, Philippe embodied the best of the Kubernetes community in this project: excellence through collaboration, and a process that made the community itself better. Thanks, Google Season of Docs, for making Philippe's work possible. —Zach Corleissen&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The &lt;a href="https://developers.google.com/season-of-docs">Google Season of Docs&lt;/a> project brings open source organizations and technical writers together to work closely on a specific documentation project.&lt;/p>
&lt;p>I was selected by the CNCF to work on Kubernetes documentation, specifically to make the API Reference documentation more accessible.&lt;/p>
&lt;p>I'm a software developer with a great interest in documentation systems. In the late 90's I started translating Linux-HOWTO documents into French. From one thing to another, I learned about documentation systems. Eventually, I wrote a Linux-HOWTO to help documentarians learn the language used at that time for writing documents, LinuxDoc/SGML.&lt;/p>
&lt;p>Shortly afterward, Linux documentation adopted the DocBook language. I helped some writers rewrite their documents in this format; for example, the Advanced Bash-Scripting Guide. I also worked on the GNU &lt;code>makeinfo&lt;/code> program to add DocBook output, making it possible to transform &lt;em>GNU Info&lt;/em> documentation into Docbook format.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The &lt;a href="https://kubernetes.io/docs/home/">Kubernetes website&lt;/a> is built with Hugo from documentation written in Markdown format in the &lt;a href="https://github.com/kubernetes/website">website repository&lt;/a>, using the &lt;a href="https://www.docsy.dev/about/">Docsy Hugo theme&lt;/a>.&lt;/p>
&lt;p>The existing API reference documentation is a large HTML file generated from the Kubernetes OpenAPI specification.&lt;/p>
&lt;p>On my side, I wanted for some time to make the API Reference more accessible, by:&lt;/p>
&lt;ul>
&lt;li>building individual and autonomous pages for each Kubernetes resource&lt;/li>
&lt;li>adapting the format to mobile reading&lt;/li>
&lt;li>reusing the website's assets and theme to build, integrate, and display the reference pages&lt;/li>
&lt;li>allowing the search engines to reference the content of the pages&lt;/li>
&lt;/ul>
&lt;p>Around one year ago, I started to work on the generator building the current unique HTML page, to add a DocBook output, so the API Reference could be generated first in DocBook format, and after that in PDF or other formats supported by DocBook processors. The first result has been some &lt;a href="https://github.com/feloy/kubernetes-resources-reference/releases">Ebook files for the API Reference&lt;/a> and an auto-edited paper book.&lt;/p>
&lt;p>I decided later to add another output to this generator, to generate Markdown files and create &lt;a href="https://web.archive.org/web/20201022201911/https://www.k8sref.io/docs/workloads/">a website with the API Reference&lt;/a>.&lt;/p>
&lt;p>When the CNCF proposed a project for the Google Season of Docs to work on the API Reference, I applied, and the match occurred.&lt;/p>
&lt;h2 id="the-project">The Project&lt;/h2>
&lt;h3 id="swagger-ui">swagger-ui&lt;/h3>
&lt;p>The first idea of the CNCF members that proposed this project was to test the &lt;a href="https://swagger.io/tools/swagger-ui/">&lt;code>swagger-ui&lt;/code> tool&lt;/a>, to try and document the Kubernetes API Reference with this standard tool.&lt;/p>
&lt;p>Because the Kubernetes API is much larger than many other APIs, it has been necessary to write a tool to split the complete API Reference by API Groups, and insert in the Documentation website several &lt;code>swagger-ui&lt;/code> components, one for each API Group.&lt;/p>
&lt;p>Generally, APIs are used by developers by calling endpoints with a specific HTTP verb, with specific parameters and waiting for a response. The &lt;code>swagger-ui&lt;/code> interface is built for this usage: the interface displays a list of endpoints and their associated verbs, and for each the parameters and responses formats.&lt;/p>
&lt;p>The Kubernetes API is most of the time used differently: users create manifest files containing resources definitions in YAML format, and use the &lt;code>kubectl&lt;/code> CLI to &lt;em>apply&lt;/em> these manifests to the cluster. In this case, the most important information is the description of the structures used as parameters and responses (the Kubernetes Resources).&lt;/p>
&lt;p>Because of this specificity, we realized that it would be difficult to adapt the &lt;code>swagger-ui&lt;/code> interface to satisfy the users of the Kubernetes API and this direction has been abandoned.&lt;/p>
&lt;h3 id="markdown-pages">Markdown pages&lt;/h3>
&lt;p>The second stage of the project has been to adapt the work I had done to create the k8sref.io website, to include it in the official documentation website.&lt;/p>
&lt;p>The main changes have been to:&lt;/p>
&lt;ul>
&lt;li>use go-templates to represent the output pages, so non-developers can adapt the generated pages without having to edit the generator code&lt;/li>
&lt;li>create a new custom &lt;a href="https://gohugo.io/content-management/shortcodes/">shortcode&lt;/a>, to easily create links from inside the website to specific pages of the API reference&lt;/li>
&lt;li>improve the navigation between the sections of the API reference&lt;/li>
&lt;li>add the code of the generator to the Kubernetes GitHub repository containing the different reference generators&lt;/li>
&lt;/ul>
&lt;p>All the discussions and work done can be found in website &lt;a href="https://github.com/kubernetes/website/pull/23294">pull request #23294&lt;/a>.&lt;/p>
&lt;p>Adding the generator code to the Kubernetes project happened in &lt;a href="https://github.com/kubernetes-sigs/reference-docs/pull/179">kubernetes-sigs/reference-docs#179&lt;/a>.&lt;/p>
&lt;p>Here are the features of the new API Reference to be included in the official documentation website:&lt;/p>
&lt;ul>
&lt;li>the resources are categorized, in the categories Workloads, Services, Config &amp;amp; Storage, Authentication, Authorization, Policies, Extend, Cluster. This structure is configurable with a simple &lt;a href="https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/toc.yaml">&lt;code>toc.yaml&lt;/code> file&lt;/a>&lt;/li>
&lt;li>each page displays associated resources at the first level ; for example: Pod, PodSpec, PodStatus, PodList&lt;/li>
&lt;li>most resource pages inline relevant definitions ; the exceptions are when those definitions are common to several resources, or are too complex to be displayed inline. With the old approach, you had to follow a hyperlink to read each extra detail.&lt;/li>
&lt;li>some widely used definitions, such as &lt;code>ObjectMeta&lt;/code>, are documented in a specific page&lt;/li>
&lt;li>required fields are indicated, and placed first&lt;/li>
&lt;li>fields of a resource can be categorized and ordered, with the help of a &lt;a href="https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/fields.yaml">&lt;code>fields.yaml&lt;/code> file&lt;/a>&lt;/li>
&lt;li>&lt;code>map&lt;/code> fields are indicated. For example the &lt;code>.spec.nodeSelector&lt;/code> for a &lt;code>Pod&lt;/code> is &lt;code>map[string]string&lt;/code>, instead of &lt;code>object&lt;/code>, using the value of &lt;code>x-kubernetes-list-type&lt;/code>&lt;/li>
&lt;li>patch strategies are indicated&lt;/li>
&lt;li>&lt;code>apiVersion&lt;/code> and &lt;code>kind&lt;/code> display the value, not the &lt;code>string&lt;/code> type&lt;/li>
&lt;li>At the top of a reference page, the page displays the Go import necessary to use these resources from a Go program.&lt;/li>
&lt;/ul>
&lt;p>The work is currently on hold pending the 1.20 release. When the release finishes and the work is integrated, the API reference will be available at &lt;a href="https://kubernetes.io/docs/reference/">https://kubernetes.io/docs/reference/&lt;/a>.&lt;/p>
&lt;h3 id="future-work">Future Work&lt;/h3>
&lt;p>There are points to improve, particularly:&lt;/p>
&lt;ul>
&lt;li>Some Kubernetes resources are deeply nested. Inlining the definition of these resources makes them difficult to understand.&lt;/li>
&lt;li>The created &lt;code>shortcode&lt;/code> uses the URL of the page to reference a Resource page. It would be easier for documentarians if they could reference a Resource by its group and name.&lt;/li>
&lt;/ul>
&lt;h2 id="appreciation">Appreciation&lt;/h2>
&lt;p>I would like to thank my mentor &lt;a href="https://github.com/zacharysarah">Zach Corleissen&lt;/a> and the lead writers &lt;a href="https://github.com/kbhawkey">Karen Bradshaw&lt;/a>, &lt;a href="https://github.com/celestehorgan">Celeste Horgan&lt;/a>, &lt;a href="https://github.com/sftim">Tim Bannister&lt;/a> and &lt;a href="https://github.com/tengqm">Qiming Teng&lt;/a> who supervised me during all the season. They all have been very encouraging and gave me tons of great advice.&lt;/p></description></item><item><title>Blog: Dockershim Deprecation FAQ</title><link>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</guid><description>
&lt;p>This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker&lt;/a>.&lt;/p>
&lt;h3 id="why-is-dockershim-being-deprecated">Why is dockershim being deprecated?&lt;/h3>
&lt;p>Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn't currently implement CRI,
thus the problem.&lt;/p>
&lt;p>Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1985-remove-dockershim">Dockershim Removal Kubernetes Enhancement Proposal&lt;/a>.&lt;/p>
&lt;p>Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.&lt;/p>
&lt;h3 id="can-i-still-use-docker-in-kubernetes-1-20">Can I still use Docker in Kubernetes 1.20?&lt;/h3>
&lt;p>Yes, the only thing changing in 1.20 is a single warning log printed at &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
startup if using Docker as the runtime.&lt;/p>
&lt;h3 id="when-will-dockershim-be-removed">When will dockershim be removed?&lt;/h3>
&lt;p>Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021. We will be working closely with vendors
and other ecosystem groups to ensure a smooth transition and will evaluate things
as the situation evolves.&lt;/p>
&lt;h3 id="can-i-still-use-dockershim-after-it-is-removed-from-kubernetes">Can I still use dockershim after it is removed from Kubernetes?&lt;/h3>
&lt;p>Update:
Mirantis and Docker have &lt;a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/">committed&lt;/a> to maintaining the dockershim after
it is removed from Kubernetes.&lt;/p>
&lt;h3 id="will-my-existing-docker-images-still-work">Will my existing Docker images still work?&lt;/h3>
&lt;p>Yes, the images produced from &lt;code>docker build&lt;/code> will work with all CRI implementations.
All your existing images will still work exactly the same.&lt;/p>
&lt;h3 id="what-about-private-images">What about private images?&lt;/h3>
&lt;p>Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.&lt;/p>
&lt;h3 id="are-docker-and-containers-the-same-thing">Are Docker and containers the same thing?&lt;/h3>
&lt;p>Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.&lt;/p>
&lt;h3 id="are-there-examples-of-folks-using-other-runtimes-in-production-today">Are there examples of folks using other runtimes in production today?&lt;/h3>
&lt;p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.&lt;/p>
&lt;p>Additionally, the &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the &lt;a href="https://cri-o.io/">CRI-O&lt;/a> runtime in production since June 2019.&lt;/p>
&lt;p>For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation (&lt;a href="https://cncf.io">CNCF&lt;/a>).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="people-keep-referencing-oci-what-is-that">People keep referencing OCI, what is that?&lt;/h3>
&lt;p>OCI stands for the &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a>, which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>, which is the underlying default runtime for both
&lt;a href="https://containerd.io/">containerd&lt;/a> and &lt;a href="https://cri-o.io/">CRI-O&lt;/a>. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.&lt;/p>
&lt;h3 id="which-cri-implementation-should-i-use">Which CRI implementation should I use?&lt;/h3>
&lt;p>That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape&lt;/a> in case another would be an
even better fit for your environment.&lt;/p>
&lt;h3 id="what-should-i-look-out-for-when-changing-cri-implementations">What should I look out for when changing CRI implementations?&lt;/h3>
&lt;p>While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:&lt;/p>
&lt;ul>
&lt;li>Logging configuration&lt;/li>
&lt;li>Runtime resource limitations&lt;/li>
&lt;li>Node provisioning scripts that call docker or use docker via it's control socket&lt;/li>
&lt;li>Kubectl plugins that require docker CLI or the control socket&lt;/li>
&lt;li>Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)&lt;/li>
&lt;li>Configuration of functionality like &lt;code>registry-mirrors&lt;/code> and insecure registries&lt;/li>
&lt;li>Other support scripts or daemons that expect Docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)&lt;/li>
&lt;li>GPUs or special hardware and how they integrate with your runtime and Kubernetes&lt;/li>
&lt;/ul>
&lt;p>If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your dockerd configuration, you’ll need to adapt that for your new container
runtime where possible.&lt;/p>
&lt;p>Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> tool as a drop-in replacement (see &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl">mapping from docker cli to crictl&lt;/a>) and for the
latter you can use newer container build options like &lt;a href="https://github.com/genuinetools/img">img&lt;/a>, &lt;a href="https://github.com/containers/buildah">buildah&lt;/a>,
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>, or &lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a> that don’t require Docker.&lt;/p>
&lt;p>For containerd, you can start with their &lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">documentation&lt;/a> to see what configuration
options are available as you migrate things over.&lt;/p>
&lt;p>For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes">Container Runtimes&lt;/a>&lt;/p>
&lt;h3 id="what-if-i-have-more-questions">What if I have more questions?&lt;/h3>
&lt;p>If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: &lt;a href="https://discuss.kubernetes.io/">https://discuss.kubernetes.io/&lt;/a>.&lt;/p>
&lt;p>You can also check out the excellent blog post
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">Wait, Docker is deprecated in Kubernetes now?&lt;/a> a more in-depth technical
discussion of the changes.&lt;/p>
&lt;h3 id="can-i-have-a-hug">Can I have a hug?&lt;/h3>
&lt;p>Always and whenever you want! 🤗🤗&lt;/p></description></item><item><title>Blog: Don't Panic: Kubernetes and Docker</title><link>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas&lt;/p>
&lt;p>Kubernetes is &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">deprecating
Docker&lt;/a>
as a container runtime after v1.20.&lt;/p>
&lt;p>&lt;strong>You do not need to panic. It’s not as dramatic as it sounds.&lt;/strong>&lt;/p>
&lt;p>TL;DR Docker as an underlying runtime is being deprecated in favor of runtimes
that use the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface (CRI)&lt;/a>
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.&lt;/p>
&lt;p>If you’re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesn’t mean the death of Docker, and it doesn’t mean you can’t, or
shouldn’t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running &lt;code>docker build&lt;/code> can still run in your Kubernetes cluster.&lt;/p>
&lt;p>If you’re using a managed Kubernetes service like GKE, EKS, or AKS (which &lt;a href="https://github.com/Azure/AKS/releases/tag/2020-11-16">defaults to containerd&lt;/a>) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.&lt;/p>
&lt;p>If you’re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).&lt;/p>
&lt;h2 id="so-why-the-confusion-and-what-is-everyone-freaking-out-about">So why the confusion and what is everyone freaking out about?&lt;/h2>
&lt;p>We’re talking about two different environments here, and that’s creating
confusion. Inside of your Kubernetes cluster, there’s a thing called a container
runtime that’s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.&lt;/p>
&lt;p>You see, the thing we call “Docker” isn’t actually one thing—it’s an entire
tech stack, and one part of it is a thing called “containerd,” which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while we’re doing development work, but those UX enhancements aren’t necessary
for Kubernetes, because it isn’t a human.&lt;/p>
&lt;p>As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. That’s not great, because it gives us another thing that has to
be maintained and can possibly break. What’s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?&lt;/p>
&lt;p>Docker isn’t compliant with CRI, the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface&lt;/a>.
If it were, we wouldn’t need the shim, and this wouldn’t be a thing. But it’s
not the end of the world, and you don’t need to panic—you just need to change
your container runtime from Docker to another supported container runtime.&lt;/p>
&lt;p>One thing to note: If you are relying on the underlying docker socket
(&lt;code>/var/run/docker.sock&lt;/code>) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>,
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>, and
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>.&lt;/p>
&lt;h2 id="what-does-this-change-mean-for-developers-though-do-we-still-write-dockerfiles-do-we-still-build-things-with-docker">What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?&lt;/h2>
&lt;p>This change addresses a different environment than most folks use to interact
with Docker. The Docker installation you’re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. It’s confusing, we understand.
As a developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isn’t really a
Docker-specific image—it’s an OCI (&lt;a href="https://opencontainers.org/">Open Container Initiative&lt;/a>) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both &lt;a href="https://containerd.io/">containerd&lt;/a> and
&lt;a href="https://cri-o.io/">CRI-O&lt;/a> know how to pull those images and run them. This is
why we have a standard for what containers should look like.&lt;/p>
&lt;p>So, this change is coming. It’s going to cause issues for some, but it isn’t
catastrophic, and generally it’s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, it’s going to make things easier. If this is still confusing
for you, that’s okay—there’s a lot going on here; Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. We hope
this has answered most of your questions and soothed some anxieties! ❤️&lt;/p>
&lt;p>Looking for more answers? Check out our accompanying &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ&lt;/a>.&lt;/p></description></item><item><title>Blog: Cloud native security for your clusters</title><link>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://twitter.com/pudijoglekar">Pushkar Joglekar&lt;/a>&lt;/p>
&lt;p>Over the last few years a small, security focused community has been working diligently to deepen our understanding of security, given the evolving cloud native infrastructure and corresponding iterative deployment practices. To enable sharing of this knowledge with the rest of the community, members of &lt;a href="https://github.com/cncf/sig-security">CNCF SIG Security&lt;/a> (a group which reports into &lt;a href="https://github.com/cncf/toc#sigs">CNCF TOC&lt;/a> and who are friends with &lt;a href="https://github.com/kubernetes/community/tree/master/sig-security">Kubernetes SIG Security&lt;/a>) led by Emily Fox, collaborated on a whitepaper outlining holistic cloud native security concerns and best practices. After over 1200 comments, changes, and discussions from 35 members across the world, we are proud to share &lt;a href="https://www.cncf.io/blog/2020/11/18/announcing-the-cloud-native-security-white-paper">cloud native security whitepaper v1.0&lt;/a> that serves as essential reading for security leadership in enterprises, financial and healthcare industries, academia, government, and non-profit organizations.&lt;/p>
&lt;p>The paper attempts to &lt;em>not&lt;/em> focus on any specific &lt;a href="https://www.cncf.io/projects/">cloud native project&lt;/a>. Instead, the intent is to model and inject security into four logical phases of cloud native application lifecycle: &lt;em>Develop, Distribute, Deploy, and Runtime&lt;/em>.&lt;/p>
&lt;p>&lt;img alt="Cloud native application lifecycle phases"
src="cloud-native-app-lifecycle-phases.svg"
style="width:60em;max-width:100%;">&lt;/p>
&lt;h2 id="kubernetes-native-security-controls">Kubernetes native security controls&lt;/h2>
&lt;p>When using Kubernetes as a workload orchestrator, some of the security controls this version of the whitepaper recommends are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies&lt;/a>: Implement a single source of truth for “least privilege” workloads across the entire cluster&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Resource requests and limits&lt;/a>: Apply requests (soft constraint) and limits (hard constraint) for shared resources such as memory and CPU&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Audit log analysis&lt;/a>: Enable Kubernetes API auditing and filtering for security relevant events&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/">Control plane authentication and certificate root of trust&lt;/a>: Enable mutual TLS authentication with a trusted CA for communication within the cluster&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets management&lt;/a>: Integrate with a built-in or external secrets store&lt;/li>
&lt;/ul>
&lt;h2 id="cloud-native-complementary-security-controls">Cloud native complementary security controls&lt;/h2>
&lt;p>Kubernetes has direct involvement in the &lt;em>deploy&lt;/em> phase and to a lesser extent in the &lt;em>runtime&lt;/em> phase. Ensuring the artifacts are securely &lt;em>developed&lt;/em> and &lt;em>distributed&lt;/em> is necessary for, enabling workloads in Kubernetes to run “secure by default”. Throughout all phases of the Cloud native application life cycle, several complementary security controls exist for Kubernetes orchestrated workloads, which includes but are not limited to:&lt;/p>
&lt;ul>
&lt;li>Develop:
&lt;ul>
&lt;li>Image signing and verification&lt;/li>
&lt;li>Image vulnerability scanners&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Distribute:
&lt;ul>
&lt;li>Pre-deployment checks for detecting excessive privileges&lt;/li>
&lt;li>Enabling observability and logging&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Deploy:
&lt;ul>
&lt;li>Using a service mesh for workload authentication and authorization&lt;/li>
&lt;li>Enforcing “default deny” network policies for inter-workload communication via &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugins&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Runtime:
&lt;ul>
&lt;li>Deploying security monitoring agents for workloads&lt;/li>
&lt;li>Isolating applications that run on the same node using SELinux, AppArmor, etc.&lt;/li>
&lt;li>Scanning configuration against recognized secure baselines for node, workload and orchestrator&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="understand-first-secure-next">Understand first, secure next&lt;/h2>
&lt;p>The cloud native way, including containers, provides great security benefits for its users: immutability, modularity, faster upgrades and consistent state across the environment. Realizing this fundamental change in “the way things are done”, motivates us to look at security with a cloud native lens. One of the things that was evident for all the authors of the paper was the fact that it’s tough to make smarter decisions on how and what to secure in a cloud native ecosystem if you do not understand the tools, patterns, and frameworks at hand (in addition to knowing your own critical assets). Hence, for all the security practitioners out there who want to be partners rather than a gatekeeper for your friends in Operations, Product Development, and Compliance, let’s make an attempt to &lt;em>learn more so we can secure better&lt;/em>.&lt;/p>
&lt;p>We recommend following this &lt;strong>7 step R.U.N.T.I.M.E. path&lt;/strong> to get started on cloud native security:&lt;/p>
&lt;ol>
&lt;li>&lt;b>R&lt;/b>ead the paper and any linked material in it&lt;/li>
&lt;li>&lt;b>U&lt;/b>nderstand challenges and constraints for your environment&lt;/li>
&lt;li>&lt;b>N&lt;/b>ote the content and controls that apply to your environment&lt;/li>
&lt;li>&lt;b>T&lt;/b>alk about your observations with your peers&lt;/li>
&lt;li>&lt;b>I&lt;/b>nvolve your leadership and ask for help&lt;/li>
&lt;li>&lt;b>M&lt;/b>ake a risk profile based on existing and missing security controls&lt;/li>
&lt;li>&lt;b>E&lt;/b>xpend time, money, and resources that improve security posture and reduce risk where appropriate.&lt;/li>
&lt;/ol>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Huge shout out to &lt;em>Emily Fox, Tim Bannister (The Scale Factory), Chase Pettet (Mirantis), and Wayne Haber (GitLab)&lt;/em> for contributing with their wonderful suggestions for this blog post.&lt;/p></description></item><item><title>Blog: Remembering Dan Kohn</title><link>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: The Kubernetes Steering Committee&lt;/p>
&lt;p>Dan Kohn was instrumental in getting Kubernetes and CNCF community to where it is today. He shared our values, motivations, enthusiasm, community spirit, and helped the Kubernetes community to become the best that it could be. Dan loved getting people together to solve problems big and small. He enabled people to grow their individual scope in the community which often helped launch their career in open source software.&lt;/p>
&lt;p>Dan built a coalition around the nascent Kubernetes project and turned that into a cornerstone to build the larger cloud native space. He loved challenges, especially ones where the payoff was great like building worldwide communities, spreading the love of open source, and helping diverse, underprivileged communities and students to get a head start in technology.&lt;/p>
&lt;p>Our heart goes out to his family. Thank you, Dan, for bringing your boys to events in India and elsewhere as we got to know how great you were as a father. Dan, your thoughts and ideas will help us make progress in our journey as a community. Thank you for your life's work!&lt;/p>
&lt;p>If Dan has made an impact on you in some way, please consider adding a memory of him in his &lt;a href="https://github.com/cncf/memorials/blob/master/dan-kohn.md">CNCF memorial&lt;/a>.&lt;/p></description></item><item><title>Blog: Announcing the 2020 Steering Committee Election Results</title><link>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Kaslin Fields&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2020">2020 Steering Committee Election&lt;/a> is now complete. In 2019, the committee arrived at its final allocation of 7 seats, 3 of which were up for election in 2020. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.&lt;/p>
&lt;p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their &lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">charter&lt;/a>.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>), VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt (&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen (&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>They join continuing members Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat; Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat; Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), VMware; and Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>), Apple. Davanum Srinivas is returning for his second term on the committee.&lt;/p>
&lt;h2 id="big-thanks">Big Thanks!&lt;/h2>
&lt;ul>
&lt;li>Thank you and congratulations on a successful election to this round’s election officers:
&lt;ul>
&lt;li>Jaice Singer DuMars (&lt;a href="https://github.com/jdumars">@jdumars&lt;/a>), Apple&lt;/li>
&lt;li>Ihor Dvoretskyi (&lt;a href="https://github.com/idvoretskyi">@idvoretskyi&lt;/a>), CNCF&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:
&lt;ul>
&lt;li>Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>), Google&lt;/li>
&lt;li>and Lachlan Evenson(&lt;a href="https://github.com/lachie8e">@lachie8e)&lt;/a>), Microsoft&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>And thank you to all the candidates who came forward to run for election. As &lt;a href="https://twitter.com/castrojo/status/1315718627639820288?s=20">Jorge Castro put it&lt;/a>: we are spoiled with capable, kind, and selfless volunteers who put the needs of the project first.&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved-with-the-steering-committee">Get Involved with the Steering Committee&lt;/h2>
&lt;p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee &lt;a href="https://github.com/kubernetes/steering/projects/1">backlog items&lt;/a> and weigh in by filing an issue or creating a PR against their &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>. They have an open meeting on &lt;a href="https://github.com/kubernetes/steering">the first Monday of the month at 6pm UTC&lt;/a> and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a>.&lt;/p>
&lt;p>You can see what the Steering Committee meetings are all about by watching past meetings on the &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>This post was written by the &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">Upstream Marketing Working Group&lt;/a>. If you want to write stories about the Kubernetes community, learn more about us.&lt;/em>&lt;/p></description></item><item><title>Blog: Contributing to the Development Guide</title><link>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</guid><description>
&lt;p>When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that's certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of &amp;quot;client.&amp;quot; I just didn't know what to expect when Google asked my compatriots and me at
&lt;a href="https://lionswaycontent.com/">Lion's Way&lt;/a> to make much-needed updates to the Kubernetes Development Guide.&lt;/p>
&lt;p>&lt;em>This article originally appeared on the &lt;a href="https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/">Kubernetes Contributor Community blog&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="the-delights-of-working-with-a-community">The Delights of Working With a Community&lt;/h2>
&lt;p>As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn't be present when working on open source documentation, but I couldn't
predict how it would change my relationship with the project.&lt;/p>
&lt;p>One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they're looking for. It can be stressful -- which is why I'm so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor &lt;a href="https://twitter.com/JoelByronBarker">Joel&lt;/a>
handles most of the client contact.&lt;/p>
&lt;p>I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.&lt;/p>
&lt;p>&amp;quot;How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?&amp;quot; These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the &lt;code>#sig-contribex&lt;/code> channel on the Kubernetes
Slack and announced that I would be working on the
&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/development.md">Development Guide&lt;/a>.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"The Kubernetes Code of Conduct is in effect, so please be excellent to each other." &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>My fears were unfounded. Immediately, I felt welcome. I like to think this isn't just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the &lt;a href="https://www.kubernetes.dev/resources/code-of-conduct/">Kubernetes Code of Conduct&lt;/a> was in
effect, and that we should, like Bill and Ted, be excellent to each other.&lt;/p>
&lt;h2 id="this-doesn-t-mean-it-s-all-easy">This Doesn't Mean It's All Easy&lt;/h2>
&lt;p>The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
&lt;a href="https://github.com/kubernetes/community">Community repo&lt;/a>: 267 additions and 88 deletions.&lt;/p>
&lt;p>The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, &lt;a href="https://github.com/kubernetes/community/pull/5003">it was successful&lt;/a>.&lt;/p>
&lt;p>Since Kubernetes is a pretty fast-moving project, and since developers typically aren't really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the &lt;a href="https://github.com/amwat">labyrinthine mind of
a brilliant engineer&lt;/a>, and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.&lt;/p>
&lt;p>This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
&lt;a href="https://github.com/kubernetes-sigs/kubetest2">&lt;code>kubetest2&lt;/code> framework&lt;/a> to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
&lt;a href="https://github.com/kubernetes/community/pull/5045">completed pull request&lt;/a>.&lt;/p>
&lt;h2 id="nobody-is-the-boss-and-everybody-gives-feedback">Nobody Is the Boss, and Everybody Gives Feedback&lt;/h2>
&lt;p>But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was &lt;em>enjoyable&lt;/em>.&lt;/p>
&lt;p>With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.&lt;/p>
&lt;p>In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.&lt;/p>
&lt;p>Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I've made in this vast open source community over the past
few months.&lt;/p></description></item><item><title>Blog: GSoC 2020 - Building operators for cluster addons</title><link>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Somtochi Onyekwere&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>&lt;a href="https://summerofcode.withgoogle.com/">Google Summer of Code&lt;/a> is a global program that is geared towards introducing students to open source. Students are matched with open-source organizations to work with them for three months during the summer.&lt;/p>
&lt;p>My name is Somtochi Onyekwere from the Federal University of Technology, Owerri (Nigeria) and this year, I was given the opportunity to work with Kubernetes (under the CNCF organization) and this led to an amazing summer spent learning, contributing and interacting with the community.&lt;/p>
&lt;p>Specifically, I worked on the &lt;em>Cluster Addons: Package all the things!&lt;/em> project. The project focused on building operators for better management of various cluster addons, extending the tooling for building these operators and making the creation of these operators a smooth process.&lt;/p>
&lt;h1 id="background">Background&lt;/h1>
&lt;p>Kubernetes has progressed greatly in the past few years with a flourishing community and a large number of contributors. The codebase is gradually moving away from the monolith structure where all the code resides in the &lt;a href="https://github.com/kubernetes/kubernetes">kubernetes/kubernetes&lt;/a> repository to being split into multiple sub-projects. Part of the focus of cluster-addons is to make some of these sub-projects work together in an easy to assemble, self-monitoring, self-healing and Kubernetes-native way. It enables them to work seamlessly without human intervention.&lt;/p>
&lt;p>The community is exploring the use of operators as a mechanism to monitor various resources in the cluster and properly manage these resources. In addition to this, it provides self-healing and it is a kubernetes-native pattern that can encode how best these addons work and manage them properly.&lt;/p>
&lt;p>What are cluster addons? Cluster addons are a collection of resources (like Services and deployment) that are used to give a Kubernetes cluster additional functionalities. They range from things as simple as the Kubernetes dashboards (for visualization) to more complex ones like Calico (for networking). These addons are essential to different applications running in the cluster and the cluster itself. The addon operator provides a nicer way of managing these addons and understanding the health and status of the various resources that comprise the addon. You can get a deeper overview in this &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#addons">article&lt;/a>.&lt;/p>
&lt;p>Operators are custom controllers with custom resource definitions that encode application-specific knowledge and are used for managing complex stateful applications. It is a widely accepted pattern. Managing addons via operators, with these operators encoding knowledge of how best the addons work, introduces a lot of advantages while setting standards that will be easy to follow and scale. This &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator">article&lt;/a> does a good job of explaining operators.&lt;/p>
&lt;p>The addon operators can solve a lot of problems, but they have their challenges. Those under the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons">cluster-addons project&lt;/a> had missing pieces and were still a proof of concept. Generating the RBAC configuration for the operators was a pain and sometimes the operators were given too much privilege. The operators weren’t very extensible as it only pulled manifests from local filesystems or HTTP(s) servers and a lot of simple addons were generating the same code.
I spent the summer working on these issues, looking at them with fresh eyes and coming up with solutions for both the known and unknown issues.&lt;/p>
&lt;h1 id="various-additions-to-kubebuilder-declarative-pattern">Various additions to kubebuilder-declarative-pattern&lt;/h1>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern">kubebuilder-declarative-pattern&lt;/a> (from here on referred to as KDP) repo is an extra layer of addon specific tooling on top of the &lt;a href="https://github.com/kubernetes-sigs/kubebuilder">kubebuilder&lt;/a> SDK that is enabled by passing the experimental &lt;code>--pattern=addon&lt;/code> flag to &lt;code>kubebuilder create&lt;/code> command. Together, they create the base code for the addon operator. During the internship, I worked on a couple of features in KDP and cluster-addons.&lt;/p>
&lt;h2 id="operator-version-checking">Operator version checking&lt;/h2>
&lt;p>Enabling version checks for operators helped in making upgrades/downgrades safer to different versions of the addon, even though the operator had complex logic. It is a way of matching the version of an addon to the version of the operator that knows how to manage it well. Most addons have different versions and these versions might need to be managed differently. This feature checks the custom resource for the &lt;code>addons.k8s.io/min-operator-version&lt;/code> annotation which states the minimum operator version that is needed to manage the version against the version of the operator. If the operator version is below the minimum version required, the operator pauses with an error telling the user that the version of the operator is too low. This helps to ensure that the correct operator is being used for the addon.&lt;/p>
&lt;h2 id="git-repository-for-storing-the-manifests">Git repository for storing the manifests&lt;/h2>
&lt;p>Previously, there was support for only local file directories and HTTPS repositories for storing manifests. Giving creators of addon operators the ability to store manifest in GitHub repository enables faster development and version control. When starting the controller, you can pass a flag to specify the location of your channels directory. The channels directory contains the manifests for different versions, the controller pulls the manifest from this directory and applies it to the cluster. During the internship period, I extended it to include Git repositories.&lt;/p>
&lt;h2 id="annotations-to-temporarily-disable-reconciliation">Annotations to temporarily disable reconciliation&lt;/h2>
&lt;p>The reconciliation loop that ensures that the desired state matches the actual state prevents modification of objects in the cluster. This makes it hard to experiment or investigate what might be wrong in the cluster as any changes made are promptly reverted. I resolved this by allowing users to place an &lt;code>addons.k8s.io/ignore&lt;/code> annotation on the resource that they don’t want the controller to reconcile. The controller checks for this annotation and doesn’t reconcile that object. To resume reconciliation, the annotation can be removed from the resource.&lt;/p>
&lt;h2 id="unstructured-support-in-kubebuilder-declarative-pattern">Unstructured support in kubebuilder-declarative-pattern&lt;/h2>
&lt;p>One of the operators that I worked on is a generic controller that could manage more than one cluster addon that did not require extra configuration. To do this, the operator couldn’t use a particular type and needed the kubebuilder-declarative-repo to support using the &lt;a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured#Unstructured">unstructured.Unstructured&lt;/a> type. There were various functions in the kubebuilder-declarative-pattern that couldn’t handle this type and returned an error if the object passed in was not of type &lt;code>addonsv1alpha1.CommonObject&lt;/code>. The functions were modified to handle both &lt;code>unstructured.Unstructured&lt;/code> and &lt;code>addonsv1alpha.CommonObject&lt;/code>.&lt;/p>
&lt;h1 id="tools-and-cli-programs">Tools and CLI programs&lt;/h1>
&lt;p>There were also some command-line programs I wrote that could be used to make working with addon operators easier. Most of them have uses outside the addon operators as they try to solve a specific problem that could surface anywhere while working with Kubernetes. I encourage you to &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools">check them out&lt;/a> when you have the chance!&lt;/p>
&lt;h2 id="rbac-generator">RBAC Generator&lt;/h2>
&lt;p>One of the biggest concerns with the operator was RBAC. You had to manually look through the manifest and add the RBAC rule for each resource as it needs to have RBAC permissions to create, get, update and delete the resources in the manifest when running in-cluster. Building the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen">RBAC generator&lt;/a> automated the process of writing the RBAC roles and role bindings. The function of the RBAC generator is simple. It accepts the file name of the manifest as a flag. Then, it parses the manifest and gets the API group and resource name of the resources and adds it to a role. It outputs the role and role binding to stdout or a file if the &lt;code>--out&lt;/code> flag is parsed.&lt;/p>
&lt;p>Additionally, the tool enables you to split the RBAC by separating the cluster roles in the manifest. This lessened the security concern of an operator being over-privileged as it needed to have all the permissions that the clusterrole has. If you want to apply the clusterrole yourself and not give the operator these permissions, you can pass in a &lt;code>--supervisory&lt;/code> boolean flag so that the generator does not add these permissions to the role. The CLI program resides &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen">here&lt;/a>.&lt;/p>
&lt;h2 id="kubectl-ownerref">Kubectl Ownerref&lt;/h2>
&lt;p>It is hard to find out at a glance which objects were created by an addon custom resource. This kubectl plugin alleviates that pain by displaying all the objects in the cluster that a resource has ownerrefs on. You simply pass the kind and the name of the resource as arguments to the program and it checks the cluster for the objects and gives the kind, name, the namespace of such an object. It could be useful to get a general overview of all the objects that the controller is reconciling by passing in the name and kind of custom resource. The CLI program resides &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools/kubectl-ownerref">here&lt;/a>.&lt;/p>
&lt;h1 id="addon-operators">Addon Operators&lt;/h1>
&lt;p>To fully understand addons operators and make changes to how they are being created, you have to try creating and using them. Part of the summer was spent building operators for some popular addons like the Kubernetes dashboard, flannel, NodeLocalDNS and so on. Please check the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons">cluster-addons&lt;/a> repository for the different addon operators. In this section, I will just highlight one that is a little different from the others.&lt;/p>
&lt;h2 id="generic-controller">Generic Controller&lt;/h2>
&lt;p>The generic controller can be shared between addons that don’t require much configuration. This minimizes resource consumption on the cluster as it reduces the number of controllers that need to be run. Also instead of building your own operator, you can just use the generic controller and whenever you feel that your needs have grown and you need a more complex operator, you can always scaffold the code with kubebuilder and continue from where the generic operator stopped. To use the generic controller, you can generate the CustomResourceDefinition(CRD) using this tool (&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md">generic-addon&lt;/a>). You pass in the kind, group, and the location of your channels directory (it could be a Git repository too!). The tool generates the - CRD, RBAC manifest and two custom resources for you.&lt;/p>
&lt;p>The process is as follows:&lt;/p>
&lt;ul>
&lt;li>Create the Generic CRD&lt;/li>
&lt;li>Generate all the manifests needed with the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md">&lt;code>generic-addon tool&lt;/code>&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>This tool creates:&lt;/p>
&lt;ol>
&lt;li>The CRD for your addon&lt;/li>
&lt;li>The RBAC rules for the CustomResourceDefinitions&lt;/li>
&lt;li>The RBAC rules for applying the manifests&lt;/li>
&lt;li>The custom resource for your addon&lt;/li>
&lt;li>A Generic custom resource&lt;/li>
&lt;/ol>
&lt;p>The Generic custom resource looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>addons.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Generic&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>generic-sample&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">objectKind&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NodeLocalDNS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1alpha1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>addons.x-k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">channel&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;../nodelocaldns/channels&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Apply these manifests but ensure to apply the CRD before the CR.
Then, run the Generic controller, either on your machine or in-cluster.&lt;/p>
&lt;p>If you are interested in building an operator, Please check out &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md">this guide&lt;/a>.&lt;/p>
&lt;h1 id="relevant-links">Relevant Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://github.com/SomtochiAma/gsoc-2020-meta-k8s">Detailed breakdown of work done during the internship&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/addons/0035-20190128-addons-via-operators.md">Addon Operator (KEP)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/issues/39">Original GSoC Issue&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/SomtochiAma/gsoc-2020-meta-k8s/blob/master/GSoC%202020%20PROPOSAL%20-%20PACKAGE%20ALL%20THINGS.pdf">Proposal Submitted for GSoC&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/commits?author=SomtochiAma">All commits to kubernetes-sigs/cluster-addons&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern/commits?author=SomtochiAma">All commits to kubernetes-sigs/kubebuidler-declarative-pattern&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="further-work">Further Work&lt;/h1>
&lt;p>A lot of work was definitely done on the cluster addons during the GSoC period. But we need more people building operators and using them in the cluster. We need wider adoption in the community. Build operators for your favourite addons and tell us how it went and if you had any issues. Check out this &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md">README.md&lt;/a> to get started.&lt;/p>
&lt;h1 id="appreciation">Appreciation&lt;/h1>
&lt;p>I really want to appreciate my mentors &lt;a href="https://github.com/justinsb">Justin Santa Barbara&lt;/a> (Google) and &lt;a href="https://github.com/stealthybox">Leigh Capili&lt;/a> (Weaveworks). My internship was awesome because they were awesome. They set a golden standard for what mentorship should be. They were accessible and always available to clear any confusion. I think what I liked best was that they didn’t just dish out tasks, instead, we had open discussions about what was wrong and what could be improved. They are really the best and I hope I get to work with them again!
Also, I want to say a huge thanks to &lt;a href="https://github.com/neolit123">Lubomir I. Ivanov&lt;/a> for reviewing this blog post!&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>So far I have learnt a lot about Go, the internals of Kubernetes, and operators. I want to conclude by encouraging people to contribute to open-source (especially Kubernetes :)) regardless of your level of experience. It has been a well-rounded experience for me and I have come to love the community. It is a great initiative and it is a great way to learn and meet awesome people. Special shoutout to Google for organizing this program.&lt;/p>
&lt;p>If you are interested in cluster addons and finding out more on addon operators, you are welcome to join our slack channel on the Kubernetes &lt;a href="https://kubernetes.slack.com/messages/cluster-addons">#cluster-addons&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/SomtochiAma">Somtochi Onyekwere&lt;/a> is a software engineer that loves contributing to open-source and exploring cloud native solutions.&lt;/em>&lt;/p></description></item><item><title>Blog: Introducing Structured Logs</title><link>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Marek Siarkowicz (Google), Nathan Beach (Google)&lt;/p>
&lt;p>Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.&lt;/p>
&lt;p>In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.&lt;/p>
&lt;p>To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those &amp;quot;key&amp;quot;=&amp;quot;value&amp;quot; pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the &lt;code>--logging-format=json&lt;/code> flag.&lt;/p>
&lt;h2 id="using-structured-logs">Using Structured Logs&lt;/h2>
&lt;p>We've added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">klog.&lt;span style="color:#00a000">InfoS&lt;/span>(&lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, klog.&lt;span style="color:#00a000">KObj&lt;/span>(pod), &lt;span style="color:#b44">&amp;#34;status&amp;#34;&lt;/span>, status)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>will result in this log:&lt;/p>
&lt;pre>&lt;code>I1025 00:15:15.525108 1 controller_utils.go:116] &amp;quot;Pod status updated&amp;quot; pod=&amp;quot;kube-system/kubedns&amp;quot; status=&amp;quot;ready&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Or, if the --logging-format=json flag is set, it will result in this output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;ts&amp;#34;&lt;/span>: &lt;span style="color:#666">1580306777.04728&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;msg&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;pod&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;coredns&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kube-system&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ready&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.&lt;/p>
&lt;p>With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.&lt;/p>
&lt;p>Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.&lt;/p>
&lt;p>Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.&lt;/p>
&lt;p>Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">migrate existing log calls to use structured logs&lt;/a>. It's a great and easy way to make your first contribution to Kubernetes!&lt;/p></description></item><item><title>Blog: Warning: Helpful Warnings Ahead</title><link>https://kubernetes.io/blog/2020/09/03/warnings/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/03/warnings/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Jordan Liggitt (Google)&lt;/p>
&lt;p>As Kubernetes maintainers, we're always looking for ways to improve usability while preserving compatibility.
As we develop features, triage bugs, and answer support questions, we accumulate information that would be helpful for Kubernetes users to know.
In the past, sharing that information was limited to out-of-band methods like release notes, announcement emails, documentation, and blog posts.
Unless someone knew to seek out that information and managed to find it, they would not benefit from it.&lt;/p>
&lt;p>In Kubernetes v1.19, we added a feature that allows the Kubernetes API server to
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1693-warnings">send warnings to API clients&lt;/a>.
The warning is sent using a &lt;a href="https://tools.ietf.org/html/rfc7234#section-5.5">standard &lt;code>Warning&lt;/code> response header&lt;/a>,
so it does not change the status code or response body in any way.
This allows the server to send warnings easily readable by any API client, while remaining compatible with previous client versions.&lt;/p>
&lt;p>Warnings are surfaced by &lt;code>kubectl&lt;/code> v1.19+ in &lt;code>stderr&lt;/code> output, and by the &lt;code>k8s.io/client-go&lt;/code> client library v0.19.0+ in log output.
The &lt;code>k8s.io/client-go&lt;/code> behavior can be &lt;a href="#customize-client-handling">overridden per-process or per-client&lt;/a>.&lt;/p>
&lt;h2 id="deprecation-warnings">Deprecation Warnings&lt;/h2>
&lt;p>The first way we are using this new capability is to send warnings for use of deprecated APIs.&lt;/p>
&lt;p>Kubernetes is a &lt;a href="https://www.cncf.io/cncf-kubernetes-project-journey/#development-velocity">big, fast-moving project&lt;/a>.
Keeping up with the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#changelog-since-v1180">changes&lt;/a>
in each release can be daunting, even for people who work on the project full-time. One important type of change is API deprecations.
As APIs in Kubernetes graduate to GA versions, pre-release API versions are deprecated and eventually removed.&lt;/p>
&lt;p>Even though there is an &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">extended deprecation period&lt;/a>,
and deprecations are &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecation">included in release notes&lt;/a>,
they can still be hard to track. During the deprecation period, the pre-release API remains functional,
allowing several releases to transition to the stable API version. However, we have found that users often don't even realize
they are depending on a deprecated API version until they upgrade to the release that stops serving it.&lt;/p>
&lt;p>Starting in v1.19, whenever a request is made to a deprecated REST API, a warning is returned along with the API response.
This warning includes details about the release in which the API will no longer be available, and the replacement API version.&lt;/p>
&lt;p>Because the warning originates at the server, and is intercepted at the client level, it works for all kubectl commands,
including high-level commands like &lt;code>kubectl apply&lt;/code>, and low-level commands like &lt;code>kubectl get --raw&lt;/code>:&lt;/p>
&lt;p>&lt;img alt="kubectl applying a manifest file, then displaying a warning message 'networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress'."
src="kubectl-warnings.png"
style="width:637px;max-width:100%;">&lt;/p>
&lt;p>This helps people affected by the deprecation to know the request they are making is deprecated,
how long they have to address the issue, and what API they should use instead.
This is especially helpful when the user is applying a manifest they didn't create,
so they have time to reach out to the authors to ask for an updated version.&lt;/p>
&lt;p>We also realized that the person &lt;em>using&lt;/em> a deprecated API is often not the same person responsible for upgrading the cluster,
so we added two administrator-facing tools to help track use of deprecated APIs and determine when upgrades are safe.&lt;/p>
&lt;h3 id="metrics">Metrics&lt;/h3>
&lt;p>Starting in Kubernetes v1.19, when a request is made to a deprecated REST API endpoint,
an &lt;code>apiserver_requested_deprecated_apis&lt;/code> gauge metric is set to &lt;code>1&lt;/code> in the kube-apiserver process.
This metric has labels for the API &lt;code>group&lt;/code>, &lt;code>version&lt;/code>, &lt;code>resource&lt;/code>, and &lt;code>subresource&lt;/code>,
and a &lt;code>removed_version&lt;/code> label that indicates the Kubernetes release in which the API will no longer be served.&lt;/p>
&lt;p>This is an example query using &lt;code>kubectl&lt;/code>, &lt;a href="https://github.com/prometheus/prom2json">prom2json&lt;/a>,
and &lt;a href="https://stedolan.github.io/jq/">jq&lt;/a> to determine which deprecated APIs have been requested
from the current instance of the API server:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl get --raw /metrics | prom2json | jq &lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> .[] | select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels
&lt;/span>&lt;span style="color:#b44">&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;removed_release&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1.22&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;removed_release&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1.22&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;clusterroles&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This shows the deprecated &lt;code>extensions/v1beta1&lt;/code> Ingress and &lt;code>rbac.authorization.k8s.io/v1beta1&lt;/code> ClusterRole APIs
have been requested on this server, and will be removed in v1.22.&lt;/p>
&lt;p>We can join that information with the &lt;code>apiserver_request_total&lt;/code> metrics to get more details about the requests being made to these APIs:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl get --raw /metrics | prom2json | jq &lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> # set $deprecated to a list of deprecated APIs
&lt;/span>&lt;span style="color:#b44"> [
&lt;/span>&lt;span style="color:#b44"> .[] |
&lt;/span>&lt;span style="color:#b44"> select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels |
&lt;/span>&lt;span style="color:#b44"> {group,version,resource}
&lt;/span>&lt;span style="color:#b44"> ] as $deprecated
&lt;/span>&lt;span style="color:#b44">
&lt;/span>&lt;span style="color:#b44"> |
&lt;/span>&lt;span style="color:#b44">
&lt;/span>&lt;span style="color:#b44"> # select apiserver_request_total metrics which are deprecated
&lt;/span>&lt;span style="color:#b44"> .[] | select(.name==&amp;#34;apiserver_request_total&amp;#34;).metrics[] |
&lt;/span>&lt;span style="color:#b44"> select(.labels | {group,version,resource} as $key | $deprecated | index($key))
&lt;/span>&lt;span style="color:#b44">&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;0&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/vnd.kubernetes.protobuf;stream=watch&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;WATCH&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;21&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;200&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/vnd.kubernetes.protobuf&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;LIST&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;200&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/json&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;clusterroles&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;LIST&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output shows that only read requests are being made to these APIs, and the most requests have been made to watch the deprecated Ingress API.&lt;/p>
&lt;p>You can also find that information through the following Prometheus query,
which returns information about requests made to deprecated APIs which will be removed in v1.22:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-promql" data-lang="promql">&lt;span style="color:#b8860b">apiserver_requested_deprecated_apis&lt;/span>{&lt;span style="color:#a0a000">removed_version&lt;/span>&lt;span style="color:#666">=&lt;/span>&amp;#34;&lt;span style="color:#b44">1.22&lt;/span>&amp;#34;}&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">*&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">on&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">group&lt;/span>,&lt;span style="color:#b8860b">version&lt;/span>,&lt;span style="color:#b8860b">resource&lt;/span>,&lt;span style="color:#b8860b">subresource&lt;/span>&lt;span style="color:#666">)&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">group_right&lt;/span>&lt;span style="color:#666">()&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b8860b">apiserver_request_total&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="audit-annotations">Audit Annotations&lt;/h3>
&lt;p>Metrics are a fast way to check whether deprecated APIs are being used, and at what rate,
but they don't include enough information to identify particular clients or API objects.
Starting in Kubernetes v1.19, &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">audit events&lt;/a>
for requests to deprecated APIs include an audit annotation of &lt;code>&amp;quot;k8s.io/deprecated&amp;quot;:&amp;quot;true&amp;quot;&lt;/code>.
Administrators can use those audit events to identify specific clients or objects that need to be updated.&lt;/p>
&lt;h2 id="custom-resource-definitions">Custom Resource Definitions&lt;/h2>
&lt;p>Along with the API server ability to warn about deprecated API use, starting in v1.19, a CustomResourceDefinition can indicate a
&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-deprecation">particular version of the resource it defines is deprecated&lt;/a>.
When API requests to a deprecated version of a custom resource are made, a warning message is returned, matching the behavior of built-in APIs.&lt;/p>
&lt;p>The author of the CustomResourceDefinition can also customize the warning for each version if they want to.
This allows them to give a pointer to a migration guide or other information if needed.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apiextensions.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CustomResourceDefinition&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>crontabs.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">versions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This indicates the v1alpha1 version of the custom resource is deprecated.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># API requests to this version receive a warning in the server response.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deprecated&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This overrides the default warning returned to clients making v1alpha1 API requests.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deprecationWarning&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example.com/v1alpha1 CronTab is deprecated; use example.com/v1 CronTab (see http://example.com/v1alpha1-v1)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This indicates the v1beta1 version of the custom resource is deprecated.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># API requests to this version receive a warning in the server response.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># A default warning message is returned for this version.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deprecated&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="admission-webhooks">Admission Webhooks&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers">Admission webhooks&lt;/a>
are the primary way to integrate custom policies or validation with Kubernetes.
Starting in v1.19, admission webhooks can &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#response">return warning messages&lt;/a>
that are passed along to the requesting API client. Warnings can be returned with allowed or rejected admission responses.&lt;/p>
&lt;p>As an example, to allow a request but warn about a configuration known not to work well, an admission webhook could send this response:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;admission.k8s.io/v1&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;AdmissionReview&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;response&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;lt;value from request.uid&amp;gt;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;allowed&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;warnings&amp;#34;&lt;/span>: [
&lt;span style="color:#b44">&amp;#34;.spec.memory: requests &amp;gt;1GB do not work on Fridays&amp;#34;&lt;/span>
]
}
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are implementing a webhook that returns a warning message, here are some tips:&lt;/p>
&lt;ul>
&lt;li>Don't include a &amp;quot;Warning:&amp;quot; prefix in the message (that is added by clients on output)&lt;/li>
&lt;li>Use warning messages to describe problems the client making the API request should correct or be aware of&lt;/li>
&lt;li>Be brief; limit warnings to 120 characters if possible&lt;/li>
&lt;/ul>
&lt;p>There are many ways admission webhooks could use this new feature, and I'm looking forward to seeing what people come up with.
Here are a couple ideas to get you started:&lt;/p>
&lt;ul>
&lt;li>webhook implementations adding a &amp;quot;complain&amp;quot; mode, where they return warnings instead of rejections,
to allow trying out a policy to verify it is working as expected before starting to enforce it&lt;/li>
&lt;li>&amp;quot;lint&amp;quot; or &amp;quot;vet&amp;quot;-style webhooks, inspecting objects and surfacing warnings when best practices are not followed&lt;/li>
&lt;/ul>
&lt;h2 id="customize-client-handling">Customize Client Handling&lt;/h2>
&lt;p>Applications that use the &lt;code>k8s.io/client-go&lt;/code> library to make API requests can customize
how warnings returned from the server are handled. By default, warnings are logged to
stderr as they are received, but this behavior can be customized
&lt;a href="https://godoc.org/k8s.io/client-go/rest#SetDefaultWarningHandler">per-process&lt;/a>
or &lt;a href="https://godoc.org/k8s.io/client-go/rest#Config">per-client&lt;/a>.&lt;/p>
&lt;p>This example shows how to make your application behave like &lt;code>kubectl&lt;/code>,
overriding message handling process-wide to deduplicate warnings
and highlighting messages using colored output where supported:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;os&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/kubectl/pkg/util/term&amp;#34;&lt;/span>
&lt;span style="color:#666">...&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
rest.&lt;span style="color:#00a000">SetDefaultWarningHandler&lt;/span>(
rest.&lt;span style="color:#00a000">NewWarningWriter&lt;/span>(os.Stderr, rest.WarningWriterOptions{
&lt;span style="color:#080;font-style:italic">// only print a given warning the first time we receive it
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> Deduplicate: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#080;font-style:italic">// highlight the output with color when the output supports it
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> Color: term.&lt;span style="color:#00a000">AllowsColorOutput&lt;/span>(os.Stderr),
},
),
)
&lt;span style="color:#666">...&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The next example shows how to construct a client that ignores warnings.
This is useful for clients that operate on metadata for all resource types
(found dynamically at runtime using the discovery API)
and do not benefit from warnings about a particular resource being deprecated.
Suppressing deprecation warnings is not recommended for clients that require use of particular APIs.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/kubernetes&amp;#34;&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">getClientWithoutWarnings&lt;/span>(config &lt;span style="color:#666">*&lt;/span>rest.Config) (kubernetes.Interface, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
&lt;span style="color:#080;font-style:italic">// copy to avoid mutating the passed-in config
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> config = rest.&lt;span style="color:#00a000">CopyConfig&lt;/span>(config)
&lt;span style="color:#080;font-style:italic">// set the warning handler for this client to ignore warnings
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> config.WarningHandler = rest.NoWarnings{}
&lt;span style="color:#080;font-style:italic">// construct and return the client
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> kubernetes.&lt;span style="color:#00a000">NewForConfig&lt;/span>(config)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="kubectl-strict-mode">Kubectl Strict Mode&lt;/h2>
&lt;p>If you want to be sure you notice deprecations as soon as possible and get a jump start on addressing them,
&lt;code>kubectl&lt;/code> added a &lt;code>--warnings-as-errors&lt;/code> option in v1.19. When invoked with this option,
&lt;code>kubectl&lt;/code> treats any warnings it receives from the server as errors and exits with a non-zero exit code:&lt;/p>
&lt;p>&lt;img alt="kubectl applying a manifest file with a --warnings-as-errors flag, displaying a warning message and exiting with a non-zero exit code."
src="kubectl-warnings-as-errors.png"
style="width:637px;max-width:100%;">&lt;/p>
&lt;p>This could be used in a CI job to apply manifests to a current server,
and required to pass with a zero exit code in order for the CI job to succeed.&lt;/p>
&lt;h2 id="future-possibilities">Future Possibilities&lt;/h2>
&lt;p>Now that we have a way to communicate helpful information to users in context,
we're already considering other ways we can use this to improve people's experience with Kubernetes.
A couple areas we're looking at next are warning about &lt;a href="http://issue.k8s.io/64841#issuecomment-395141013">known problematic values&lt;/a>
we cannot reject outright for compatibility reasons, and warning about use of deprecated fields or field values
(like selectors using beta os/arch node labels, &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#beta-kubernetes-io-arch-deprecated">deprecated in v1.14&lt;/a>).
I'm excited to see progress in this area, continuing to make it easier to use Kubernetes.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/liggitt">Jordan Liggitt&lt;/a> is a software engineer at Google, and helps lead Kubernetes authentication, authorization, and API efforts.&lt;/em>&lt;/p></description></item><item><title>Blog: Scaling Kubernetes Networking With EndpointSlices</title><link>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Rob Scott (Google)&lt;/p>
&lt;p>EndpointSlices are an exciting new API that provides a scalable and extensible alternative to the Endpoints API. EndpointSlices track IP addresses, ports, readiness, and topology information for Pods backing a Service.&lt;/p>
&lt;p>In Kubernetes 1.19 this feature is enabled by default with kube-proxy reading from &lt;a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">EndpointSlices&lt;/a> instead of Endpoints. Although this will mostly be an invisible change, it should result in noticeable scalability improvements in large clusters. It also enables significant new features in future Kubernetes releases like &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service-topology/">Topology Aware Routing&lt;/a>.&lt;/p>
&lt;h2 id="scalability-limitations-of-the-endpoints-api">Scalability Limitations of the Endpoints API&lt;/h2>
&lt;p>With the Endpoints API, there was only one Endpoints resource for a Service. That meant that it needed to be able to store IP addresses and ports (network endpoints) for every Pod that was backing the corresponding Service. This resulted in huge API resources. To compound this problem, kube-proxy was running on every node and watching for any updates to Endpoints resources. If even a single network endpoint changed in an Endpoints resource, the whole object would have to be sent to each of those instances of kube-proxy.&lt;/p>
&lt;p>A further limitation of the Endpoints API is that it limits the number of network endpoints that can be tracked for a Service. The default size limit for an object stored in etcd is 1.5MB. In some cases that can limit an Endpoints resource to 5,000 Pod IPs. This is not an issue for most users, but it becomes a significant problem for users with Services approaching this size.&lt;/p>
&lt;p>To show just how significant these issues become at scale it helps to have a simple example. Think about a Service which has 5,000 Pods, it might end up with a 1.5MB Endpoints resource. If even a single network endpoint in that list changes, the full Endpoints resource will need to be distributed to each Node in the cluster. This becomes quite an issue in a large cluster with 3,000 Nodes. Each update would involve sending 4.5GB of data (1.5MB Endpoints * 3,000 Nodes) across the cluster. That's nearly enough to fill up a DVD, and it would happen for each Endpoints change. Imagine a rolling update that results in all 5,000 Pods being replaced - that's more than 22TB (or 5,000 DVDs) worth of data transferred.&lt;/p>
&lt;h2 id="splitting-endpoints-up-with-the-endpointslice-api">Splitting endpoints up with the EndpointSlice API&lt;/h2>
&lt;p>The EndpointSlice API was designed to address this issue with an approach similar to sharding. Instead of tracking all Pod IPs for a Service with a single Endpoints resource, we split them into multiple smaller EndpointSlices.&lt;/p>
&lt;p>Consider an example where a Service is backed by 15 pods. We'd end up with a single Endpoints resource that tracked all of them. If EndpointSlices were configured to store 5 endpoints each, we'd end up with 3 different EndpointSlices:
&lt;img src="https://kubernetes.io/images/blog/2020-09-02-scaling-kubernetes-networking-endpointslices/endpoint-slices.png" alt="EndpointSlices">&lt;/p>
&lt;p>By default, EndpointSlices store as many as 100 endpoints each, though this can be configured with the &lt;code>--max-endpoints-per-slice&lt;/code> flag on kube-controller-manager.&lt;/p>
&lt;h2 id="endpointslices-provide-10x-scalability-improvements">EndpointSlices provide 10x scalability improvements&lt;/h2>
&lt;p>This API dramatically improves networking scalability. Now when a Pod is added or removed, only 1 small EndpointSlice needs to be updated. This difference becomes quite noticeable when hundreds or thousands of Pods are backing a single Service.&lt;/p>
&lt;p>Potentially more significant, now that all Pod IPs for a Service don't need to be stored in a single resource, we don't have to worry about the size limit for objects stored in etcd. EndpointSlices have already been used to scale Services beyond 100,000 network endpoints.&lt;/p>
&lt;p>All of this is brought together with some significant performance improvements that have been made in kube-proxy. When using EndpointSlices at scale, significantly less data will be transferred for endpoints updates and kube-proxy should be faster to update iptables or ipvs rules. Beyond that, Services can now scale to at least 10 times beyond any previous limitations.&lt;/p>
&lt;h2 id="endpointslices-enable-new-functionality">EndpointSlices enable new functionality&lt;/h2>
&lt;p>Introduced as an alpha feature in Kubernetes v1.16, EndpointSlices were built to enable some exciting new functionality in future Kubernetes releases. This could include dual-stack Services, topology aware routing, and endpoint subsetting.&lt;/p>
&lt;p>Dual-Stack Services are an exciting new feature that has been in development alongside EndpointSlices. They will utilize both IPv4 and IPv6 addresses for Services and rely on the addressType field on EndpointSlices to track these addresses by IP family.&lt;/p>
&lt;p>Topology aware routing will update kube-proxy to prefer routing requests within the same zone or region. This makes use of the topology fields stored for each endpoint in an EndpointSlice. As a further refinement of that, we're exploring the potential of endpoint subsetting. This would allow kube-proxy to only watch a subset of EndpointSlices. For example, this might be combined with topology aware routing so that kube-proxy would only need to watch EndpointSlices containing endpoints within the same zone. This would provide another very significant scalability improvement.&lt;/p>
&lt;h2 id="what-does-this-mean-for-the-endpoints-api">What does this mean for the Endpoints API?&lt;/h2>
&lt;p>Although the EndpointSlice API is providing a newer and more scalable alternative to the Endpoints API, the Endpoints API will continue to be considered generally available and stable. The most significant change planned for the Endpoints API will involve beginning to truncate Endpoints that would otherwise run into scalability issues.&lt;/p>
&lt;p>The Endpoints API is not going away, but many new features will rely on the EndpointSlice API. To take advantage of the new scalability and functionality that EndpointSlices provide, applications that currently consume Endpoints will likely want to consider supporting EndpointSlices in the future.&lt;/p></description></item><item><title>Blog: Ephemeral volumes with storage capacity tracking: EmptyDir on steroids</title><link>https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>Some applications need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance. Other applications expect some read-only input
data to be present in files, like configuration data or secret keys.&lt;/p>
&lt;p>Kubernetes already supports several kinds of such &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes">ephemeral
volumes&lt;/a>, but the
functionality of those is limited to what is implemented inside
Kubernetes.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/">CSI ephemeral volumes&lt;/a>
made it possible to extend Kubernetes with CSI
drivers that provide light-weight, local volumes. These &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md#motivation">&lt;em>inject
arbitrary states, such as configuration, secrets, identity, variables
or similar
information&lt;/em>&lt;/a>.
CSI drivers must be modified to support this Kubernetes feature,
i.e. normal, standard-compliant CSI drivers will not work, and
by design such volumes are supposed to be usable on whatever node
is chosen for a pod.&lt;/p>
&lt;p>This is problematic for volumes which consume significant resources on
a node or for special storage that is only available on some nodes.
Therefore, Kubernetes 1.19 introduces two new alpha features for
volumes that are conceptually more like the &lt;code>EmptyDir&lt;/code> volumes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes#generic-ephemeral-volumes">&lt;em>generic&lt;/em> ephemeral volumes&lt;/a> and&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity">CSI storage capacity tracking&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The advantages of the new approach are:&lt;/p>
&lt;ul>
&lt;li>Storage can be local or network-attached.&lt;/li>
&lt;li>Volumes can have a fixed size that applications are never able to exceed.&lt;/li>
&lt;li>Works with any CSI driver that supports provisioning of persistent
volumes and (for capacity tracking) implements the CSI &lt;code>GetCapacity&lt;/code> call.&lt;/li>
&lt;li>Volumes may have some initial data, depending on the driver and
parameters.&lt;/li>
&lt;li>All of the typical volume operations (snapshotting,
resizing, the future storage capacity tracking, etc.)
are supported.&lt;/li>
&lt;li>The volumes are usable with any app controller that accepts
a Pod or volume specification.&lt;/li>
&lt;li>The Kubernetes scheduler itself picks suitable nodes, i.e. there is
no need anymore to implement and configure scheduler extenders and
mutating webhooks.&lt;/li>
&lt;/ul>
&lt;p>This makes generic ephemeral volumes a suitable solution for several
use cases:&lt;/p>
&lt;h1 id="use-cases">Use cases&lt;/h1>
&lt;h2 id="persistent-memory-as-dram-replacement-for-memcached">Persistent Memory as DRAM replacement for memcached&lt;/h2>
&lt;p>Recent releases of memcached added &lt;a href="https://memcached.org/blog/persistent-memory/">support for using Persistent
Memory&lt;/a> (PMEM) instead
of standard DRAM. When deploying memcached through one of the app
controllers, generic ephemeral volumes make it possible to request a PMEM volume
of a certain size from a CSI driver like
&lt;a href="https://intel.github.io/pmem-csi/">PMEM-CSI&lt;/a>.&lt;/p>
&lt;h2 id="local-lvm-storage-as-scratch-space">Local LVM storage as scratch space&lt;/h2>
&lt;p>Applications working with data sets that exceed the RAM size can
request local storage with performance characteristics or size that is
not met by the normal Kubernetes &lt;code>EmptyDir&lt;/code> volumes. For example,
&lt;a href="https://github.com/cybozu-go/topolvm">TopoLVM&lt;/a> was written for that
purpose.&lt;/p>
&lt;h2 id="read-only-access-to-volumes-with-data">Read-only access to volumes with data&lt;/h2>
&lt;p>Provisioning a volume might result in a non-empty volume:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support">restore a snapshot&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource">cloning a volume&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20200120-generic-data-populators.md">generic data populators&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Such volumes can be mounted read-only.&lt;/p>
&lt;h1 id="how-it-works">How it works&lt;/h1>
&lt;h2 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h2>
&lt;p>The key idea behind generic ephemeral volumes is that a new volume
source, the so-called
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/#ephemeralvolumesource-v1alpha1-core">&lt;code>EphemeralVolumeSource&lt;/code>&lt;/a>
contains all fields that are needed to created a volume claim
(historically called persistent volume claim, PVC). A new controller
in the &lt;code>kube-controller-manager&lt;/code> waits for Pods which embed such a
volume source and then creates a PVC for that pod. To a CSI driver
deployment, that PVC looks like any other, so no special support is
needed.&lt;/p>
&lt;p>As long as these PVCs exist, they can be used like any other volume claim. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.&lt;/p>
&lt;p>Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (&lt;code>-&lt;/code>) in the
middle. This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known. The downside is that the name might
be in use already. This is detected by Kubernetes and then blocks Pod
startup.&lt;/p>
&lt;p>To ensure that the volume gets deleted together with the pod, the
controller makes the Pod the owner of the volume claim. When the Pod
gets deleted, the normal garbage-collection mechanism also removes the
claim and thus the volume.&lt;/p>
&lt;p>Claims select the storage driver through the normal storage class
mechanism. Although storage classes with both immediate and late
binding (aka &lt;code>WaitForFirstConsumer&lt;/code>) are supported, for ephemeral
volumes it makes more sense to use &lt;code>WaitForFirstConsumer&lt;/code>: then Pod
scheduling can take into account both node utilization and
availability of storage when choosing a node. This is where the other
new feature comes in.&lt;/p>
&lt;h2 id="storage-capacity-tracking">Storage capacity tracking&lt;/h2>
&lt;p>Normally, the Kubernetes scheduler has no information about where a
CSI driver might be able to create a volume. It also has no way of
talking directly to a CSI driver to retrieve that information. It
therefore tries different nodes until it finds one where all volumes
can be made available (late binding) or leaves it entirely to the
driver to choose a location (immediate binding).&lt;/p>
&lt;p>The new &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io">&lt;code>CSIStorageCapacity&lt;/code> alpha
API&lt;/a>
allows storing the necessary information in etcd where it is available to the
scheduler. In contrast to support for generic ephemeral volumes,
storage capacity tracking must be &lt;a href="https://github.com/kubernetes-csi/external-provisioner/blob/master/README.md#capacity-support">enabled when deploying a CSI
driver&lt;/a>:
the &lt;code>external-provisioner&lt;/code> must be told to publish capacity
information that it then retrieves from the CSI driver through the normal
&lt;code>GetCapacity&lt;/code> call.&lt;/p>
&lt;!-- TODO: update the link with a revision once https://github.com/kubernetes-csi/external-provisioner/pull/450 is merged -->
&lt;p>When the Kubernetes scheduler needs to choose a node for a Pod with an
unbound volume that uses late binding and the CSI driver deployment
has opted into the feature by setting the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csidriver-v1beta1-storage-k8s-io">&lt;code>CSIDriver.storageCapacity&lt;/code>
flag&lt;/a>
flag, the scheduler automatically filters out nodes that do not have
access to enough storage capacity. This works for generic ephemeral
and persistent volumes but &lt;em>not&lt;/em> for CSI ephemeral volumes because the
parameters of those are opaque for Kubernetes.&lt;/p>
&lt;p>As usual, volumes with immediate binding get created before scheduling
pods, with their location chosen by the storage driver. Therefore, the
external-provisioner's default configuration skips storage
classes with immediate binding as the information wouldn't be used anyway.&lt;/p>
&lt;p>Because the Kubernetes scheduler must act on potentially outdated
information, it cannot be ensured that the capacity is still available
when a volume is to be created. Still, the chances that it can be created
without retries should be higher.&lt;/p>
&lt;h1 id="security">Security&lt;/h1>
&lt;h2 id="csistoragecapacity">CSIStorageCapacity&lt;/h2>
&lt;p>CSIStorageCapacity objects are namespaced. When deploying each CSI
drivers in its own namespace and, as recommended, limiting the RBAC
permissions for CSIStorageCapacity to that namespace, it is
always obvious where the data came from. However, Kubernetes does
not check that and typically drivers get installed in the same
namespace anyway, so ultimately drivers are &lt;em>expected to behave&lt;/em> and
not publish incorrect data.&lt;/p>
&lt;h2 id="generic-ephemeral-volumes-1">Generic ephemeral volumes&lt;/h2>
&lt;p>If users have permission to create a Pod (directly or indirectly),
then they can also create generic ephemeral volumes even when they do
not have permission to create a volume claim. That's because RBAC
permission checks are applied to the controller which creates the
PVC, not the original user. This is a fundamental change that must be
&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes#security">taken into
account&lt;/a> before
enabling the feature in clusters where untrusted users are not
supposed to have permission to create volumes.&lt;/p>
&lt;h1 id="example">Example&lt;/h1>
&lt;p>A &lt;a href="https://github.com/intel/pmem-csi/commits/kubernetes-1-19-blog-post">special branch&lt;/a>
in PMEM-CSI contains all the necessary changes to bring up a
Kubernetes 1.19 cluster inside QEMU VMs with both alpha features
enabled. The PMEM-CSI driver code is used unchanged, only the
deployment was updated.&lt;/p>
&lt;p>On a suitable machine (Linux, non-root user can use Docker - see the
&lt;a href="https://intel.github.io/pmem-csi/0.7/docs/autotest.html#qemu-and-kubernetes">QEMU and
Kubernetes&lt;/a>
section in the PMEM-CSI documentation), the following commands bring
up a cluster and install the PMEM-CSI driver:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">git clone --branch=kubernetes-1-19-blog-post https://github.com/intel/pmem-csi.git
cd pmem-csi
export TEST_KUBERNETES_VERSION=1.19 TEST_FEATURE_GATES=CSIStorageCapacity=true,GenericEphemeralVolume=true TEST_PMEM_REGISTRY=intel
make start &amp;amp;&amp;amp; echo &amp;amp;&amp;amp; test/setup-deployment.sh
&lt;/code>&lt;/pre>&lt;p>If all goes well, the output contains the following usage
instructions:&lt;/p>
&lt;pre>&lt;code>The test cluster is ready. Log in with [...]/pmem-csi/_work/pmem-govm/ssh.0, run
kubectl once logged in. Alternatively, use kubectl directly with the
following env variable:
KUBECONFIG=[...]/pmem-csi/_work/pmem-govm/kube.config
secret/pmem-csi-registry-secrets created
secret/pmem-csi-node-secrets created
serviceaccount/pmem-csi-controller created
...
To try out the pmem-csi driver ephemeral volumes:
cat deploy/kubernetes-1.19/pmem-app-ephemeral.yaml |
[...]/pmem-csi/_work/pmem-govm/ssh.0 kubectl create -f -
&lt;/code>&lt;/pre>&lt;p>The CSIStorageCapacity objects are not meant to be human-readable, so
some post-processing is needed. The following Golang template filters
all objects by the storage class that the example uses and prints the
name, topology and capacity:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get \
-o go-template='{{range .items}}{{if eq .storageClassName &amp;quot;pmem-csi-sc-late-binding&amp;quot;}}{{.metadata.name}} {{.nodeTopology.matchLabels}} {{.capacity}}
{{end}}{{end}}' \
csistoragecapacities
&lt;/code>&lt;/pre>&lt;pre>&lt;code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 30716Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
&lt;/code>&lt;/pre>&lt;p>One individual object has the following content:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl describe csistoragecapacities/csisc-6cw8j
&lt;/code>&lt;/pre>&lt;pre>&lt;code>Name: csisc-sqdnt
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
API Version: storage.k8s.io/v1alpha1
Capacity: 30716Mi
Kind: CSIStorageCapacity
Metadata:
Creation Timestamp: 2020-08-11T15:41:03Z
Generate Name: csisc-
Managed Fields:
...
Owner References:
API Version: apps/v1
Controller: true
Kind: StatefulSet
Name: pmem-csi-controller
UID: 590237f9-1eb4-4208-b37b-5f7eab4597d1
Resource Version: 2994
Self Link: /apis/storage.k8s.io/v1alpha1/namespaces/default/csistoragecapacities/csisc-sqdnt
UID: da36215b-3b9d-404a-a4c7-3f1c3502ab13
Node Topology:
Match Labels:
pmem-csi.intel.com/node: pmem-csi-pmem-govm-worker1
Storage Class Name: pmem-csi-sc-late-binding
Events: &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;p>Now let's create the example app with one generic ephemeral
volume. The &lt;code>pmem-app-ephemeral.yaml&lt;/code> file contains:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#080;font-style:italic"># This example Pod definition demonstrates&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># how to use generic ephemeral inline volumes&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># with a PMEM-CSI storage class.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-app-inline-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-frontend&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>intel/pmem-csi-driver-test:v0.7.14&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;sleep&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;100000&amp;#34;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/data&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeClaimTemplate&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>4Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pmem-csi-sc-late-binding&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After creating that as shown in the usage instructions above, we have one additional Pod and PVC:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get pods/my-csi-app-inline-volume -o wide
&lt;/code>&lt;/pre>&lt;pre>&lt;code>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
my-csi-app-inline-volume 1/1 Running 0 6m58s 10.36.0.2 pmem-csi-pmem-govm-worker1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get pvc/my-csi-app-inline-volume-my-csi-volume
&lt;/code>&lt;/pre>&lt;pre>&lt;code>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
my-csi-app-inline-volume-my-csi-volume Bound pvc-c11eb7ab-a4fa-46fe-b515-b366be908823 4Gi RWO pmem-csi-sc-late-binding 9m21s
&lt;/code>&lt;/pre>&lt;p>That PVC is owned by the Pod:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get -o yaml pvc/my-csi-app-inline-volume-my-csi-volume
&lt;/code>&lt;/pre>&lt;pre>&lt;code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
annotations:
pv.kubernetes.io/bind-completed: &amp;quot;yes&amp;quot;
pv.kubernetes.io/bound-by-controller: &amp;quot;yes&amp;quot;
volume.beta.kubernetes.io/storage-provisioner: pmem-csi.intel.com
volume.kubernetes.io/selected-node: pmem-csi-pmem-govm-worker1
creationTimestamp: &amp;quot;2020-08-11T15:44:57Z&amp;quot;
finalizers:
- kubernetes.io/pvc-protection
managedFields:
...
name: my-csi-app-inline-volume-my-csi-volume
namespace: default
ownerReferences:
- apiVersion: v1
blockOwnerDeletion: true
controller: true
kind: Pod
name: my-csi-app-inline-volume
uid: 75c925bf-ca8e-441a-ac67-f190b7a2265f
...
&lt;/code>&lt;/pre>&lt;p>Eventually, the storage capacity information for &lt;code>pmem-csi-pmem-govm-worker1&lt;/code> also gets updated:&lt;/p>
&lt;pre>&lt;code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 26620Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
&lt;/code>&lt;/pre>&lt;p>If another app needs more than 26620Mi, the Kubernetes
scheduler will not pick &lt;code>pmem-csi-pmem-govm-worker1&lt;/code> anymore.&lt;/p>
&lt;h1 id="next-steps">Next steps&lt;/h1>
&lt;p>Both features are under development. Several open questions were
already raised during the alpha review process. The two enhancement
proposals document the work that will be needed for migration to beta and what
alternatives were already considered and rejected:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/9d7a75d/keps/sig-storage/1698-generic-ephemeral-volumes/README.md">KEP-1698: generic ephemeral inline
volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/9d7a75d/keps/sig-storage/1472-storage-capacity-tracking">KEP-1472: Storage Capacity
Tracking&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Your feedback is crucial for driving that development. SIG-Storage
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets
regularly&lt;/a>
and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and a mailing
list&lt;/a>.&lt;/p></description></item><item><title>Blog: Increasing the Kubernetes Support Window to One Year</title><link>https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Tim Pepper (VMware), Nick Young (VMware)&lt;/p>
&lt;p>Starting with Kubernetes 1.19, the support window for Kubernetes versions &lt;a href="https://github.com/kubernetes/enhancements/issues/1498">will increase from 9 months to one year&lt;/a>. The longer support window is intended to allow organizations to perform major upgrades at a time of the year that works the best for them.&lt;/p>
&lt;p>This is a big change. For many years, the Kubernetes project has delivered a new minor release (e.g.: 1.13 or 1.14) every 3 months. The project provides bugfix support via patch releases (e.g.: 1.13.Y) for three parallel branches of the codebase. Combined, this led to each minor release (e.g.: 1.13) having a patch release stream of support for approximately 9 months. In the end, a cluster operator had to upgrade at least every 9 months to remain supported.&lt;/p>
&lt;p>A survey conducted in early 2019 by the WG LTS showed that a significant subset of Kubernetes end-users fail to upgrade within the 9-month support period.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-31-increase-kubernetes-support-one-year/versions-in-production-text-2.png" alt="Versions in Production">&lt;/p>
&lt;p>This, and other responses from the survey, suggest that a considerable portion of our community would better be able to manage their deployments on supported versions if the patch support period were extended to 12-14 months. It appears to be true regardless of whether the users are on DIY builds or commercially vendored distributions. An extension in the patch support length of time would thus lead to a larger percentage of our user base running supported versions compared to what we have now.&lt;/p>
&lt;p>A yearly support period provides the cushion end-users appear to desire, and is more aligned with familiar annual planning cycles.
There are many unknowns about changing the support windows for a project with as many moving parts as Kubernetes. Keeping the change relatively small (relatively being the important word), gives us the chance to find out what those unknowns are in detail and address them.
From Kubernetes version 1.19 on, the support window will be extended to one year. For Kubernetes versions 1.16, 1.17, and 1.18, the story is more complicated.&lt;/p>
&lt;p>All of these versions still fall under the older “three releases support” model, and will drop out of support when 1.19, 1.20 and 1.21 are respectively released. However, because the 1.19 release has been delayed due to the events of 2020, they will end up with close to a year of support (depending on their exact release dates).&lt;/p>
&lt;p>For example, 1.19 was released on the 26th of August 2020, which is 11 months since the release of 1.16. Since 1.16 is still under the old release policy, this means that it is now out of support.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-31-increase-kubernetes-support-one-year/support-timeline.png" alt="Support Timeline">&lt;/p>
&lt;p>If you’ve got thoughts or feedback, we’d love to hear them. Please contact us on &lt;a href="https://kubernetes.slack.com/messages/wg-lts/">#wg-lts&lt;/a> on the Kubernetes Slack, or to the &lt;a href="https://groups.google.com/g/kubernetes-wg-lts">kubernetes-wg-lts mailing list&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.19: Accentuate the Paw-sitive</title><link>https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/</link><pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md">Kubernetes 1.19 Release Team&lt;/a>&lt;/p>
&lt;p>Finally, we have arrived with Kubernetes 1.19, the second release for 2020, and by far the longest release cycle lasting 20 weeks in total. It consists of 34 enhancements: 10 enhancements are moving to stable, 15 enhancements in beta, and 9 enhancements in alpha.&lt;/p>
&lt;p>The 1.19 release was quite different from a regular release due to COVID-19, the George Floyd protests, and several other global events that we experienced as a release team. Due to these events, we made the decision to adjust our timeline and allow the SIGs, Working Groups, and contributors more time to get things done. The extra time also allowed for people to take time to focus on their lives outside of the Kubernetes project, and ensure their mental wellbeing was in a good place.&lt;/p>
&lt;p>Contributors are the heart of Kubernetes, not the other way around. The Kubernetes code of conduct asks that people be excellent to one another and despite the unrest in our world, we saw nothing but greatness and humility from the community.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="increase-kubernetes-support-window-to-one-year">Increase Kubernetes support window to one year&lt;/h3>
&lt;p>A survey conducted in early 2019 by the &lt;a href="https://github.com/kubernetes/community/tree/master/wg-lts#readme">Long Term Support (LTS) working group&lt;/a> showed that a significant subset of Kubernetes end-users fail to upgrade within the current 9-month support period.
This, and other responses from the survey, suggest that 30% of users would be able to keep their deployments on supported versions if the patch support period were extended to 12-14 months. This appears to be true regardless of whether the users are on self build or commercially vendored distributions. An extension would thus lead to more than 80% of users being on supported versions, instead of the 50-60% we have now.
A yearly support period provides the cushion end-users appear to desire, and is more in harmony with familiar annual planning cycles.
From Kubernetes version 1.19 on, the support window will be extended to one year.&lt;/p>
&lt;h3 id="storage-capacity-tracking">Storage capacity tracking&lt;/h3>
&lt;p>Traditionally, the Kubernetes scheduler was based on the assumptions that additional persistent storage is available everywhere in the cluster and has infinite capacity. Topology constraints addressed the first point, but up to now pod scheduling was still done without considering that the remaining storage capacity may not be enough to start a new pod. &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">Storage capacity tracking&lt;/a>, a new alpha feature, addresses that by adding an API for a CSI driver to report storage capacity and uses that information in the Kubernetes scheduler when choosing a node for a pod. This feature serves as a stepping stone for supporting dynamic provisioning for local volumes and other volume types that are more capacity constrained.&lt;/p>
&lt;h4 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h4>
&lt;p>Kubernetes provides volume plugins whose lifecycle is tied to a pod and can be used as scratch space (e.g. the builtin &lt;code>emptydir&lt;/code> volume type) or to load some data in to a pod (e.g. the builtin &lt;code>configmap&lt;/code> and &lt;code>secret&lt;/code> volume types, or “CSI inline volumes”). The new &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">generic ephemeral volumes&lt;/a> alpha feature allows any existing storage driver that supports dynamic provisioning to be used as an ephemeral volume with the volume’s lifecycle bound to the Pod.
It can be used to provide scratch storage that is different from the root disk, for example persistent memory, or a separate local disk on that node.
All StorageClass parameters for volume provisioning are supported.
All features supported with PersistentVolumeClaims are supported, such as storage capacity tracking, snapshots and restore, and volume resizing.&lt;/p>
&lt;h4 id="csi-volume-health-monitoring">CSI Volume Health Monitoring&lt;/h4>
&lt;p>The alpha version of CSI health monitoring is being released with Kubernetes 1.19. This feature enables CSI Drivers to share abnormal volume conditions from the underlying storage systems with Kubernetes so that they can be reported as events on PVCs or Pods. This feature serves as a stepping stone towards programmatic detection and resolution of individual volume health issues by Kubernetes.&lt;/p>
&lt;h3 id="ingress-graduates-to-general-availability">Ingress graduates to General Availability&lt;/h3>
&lt;p>In terms of moving the Ingress API towards GA, the API itself has been available in beta for so long that it has attained de facto GA status through usage and adoption (both by users and by load balancer / ingress controller providers). Abandoning it without a full replacement is not a viable approach. It is clearly a useful API and captures a non-trivial set of use cases. At this point, it seems more prudent to declare the current API as something the community will support as a V1, codifying its status, while working on either a V2 Ingress API or an entirely different API with a superset of features.&lt;/p>
&lt;h3 id="structured-logging">Structured logging&lt;/h3>
&lt;p>Before v1.19, logging in the Kubernetes control plane couldn't guarantee any uniform structure for log messages and references to Kubernetes objects in those logs. This makes parsing, processing, storing, querying and analyzing logs hard and forces administrators and developers to rely on ad-hoc solutions in most cases based on some regular expressions. Due to those problems any analytical solution based on those logs is hard to implement and maintain.&lt;/p>
&lt;h4 id="new-klog-methods">New klog methods&lt;/h4>
&lt;p>This Kubernetes release introduces new methods to the &lt;em>klog&lt;/em> library that provide a more structured interface for formatting log messages. Each existing formatted log method (&lt;code>Infof&lt;/code>, &lt;code>Errorf&lt;/code>) is now matched by a structured method (&lt;code>InfoS&lt;/code>, &lt;code>ErrorS&lt;/code>). The new logging methods accept log messages as a first argument and a list of key-values pairs as a variadic second argument. This approach allows incremental adoption of structured logging without converting &lt;strong>all&lt;/strong> of Kubernetes to a new API at one time.&lt;/p>
&lt;h3 id="client-tls-certificate-rotation-for-kubelet">Client TLS certificate rotation for kubelet&lt;/h3>
&lt;p>A kubelet authenticates the kubelet to the kube-apiserver using a private key and certificate. The certificate is supplied to the kubelet when it is first booted, via an out-of-cluster mechanism. Since Kubernetes v1.8, clusters have included a (beta) process for obtaining the initial cert/key pair and rotating it as expiration of the certificate approaches. In Kubernetes v1.19 this graduates to stable.&lt;/p>
&lt;p>During the kubelet start-up sequence, the filesystem is scanned for an existing cert/key pair, which is managed by the certificate manager. In the case that a cert/key is available it will be loaded. If not, the kubelet checks its config file for an encoded certificate value or a file reference in the kubeconfig. If the certificate is a bootstrap certificate, this will be used to generate a key, create a certificate signing request and request a signed certificate from the API server.&lt;/p>
&lt;p>When an expiration approaches the cert manager takes care of providing the correct certificate, generating new private keys and requesting new certificates. With the kubelet requesting certificates be signed as part of its boot sequence, and on an ongoing basis, certificate signing requests from the kubelet need to be auto approved to make cluster administration manageable.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/135">Seccomp&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/266">Kubelet client TLS certificate rotation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/279">Limit node access to API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/383">Redesign Event API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1453">Graduate Ingress to V1&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1547">Building Kubelet without Docker&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/693">Node Topology Manager&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">New Endpoint API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1498">Increase Kubernetes support window to one year&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="other-notable-features">Other Notable Features&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1451">Run multiple Scheduling Profiles&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1412">Immutable Secrets and ConfigMaps&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="release-notes">Release Notes&lt;/h2>
&lt;p>Check out the full details of the Kubernetes 1.19 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md">release notes&lt;/a>.&lt;/p>
&lt;h2 id="availability">Availability&lt;/h2>
&lt;p>Kubernetes 1.19 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.19.0">GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local Kubernetes clusters using Docker container “nodes” with &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> (Kubernetes in Docker). You can also easily install 1.19 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h2 id="release-team">Release Team&lt;/h2>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md">release team&lt;/a> led by Taylor Dolezal, Senior Developer Advocate at HashiCorp. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">49,000 individual contributors&lt;/a> to date and an active community of more than 3,000 people.&lt;/p>
&lt;h2 id="release-logo">Release Logo&lt;/h2>
&lt;p>All of you inspired this Kubernetes 1.19 release logo! This release was a bit more of a marathon and a testament to when the world is a wild place, we can come together and do unbelievable things.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-26-kubernetes-1.19-release-announcement/accentuate.png" alt="Kubernetes 1.19 Release Logo">&lt;/p>
&lt;p>&amp;quot;Accentuate the Paw-sitive&amp;quot; was chosen as the release theme because it captures the positive outlook that the release team had, despite the state of the world. The characters pictured in the 1.19 logo represent everyone's personalities on our release team, from emo to peppy, and beyond!&lt;/p>
&lt;p>About the designer: Hannabeth Lagerlof is a Visual Designer based in Los Angeles, California, and she has an extensive background in Environments and Graphic Design. Hannabeth creates art and user experiences that inspire connection. You can find Hannabeth on Twitter as @emanate_design.&lt;/p>
&lt;h2 id="the-long-run">The Long Run&lt;/h2>
&lt;p>The release was also different from the enhancements side of things. Traditionally, we have had 3-4 weeks between the call for enhancements and Enhancements Freeze, which ends the phase in which contributors can acknowledge whether a particular feature will be part of the cycle. This release cycle, being unique, we had five weeks for the same milestone. The extended duration gave the contributors more time to plan and decide about the graduation of their respective features.&lt;/p>
&lt;p>The milestone until which contributors implement the features was extended from the usual five weeks to 7 weeks. Contributors were provided with 40% more time to work on their features, resulting in reduced fatigue and more to think through about the implementation. We also noticed a considerable reduction in last-minute hustles. There were also a lesser number of exception requests this cycle - 6 compared to 14 the previous release cycle.&lt;/p>
&lt;h2 id="user-highlights">User Highlights&lt;/h2>
&lt;ul>
&lt;li>The CNCF grants Zalando, Europe’s leading online platform for fashion and lifestyle, the &lt;a href="https://www.cncf.io/announcement/2020/08/20/cloud-native-computing-foundation-grants-zalando-the-top-end-user-award/">Top End User Award&lt;/a>. Zalando leverages numerous CNCF projects and open sourced multiple of their own development.&lt;/li>
&lt;/ul>
&lt;h2 id="ecosystem-updates">Ecosystem Updates&lt;/h2>
&lt;ul>
&lt;li>The CNCF just concluded its very first Virtual KubeCon. All talks are &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">on-demand&lt;/a> for anyone registered, it's not too late!&lt;/li>
&lt;li>The &lt;a href="https://www.cncf.io/blog/2020/07/15/certified-kubernetes-security-specialist-cks-coming-in-november/">Certified Kubernetes Security Specialist&lt;/a> (CKS) coming in November! CKS focuses on cluster &amp;amp; system hardening, minimizing microservice vulnerabilities and the security of the supply chain.&lt;/li>
&lt;li>CNCF published the second &lt;a href="https://www.cncf.io/blog/2020/08/14/state-of-cloud-native-development/">State of Cloud Native Development&lt;/a>, showing the massively growing number of cloud native developer using container and serverless technology.&lt;/li>
&lt;li>&lt;a href="https://www.kubernetes.dev">Kubernetes.dev&lt;/a>, a Kubernetes contributor focused website has been launched. It brings the contributor documentation, resources and project event information into one central location.&lt;/li>
&lt;/ul>
&lt;h2 id="project-velocity">Project Velocity&lt;/h2>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">Kubernetes DevStats dashboard&lt;/a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. If you want to gather numbers, facts and figures from Kubernetes and the CNCF community it is the best place to start.&lt;/p>
&lt;p>During this release cycle from April till August, 382 different companies and over 2,464 individuals contributed to Kubernetes. &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&amp;amp;from=1585692000000&amp;amp;to=1598392799000">Check out DevStats&lt;/a> to learn more about the overall velocity of the Kubernetes project and community.&lt;/p>
&lt;h2 id="upcoming-release-webinar">Upcoming release webinar&lt;/h2>
&lt;p>Join the members of the Kubernetes 1.19 release team on September 25th, 2020 to learn about the major features in this release including storage capacity tracking, structured logging, Ingress V1 GA, and many more. Register here: &lt;a href="https://www.cncf.io/webinars/kubernetes-1-19/">https://www.cncf.io/webinars/kubernetes-1-19/&lt;/a>.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our monthly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the new &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Moving Forward From Beta</title><link>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</link><pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Tim Bannister, The Scale Factory&lt;/p>
&lt;p>In Kubernetes, features follow a defined
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">lifecycle&lt;/a>.
First, as the twinkle of an eye in an interested developer. Maybe, then,
sketched in online discussions, drawn on the online equivalent of a cafe
napkin. This rough work typically becomes a
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/0001-kubernetes-enhancement-proposal-process.md#kubernetes-enhancement-proposal-process">Kubernetes Enhancement Proposal&lt;/a> (KEP), and
from there it usually turns into code.&lt;/p>
&lt;p>For Kubernetes v1.20 and onwards, we're focusing on helping that code
graduate into stable features.&lt;/p>
&lt;p>That lifecycle I mentioned runs as follows:&lt;/p>
&lt;p>&lt;img src="feature_stages.svg" alt="Alpha → Beta → General Availability">&lt;/p>
&lt;p>Usually, alpha features aren't enabled by default. You turn them on by setting a feature
gate; usually, by setting a command line flag on each of the components that use the
feature.&lt;/p>
&lt;p>(If you use Kubernetes through a managed service offering such as AKS, EKS, GKE, etc then
the vendor who runs that service may have decided what feature gates are enabled for you).&lt;/p>
&lt;p>There's a defined process for graduating an existing, alpha feature into the beta phase.
This is important because &lt;strong>beta features are enabled by default&lt;/strong>, with the feature flag still
there so cluster operators can opt out if they want.&lt;/p>
&lt;p>A similar but more thorough set of graduation criteria govern the transition to general
availability (GA), also known as &amp;quot;stable&amp;quot;. GA features are part of Kubernetes, with a
commitment that they are staying in place throughout the current major version.&lt;/p>
&lt;p>Having beta features on by default lets Kubernetes and its contributors get valuable
real-world feedback. However, there's a mismatch of incentives. Once a feature is enabled
by default, people will use it. Even if there might be a few details to shake out,
the way Kubernetes' REST APIs and conventions work mean that any future stable API is going
to be compatible with the most recent beta API: your API objects won't stop working when
a beta feature graduates to GA.&lt;/p>
&lt;p>For the API and its resources in particular, there's a much less strong incentive to move
features from beta to GA than from alpha to beta. Vendors who want a particular feature
have had good reason to help get code to the point where features are enabled by default,
and beyond that the journey has been less clear.&lt;/p>
&lt;p>KEPs track more than code improvements. Essentially, anything that would need
communicating to the wider community merits a KEP. That said, most KEPs cover
Kubernetes features (and the code to implement them).&lt;/p>
&lt;p>You might know that &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
has been in Kubernetes for a while, but did you realize that it actually went beta in 2015? To help
drive things forward, Kubernetes' Architecture Special Interest Group (SIG) have a new approach in
mind.&lt;/p>
&lt;h2 id="avoiding-permanent-beta">Avoiding permanent beta&lt;/h2>
&lt;p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts a countdown.
The beta-quality API now has &lt;strong>three releases&lt;/strong> (about nine calendar months) to either:&lt;/p>
&lt;ul>
&lt;li>reach GA, and deprecate the beta, or&lt;/li>
&lt;li>have a new beta version (&lt;em>and deprecate the previous beta&lt;/em>).&lt;/li>
&lt;/ul>
&lt;p>To be clear, at this point &lt;strong>only REST APIs are affected&lt;/strong>. For example, &lt;em>APIListChunking&lt;/em> is
a beta feature but isn't itself a REST API. Right now there are no plans to automatically
deprecate &lt;em>APIListChunking&lt;/em> nor any other features that aren't REST APIs.&lt;/p>
&lt;p>If a beta API has not graduated to GA after three Kubernetes releases, then the
next Kubernetes release will deprecate that API version. There's no option for
the REST API to stay at the same beta version beyond the first Kubernetes
release to come out after the release window.&lt;/p>
&lt;h3 id="what-this-means-for-you">What this means for you&lt;/h3>
&lt;p>If you're using Kubernetes, there's a good chance that you're using a beta feature. Like
I said, there are lots of them about.
As well as Ingress, you might be using &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a>,
or &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a>, or others.
There's an even bigger chance that you're running on a control plane with at least one beta
feature enabled.&lt;/p>
&lt;p>If you're using or generating Kubernetes manifests that use beta APIs like Ingress, you'll
need to plan to revise those. The current APIs are going to be deprecated following a
schedule (the 9 months I mentioned earlier) and after a further 9 months those deprecated
APIs will be removed. At that point, to stay current with Kubernetes, you should already
have migrated.&lt;/p>
&lt;h3 id="what-this-means-for-kubernetes-contributors">What this means for Kubernetes contributors&lt;/h3>
&lt;p>The motivation here seems pretty clear: get features stable. Guaranteeing that beta
features will be deprecated adds a pretty big incentive so that people who want the
feature continue their effort until the code, documentation and tests are ready for this
feature to graduate to stable, backed by several Kubernetes' releases of evidence in
real-world use.&lt;/p>
&lt;h3 id="what-this-means-for-the-ecosystem">What this means for the ecosystem&lt;/h3>
&lt;p>In my opinion, these harsh-seeming measures make a lot of sense, and are going to be
good for Kubernetes. Deprecating existing APIs, through a rule that applies across all
the different Special Interest Groups (SIGs), helps avoid stagnation and encourages
fixes.&lt;/p>
&lt;p>Let's say that an API goes to beta and then real-world experience shows that it
just isn't right - that, fundamentally, the API has shortcomings. With that 9 month
countdown ticking, the people involved have the means and the justification to revise
and release an API that deals with the problem cases. Anyone who wants to live with
the deprecated API is welcome to - Kubernetes is open source - but their needs do not
have to hold up progress on the feature.&lt;/p></description></item><item><title>Blog: Introducing Hierarchical Namespaces</title><link>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</link><pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adrian Ludwin (Google)&lt;/p>
&lt;p>Safely hosting large numbers of users on a single Kubernetes cluster has always
been a troublesome task. One key reason for this is that different organizations
use Kubernetes in different ways, and so no one tenancy model is likely to suit
everyone. Instead, Kubernetes offers you building blocks to create your own
tenancy solution, such as Role Based Access Control (RBAC) and NetworkPolicies;
the better these building blocks, the easier it is to safely build a multitenant
cluster.&lt;/p>
&lt;h1 id="namespaces-for-tenancy">Namespaces for tenancy&lt;/h1>
&lt;p>By far the most important of these building blocks is the namespace, which forms
the backbone of almost all Kubernetes control plane security and sharing
policies. For example, RBAC, NetworkPolicies and ResourceQuotas all respect
namespaces by default, and objects such as Secrets, ServiceAccounts and
Ingresses are freely usable &lt;em>within&lt;/em> any one namespace, but fully segregated
from &lt;em>other&lt;/em> namespaces.&lt;/p>
&lt;p>Namespaces have two key properties that make them ideal for policy enforcement.
Firstly, they can be used to &lt;strong>represent ownership&lt;/strong>. Most Kubernetes objects
&lt;em>must&lt;/em> be in a namespace, so if you use namespaces to represent ownership, you
can always count on there being an owner.&lt;/p>
&lt;p>Secondly, namespaces have &lt;strong>authorized creation and use&lt;/strong>. Only
highly-privileged users can create namespaces, and other users require explicit
permission to use those namespaces - that is, create, view or modify objects in
those namespaces. This allows them to be carefully created with appropriate
policies, before unprivileged users can create “regular” objects like pods and
services.&lt;/p>
&lt;h1 id="the-limits-of-namespaces">The limits of namespaces&lt;/h1>
&lt;p>However, in practice, namespaces are not flexible enough to meet some common use
cases. For example, let’s say that one team owns several microservices with
different secrets and quotas. Ideally, they should place these services into
different namespaces in order to isolate them from each other, but this presents
two problems.&lt;/p>
&lt;p>Firstly, these namespaces have no common concept of ownership, even though
they’re both owned by the same team. This means that if the team controls
multiple namespaces, not only does Kubernetes not have any record of their
common owner, but namespaced-scoped policies cannot be applied uniformly across
them.&lt;/p>
&lt;p>Secondly, teams generally work best if they can operate autonomously, but since
namespace creation is highly privileged, it’s unlikely that any member of the
dev team is allowed to create namespaces. This means that whenever a team wants
a new namespace, they must raise a ticket to the cluster administrator. While
this is probably acceptable for small organizations, it generates unnecessary
toil as the organization grows.&lt;/p>
&lt;h1 id="introducing-hierarchical-namespaces">Introducing hierarchical namespaces&lt;/h1>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic">Hierarchical
namespaces&lt;/a>
are a new concept developed by the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">Kubernetes Working Group for Multi-Tenancy
(wg-multitenancy)&lt;/a> in order to
solve these problems. In its simplest form, a hierarchical namespace is a
regular Kubernetes namespace that contains a small custom resource that
identifies a single, optional, parent namespace. This establishes the concept of
ownership &lt;em>across&lt;/em> namespaces, not just &lt;em>within&lt;/em> them.&lt;/p>
&lt;p>This concept of ownership enables two additional types of behaviours:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Policy inheritance:&lt;/strong> if one namespace is a child of another, policy objects
such as RBAC RoleBindings are &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-propagation">copied from the parent to the
child&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Delegated creation:&lt;/strong> you usually need cluster-level privileges to create a
namespace, but hierarchical namespaces adds an alternative:
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-subns">&lt;em>subnamespaces&lt;/em>&lt;/a>,
which can be manipulated using only limited permissions in the parent
namespace.&lt;/li>
&lt;/ul>
&lt;p>This solves both of the problems for our dev team. The cluster administrator can
create a single “root” namespace for the team, along with all necessary
policies, and then delegate permission to create subnamespaces to members of
that team. Those team members can then create subnamespaces for their own use,
without violating the policies that were imposed by the cluster administrators.&lt;/p>
&lt;h1 id="hands-on-with-hierarchical-namespaces">Hands-on with hierarchical namespaces&lt;/h1>
&lt;p>Hierarchical namespaces are provided by a Kubernetes extension known as the
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc">&lt;strong>Hierarchical Namespace
Controller&lt;/strong>&lt;/a>,
or &lt;strong>HNC&lt;/strong>. The HNC consists of two components:&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>manager&lt;/strong> runs on your cluster, manages subnamespaces, propagates policy
objects, ensures that your hierarchies are legal and manages extension points.&lt;/li>
&lt;li>The &lt;strong>kubectl plugin&lt;/strong>, called &lt;code>kubectl-hns&lt;/code>, makes it easy for users to
interact with the manager.&lt;/li>
&lt;/ul>
&lt;p>Both can be easily installed from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases">releases page of our
repo&lt;/a>.&lt;/p>
&lt;p>Let’s see HNC in action. Imagine that I do not have namespace creation
privileges, but I can view the namespace &lt;code>team-a&lt;/code> and create subnamespaces
within it&lt;sup>&lt;a href="#note-1">1&lt;/a>&lt;/sup>. Using the plugin, I can now say:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns create svc1-team-a -n team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This creates a subnamespace called &lt;code>svc1-team-a&lt;/code>. Note that since subnamespaces
are just regular Kubernetes namespaces, all subnamespace names must still be
unique.&lt;/p>
&lt;p>I can view the structure of these namespaces by asking for a tree view:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns tree team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
team-a
└── svc1-team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And if there were any policies in the parent namespace, these now appear in the
child as well&lt;sup>&lt;a href="#note-2">2&lt;/a>&lt;/sup>. For example, let’s say that &lt;code>team-a&lt;/code> had
an RBAC RoleBinding called &lt;code>sres&lt;/code>. This rolebinding will also be present in the
subnamespace:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl describe rolebinding sres -n svc1-team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
Name: sres
Labels: hnc.x-k8s.io/inheritedFrom&lt;span style="color:#666">=&lt;/span>team-a &lt;span style="color:#080;font-style:italic"># inserted by HNC&lt;/span>
Annotations: &amp;lt;none&amp;gt;
Role:
Kind: ClusterRole
Name: admin
Subjects: ...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, HNC adds labels to these namespaces with useful information about the
hierarchy which you can use to apply other policies. For example, you can create
the following NetworkPolicy:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NetworkPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>allow-team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;team-a.tree.hnc.x-k8s.io/depth&amp;#39;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Label created by HNC&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Exists&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This policy will both be propagated to all descendants of &lt;code>team-a&lt;/code>, and will
&lt;em>also&lt;/em> allow ingress traffic between all of those namespaces. The “tree” label
can only be applied by HNC, and is guaranteed to reflect the latest hierarchy.&lt;/p>
&lt;p>You can learn all about the features of HNC from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc/docs/user-guide">user
guide&lt;/a>.&lt;/p>
&lt;h1 id="next-steps-and-getting-involved">Next steps and getting involved&lt;/h1>
&lt;p>If you think that hierarchical namespaces can work for your organization, &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases/tag/hnc-v0.5.1">HNC
v0.5.1 is available on
GitHub&lt;/a>.
We’d love to know what you think of it, what problems you’re using it to solve
and what features you’d most like to see added. As with all early software, you
should be cautious about using HNC in production environments, but the more
feedback we get, the sooner we’ll be able to drive to HNC 1.0.&lt;/p>
&lt;p>We’re also open to additional contributors, whether it’s to fix or report bugs,
or help prototype new features such as exceptions, improved monitoring,
hierarchical resource quotas or fine-grained configuration.&lt;/p>
&lt;p>Please get in touch with us via our
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">repo&lt;/a>, &lt;a href="https://groups.google.com/g/kubernetes-wg-multitenancy">mailing
list&lt;/a> or on
&lt;a href="https://kubernetes.slack.com/messages/wg-multitenancy">Slack&lt;/a> - we look forward
to hearing from you!&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/aludwin">Adrian Ludwin&lt;/a> is a software engineer and the
tech lead for the Hierarchical Namespace Controller.&lt;/em>&lt;/p>
&lt;a name="note-1"/>
&lt;p>&lt;em>Note 1: technically, you create a small object called a &amp;quot;subnamespace anchor&amp;quot;
in the parent namespace, and then HNC creates the subnamespace for you.&lt;/em>&lt;/p>
&lt;a name="note-2"/>
&lt;p>&lt;em>Note 2: By default, only RBAC Roles and RoleBindings are propagated, but you
can configure HNC to propagate any namespaced Kubernetes object.&lt;/em>&lt;/p></description></item><item><title>Blog: Physics, politics and Pull Requests: the Kubernetes 1.18 release interview</title><link>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The start of the COVID-19 pandemic couldn't delay the release of Kubernetes 1.18, but unfortunately &lt;a href="https://github.com/kubernetes/utils/issues/141">a small bug&lt;/a> could — thankfully only by a day. This was the last cat that needed to be herded by 1.18 release lead &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> before the &lt;a href="https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/">release on March 25&lt;/a>.&lt;/p>
&lt;p>One of the best parts about co-hosting the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> is the conversations we have with the people who help bring Kubernetes releases together. &lt;a href="https://kubernetespodcast.com/episode/096-kubernetes-1.18/">Jorge was our guest on episode 96&lt;/a> back in March, and &lt;a href="https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/">just like last week&lt;/a> we are delighted to bring you the transcript of this interview.&lt;/p>
&lt;p>If you'd rather enjoy the &amp;quot;audiobook version&amp;quot;, including another interview when 1.19 is released later this month, &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe to the show&lt;/a> wherever you get your podcasts.&lt;/p>
&lt;p>In the last few weeks, we've talked to long-time Kubernetes contributors and SIG leads &lt;a href="https://kubernetespodcast.com/episode/114-scheduling/">David Oppenheimer&lt;/a>, &lt;a href="https://kubernetespodcast.com/episode/113-instrumentation-and-cadvisor/">David Ashpole&lt;/a> and &lt;a href="https://kubernetespodcast.com/episode/111-scalability/">Wojciech Tyczynski&lt;/a>. All are worth taking the dog for a longer walk to listen to!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You're a former physicist. I have to ask, what kind of physics did you work on?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Back in my days of math and all that, I used to work in &lt;a href="https://en.wikipedia.org/wiki/Computational_biology">computational biology&lt;/a> and a little bit of high energy physics. Computational biology was, for the most part, what I spent most of my time on. And it was essentially exploring the big idea of we have the structure of proteins. We know what they're made of. Now, based on that structure, we want to be able to predict &lt;a href="https://en.wikipedia.org/wiki/Protein_folding">how they're going to fold&lt;/a> and how they're going to behave, which essentially translates into the whole idea of designing pharmaceuticals, designing vaccines, or anything that you can possibly think of that has any connection whatsoever to a living organism.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That would seem to ladder itself well into maybe going to something like bioinformatics. Did you take a tour into that, or did you decide to go elsewhere directly?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It is related, and I worked a little bit with some people that did focus on bioinformatics on the field specifically, but I never took a detour into it. Really, my big idea with computational biology, to be honest, it wasn't even the biology. That's usually what sells it, what people are really interested in, because protein engineering, all the cool and amazing things that you can do.&lt;/p>
&lt;p>Which is definitely good, and I don't want to take away from it. But my big thing is because biology is such a real thing, it is amazingly complicated. And the math— the models that you have to design to study those systems, to be able to predict something that people can actually experiment and measure, it just captivated me. The level of complexity, the beauty, the mechanisms, all the structures that you see once you got through the math and look at things, it just kind of got to me.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How did you go from that world into the world of Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: That's both a really boring story and an interesting one.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>I did my thing with physics, and it was good. It was fun. But at some point, I wanted— working in academia— at least my feeling for it is that generally all the people that you're surrounded with are usually academics. Just another bunch of physics, a bunch of mathematicians.&lt;/p>
&lt;p>But very seldom do you actually get the opportunity to take what you're working on and give it to someone else to use. Even with the mathematicians and physicists, the things that we're working on are super specialized, and you can probably find three, four, five people that can actually understand everything that you're saying. A lot of people are going to get the gist of it, but understanding the details, it's somewhat rare.&lt;/p>
&lt;p>One of the things that I absolutely love about tech, about software engineering, coding, all that, is how open and transparent everything is. You can write your library in Python, you can publish it, and suddenly the world is going to actually use it, actually consume it. And because normally, I've seen that it has a large avenue where you can work in something really complicated, you can communicate it, and people can actually go ahead and take it and run with it in their given direction. And that is kind of what happened.&lt;/p>
&lt;p>At some point, by pure accident and chance, I came across this group of people on the internet, and they were in the stages of making up this new group that's called &lt;a href="https://datafordemocracy.org/">Data for Democracy&lt;/a>, a non-profit. And the whole idea was the internet, especially Twitter— that's how we congregated— Twitter, the internet. We have a ton of data scientists, people who work as software engineers, and the like. What if we all come together and try to solve some issues that actually affect the daily lives of people. And there were a ton of projects. Helping the ACLU gather data for something interesting that they were doing, gather data and analyze it for local governments— where do you have potholes, how much water is being consumed.&lt;/p>
&lt;p>Try to apply all the science that we knew, combined with all the code that we could write, and offer a good and digestible idea for people to say, OK, this makes sense, let's do something about it— policy, action, whatever. And I started working with this group, Data for Democracy— wonderful set of people. And the person who I believe we can blame for Data for Democracy— the one who got the idea and got it up and running, his name is Jonathan Morgan. And eventually, we got to work together. He started a startup, and I went to work with the startup. And that was essentially the thing that took me away from physics and into the world of software engineering— Data for Democracy, definitely.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Were you using Kubernetes as part of that work there?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: No, it was simple as it gets. You just try to get some data. You create a couple &lt;a href="https://ipython.org/">IPython notebooks&lt;/a>, some setting up of really simple MySQL databases, and that was it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Where did you get started using Kubernetes? And was it before you started contributing to it and being a part, or did you decide to jump right in?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: When I first started using Kubernetes, it was also on my first job. So there wasn't a lot of specific training in regards to software engineering or anything of the sort that I did before I actually started working as a software engineer. I just went from physicist to engineer. And in my days of physics, at least on the computer side, I was completely trained in the super old school system administrator, where you have your 10, 20 computers. You know physically where they are, and you have to connect the cables.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: All pets— all pets all the time.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] You have to have your huge Python, bash scripts, three, five major versions, all because doing an upgrade will break something really important and you have no idea how to work on it. And that was my training. That was the way that I learned how to do things. Those were the kind of things that I knew how to do.&lt;/p>
&lt;p>And when I got to this company— startup— we were pretty much starting from scratch. We were building a couple applications. We work testing them, we were deploying them on a couple of managed instances. But like everything, there was a lot of toil that we wanted to automate. The whole issue of, OK, after days of work, we finally managed to get this version of the application up and running in these machines.&lt;/p>
&lt;p>It's open to the internet. People can test it out. But it turns out that it is now two weeks behind the latest on all the master branches for this repo, so now we want to update. And we have to go through the process of bringing it back up, creating new machines, do that whole thing. And I had no idea what Kubernetes was, to be honest. My boss at the moment mentioned it to me like, hey, we should use Kubernetes because apparently, Kubernetes is something that might be able to help us here. And we did some— I want to call it research and development.&lt;/p>
&lt;p>It was actually just making— again, startup, small company, small team, so really me just playing around with Kubernetes trying to get it to work, trying to get it to run. I was so lost. I had no idea what I was doing— not enough. I didn't have an idea of how Kubernetes was supposed to help me. And at that point, I did the best Googling that I could manage. Didn't really find a lot of examples. Didn't find a lot of blog posts. It was early.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What time frame was this?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Three, four years ago, so definitely not 1.13. That's the best guesstimate that I can give at this point. But I wasn't able to find any good examples, any tutorials. The only book that I was able to get my hands on was the one written by Joe Beda, Kelsey Hightower, and I forget the other author. But what is it? &amp;quot;&lt;a href="%5D(http://shop.oreilly.com/product/0636920223788.do)">Kubernetes— Up and Running&lt;/a>&amp;quot;?&lt;/p>
&lt;p>And in general, right now I use it as reference— it's really good. But as a beginner, I still was lost. They give all these amazing examples, they provide the applications, but I had no idea why someone might need a Pod, why someone might need a Deployment. So my last resort was to try and find someone who actually knew Kubernetes.&lt;/p>
&lt;p>By accident, during my eternal Googling, I actually found a link to the &lt;a href="http://slack.kubernetes.io/">Kubernetes Slack&lt;/a>. I jumped into the Kubernetes Slack hoping that someone might be able to help me out. And that was my entry point into the Kubernetes community. I just kept on exploring the Slack, tried to see what people were talking about, what they were asking to try to make sense of it, and just kept on iterating. And at some point, I think I got the hang of it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What made you decide to be a release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The answer to this is my answer to why I have been contributing to Kubernetes. I really just want to be able to help out the community. Kubernetes is something that I absolutely adore.&lt;/p>
&lt;p>Comparing Kubernetes to old school system administration, a handful of years ago, it took me like a week to create a node for an application to run. It took me months to get something that vaguely looked like an Ingress resource— just setting up the Nginx, and allowing someone else to actually use my application. And the fact that I could do all of that in five minutes, it really captivated me. Plus I've got to blame it on the physics. The whole idea with physics, I really like the patterns, and I really like the design of Kubernetes.&lt;/p>
&lt;p>Once I actually got the hang of it, I loved the idea of how everything was designed, and I just wanted to learn a lot more about it. And I wanted to help the contributors. I wanted to help the people who actually build it. I wanted to help maintain it, and help provide the information for new contributors or new users. So instead of taking months for them to be up and running, let's just chat about what your issue is, and let's try to get a fix within the next hour or so.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: You work for a stealth startup right now. Is it fair to assume that they're using Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yes—&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>—for everything.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Are you able to say what &lt;a href="https://www.searchable.ai/">Searchable&lt;/a> does?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The thing that we are trying to build is kind of like a search engine for your documents. Usually, if people have a question, they jump on Google. And for the most part, you're going to be able to get a good answer. You can ask something really random, like 'what is the weight of an elephant?'&lt;/p>
&lt;p>Which, if you think about it, it's kind of random, but Google is going to give you an answer. And the thing that we are trying to build is something similar to that, but for files. So essentially, a search engine for your files. And most people, you have your local machine loaded up with— at least mine, I have a couple tens of gigabytes of different files.&lt;/p>
&lt;p>I have Google Drive. I have a lot of documents that live in my email and the like. So the idea is to kind of build a search engine that is going to be able to connect all of those pieces. And besides doing simple word searches— for example, 'Kubernetes interview', and bring me the documents that we're looking at with all the questions— I can also ask things like what issue did I find last week while testing Prometheus. And it's going to be able to read my files, like through natural language processing, understand it, and be able to give me an answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: It is a Google for your personal and non-public information, essentially?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Hopefully.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is the work that you do with Kubernetes as the release lead— is that part of your day job, or is that something that you're doing kind of nights and weekends separate from your day job?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Both. Strictly speaking, my day job is just keep working on the application, build the things that it needs, maintain the infrastructure, and all that. When I started working at the company— which by the way, the person who brought me into the company was also someone that I met from my days in Data for Democracy— we started talking about the work.&lt;/p>
&lt;p>I mentioned that I do a lot of work with the Kubernetes community and if it was OK that I continue doing it. And to my surprise, the answer was not only a yes, but yeah, you can do it during your day work. And at least for the time being, I just balance— I try to keep things organized.&lt;/p>
&lt;p>Some days I just focus on Kubernetes. Some mornings I do Kubernetes. And then afternoon, I do Searchable, vice-versa, or just go back and forth, and try to balance the work as much as possible. But being release lead, definitely, it is a lot, so nights and weekends.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How much time does it take to be the release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It varies, but probably, if I had to give an estimate, at the very least you have to be able to dedicate four hours most days.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Four hours a day?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, most days. It varies a lot. For example, at the beginning of the release cycle, you don't need to put in that much work because essentially, you're just waiting and helping people get set up, and people are writing their &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposals&lt;/a>, they are implementing it, and you can answer some questions. It's relatively easy, but for the most part, a lot of the time the four hours go into talking with people, just making sure that, hey, are people actually writing their enhancements, do we have all the enhancements that we want. And most of those fours hours, going around, chatting with people, and making sure that things are being done. And if, for some reason, someone needs help, just directing them to the right place to get their answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What does Searchable get out of you doing this work?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Physically, nothing. The thing that we're striving for is to give back to the community. My manager/boss/homeslice— I told him I was going to call him my homeslice— both of us have experience working in open source. At some point, he was also working on a project that I'm probably going to mispronounce, but Mahout with Apache.&lt;/p>
&lt;p>And he also has had this experience. And both of us have this general idea and strive to build something for Searchable that's going to be useful for people, but also build knowledge, build guides, build applications that are going to be useful for the community. And at least one of the things that I was able to do right now is be the lead for the Kubernetes team. And this is a way of giving back to the community. We're using Kubernetes to run our things, so let's try to balance how things work.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Lachlan Evenson was the release lead on 1.16 as well as &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">our guest back in episode 72&lt;/a>, and he's returned on this release as the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks/emeritus-adviser">emeritus advisor&lt;/a>. What did you learn from him?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Oh, everything. And it actually all started back on 1.16. So like you said, an amazing person— he's an amazing individual. And it's truly an opportunity to be able to work with him. During 1.16, I was the CI Signal lead, and Lachie is very hands on.&lt;/p>
&lt;p>He's not the kind of person to just give you a list of things and say, do them. He actually comes to you, has a conversation, and he works with you more than anything. And when we were working together on 1.16, I got to learn a lot from him in terms of CI Signal. And especially because we talked about everything just to make sure that 1.16 was ready to go, I also got to pick up a couple of things that a release lead has to know, has to be able to do, has to work on to get a release out the door.&lt;/p>
&lt;p>And now, during this release, there is a lot of information that's really useful, and there's a lot of advice and general wisdom that comes in handy. For most of the things that impact a lot of things, we are always in communication. Like, I'm doing this, you're doing that, advice. And essentially, every single thing that we do is pretty much a code review. You do it, and then you wait for someone else to give you comments. And that's been a strong part of our relationship working.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would you say the theme for this release is?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I think one of the themes is &amp;quot;fit and finish&amp;quot;. There are a lot of features that we are bumping from alpha to beta, from beta to stable. And we want to make sure that people have a good user experience. Operators and developers alike just want to get rid of as many bugs as possible, improve the flow of things.&lt;/p>
&lt;p>But the other really cool thing is we have about an equal distribution between alpha, beta, and stable. We are also bringing up a lot of new features. So besides making Kubernetes more stable for all the users that are already using it, we are working on bringing up new things that people can try out for the next release and see how it goes in the future.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did you have a release team mascot?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Kind of.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Who/what was it?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] I say kind of because I'm using the mascot in the &lt;a href="https://twitter.com/KubernetesPod/status/1242953121380392963">logo&lt;/a>, and the logo is inspired by the Large Hadron Collider.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, fantastic.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Being the release lead, I really had to take a chance on this opportunity to use the LHC as the mascot.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We've had &lt;a href="https://kubernetespodcast.com/episode/062-cern/">some of the folks from the LHC on the show&lt;/a>, and I know they listen, and they will be thrilled with that.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] Hopefully, they like the logo.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you look at this release, what part of this release, what thing that has been added to it are you personally most excited about?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Like a parent can't choose which child is his or her favorite, you really can't choose a specific thing.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We have been following online and in the issues an enhancement that's called &lt;a href="https://github.com/kubernetes/enhancements/issues/753">sidecar containers&lt;/a>. You'd be able to mark the order of containers starting in a pod. Tim Hockin posted &lt;a href="https://github.com/kubernetes/enhancements/issues/753#issuecomment-597372056">a long comment on behalf of a number of SIG Node contributors&lt;/a> citing social, procedural, and technical concerns about what's going on with that— in particular, that it moved out of 1.18 and is now moving to 1.19. Did you have any thoughts on that?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The sidecar enhancement has definitely been an interesting one. First off, thank you very much to Joseph Irving, the author of the KEP. And thank you very much to Tim Hockin, who voiced out the point of view of the approvers, maintainers of SIG Node. And I guess a little bit of context before we move on is, in the Kubernetes community, we have contributors, we have reviewers, and we have approvers.&lt;/p>
&lt;p>Contributors are people who write PRs, who file issues, who troubleshoot issues. Reviewers are contributors who focus on one or multiple specific areas within the project, and then approvers are maintainers for the specific area, for one or multiple specific areas, of the project. So you can think of approvers as people who have write access in a repo or someplace within a repo.&lt;/p>
&lt;p>The issue with the sidecar enhancement is that it has been deferred for multiple releases now, and that's been because there hasn't been a lot of collaboration between the KEP authors and the approvers for specific parts of the project. Something worthwhile to mention— and this was brought up during the original discussion— is this can obviously be frustrating for both contributors and for approvers. From the contributor's side of things, you are working on something. You are doing your best to make sure that it works.&lt;/p>
&lt;p>And to build something that's going to be used by people, both from the approver side of things and, I think, for the most part, every single person in the Kubernetes community, we are all really excited to see this project grow. We want to help improve it, and we love when new people come in and work on new enhancements, bug fixes, and the like.&lt;/p>
&lt;p>But one of the limitations is the day only has so many hours, and there are only so many things that we can work on at a time. So people prioritize in whatever way works best, and some things just fall behind. And a lot of the time, the things that fall behind are not because people don't want them to continue moving forward, but it's just a limited amount of resources, a limited amount of people.&lt;/p>
&lt;p>And I think this discussion around the sidecar enhancement proposal has been very useful, and it points us to the need for more standardized mentoring programs. This is something that multiple SIGs are working on. For example, SIG Contribex, SIG Cluster Lifecycle, SIG Release. The idea is to standardize some sort of mentoring experience so that we can better prepare new contributors to become reviewers and ultimately approvers.&lt;/p>
&lt;p>Because ultimately at the end of the day, if we have more people who are knowledgeable about Kubernetes, or even some specific area of Kubernetes, we can better distribute the load, and we can better collaborate on whatever new things come up. I think the sidecar enhancement has shown us mentoring is something worthwhile, and we need a lot more of it. Because as much work as we do, more things are going to continue popping in throughout the project. And the more people we have who are comfortable working in these really complicated areas of Kubernetes, the better off that we are going to be.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Was there any talk of delaying 1.18 due to the current worldwide health situation?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: We thought about it, and the plan was to just wait and see how people felt. Tried make sure that people were comfortable continuing to work and all the people were landing in new enhancements, or fixing tests, or members of the release team who were making sure that things were happening. We wanted to see that people were comfortable, that they could continue doing their job. And for a moment, I actually thought about delaying just outright— we're going to give it more time, and hopefully at some point, things are going to work out.&lt;/p>
&lt;p>But people just continue doing their amazing work. There was no delay. There was no hitch throughout the process. So at some point, I just figured we stay with the current timeline and see how we went. And at this point, things are more or less set.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Amazing power of a distributed team.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, definitely.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: &lt;a href="https://twitter.com/alejandrox135/status/1239629281766096898">Taylor Dolezal was announced as the 1.19 release lead&lt;/a>. Do you know how that choice was made, and by whom?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I actually got to choose the lead. The practice is the current lead for the release team is going to look at people and see, first off, who's interested and out of the people interested, who can do the job, who's comfortable enough with the release team, with the Kubernetes community at large who can actually commit the amount of hours throughout the next, hopefully, three months.&lt;/p>
&lt;p>And for one, I think Taylor has been part of my team. So there is the release team. Then the release team has multiple subgroups. One of those subgroups is actually just for me and my shadows. So for this release, it was mrbobbytables and Taylor. And Taylor volunteered to take over 1.19, and I'm sure that he will do an amazing job.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I am as well. What advice will you give Taylor?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Over-communicate as much as possible. Normally, if you made it to the point that you are the lead for a release, or even the shadow for a release, you more or less are familiar with a lot of the work— CI Signal, enhancements, documentation, and the like. And a lot of people, if they know how to do their job, they might tell themselves, yeah, I could do it— no need to worry about it. I'm just going to go ahead and sign this PR, debug this test, whatever.&lt;/p>
&lt;p>But one of the interesting aspects is whenever we are actually working in a release, 50% of the work has to go into actually making the release happen. The other 50% of the work has to go into mentoring people, and making sure the newcomers, new members are able to learn everything that they need to learn to do your job, you being in the lead for a subgroup or the entire team. And whenever you actually see that things need to happen, just over-communicate.&lt;/p>
&lt;p>Try to provide the opportunity for someone else to do the work, and over-communicate with them as much as possible to make sure that they are learning whatever it is that they need to learn. If neither you or the other person knows what's going on, then I can over-communicate, so someone hopefully will see your messages and come to the rescue. That happens a lot. There's a lot of really nice and kind people who will come out and tell you how something works, help you fix it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you were to sum up your experience running this release, what would it be?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It's been super fun and a little bit stressing, to be honest. Being the release lead is definitely amazing. You're kind of sitting at the center of Kubernetes.&lt;/p>
&lt;p>You not only see the people who are working on things— the things that are broken, and the users filling out issues, and saying what broke, and the like. But you also get the opportunity to work with a lot of people who do a lot of non-code related work. Docs is one of the most obvious things. There's a lot of work that goes into communications, contributor experience, public relations.&lt;/p>
&lt;p>And being connected, getting to talk with those people mostly every other day, it's really fun. It's a really good experience in terms of becoming a better contributor to the community, but also taking some of that knowledge home with you and applying it somewhere else. If you are a software engineer, if you are a project manager, whatever, it's amazing how much you can learn.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I know the community likes to rotate around who are the release leads. But if you were given the opportunity to be a release lead for a future release of Kubernetes, would you do it again?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, it's a fun job. To be honest, it can be really stressing. Especially, as I mentioned, at some point, most of that work is just going to be talking with people, and talking requires a lot more thought and effort than just sitting down and thinking about things sometimes. And some of that can be really stressful.&lt;/p>
&lt;p>But the job itself, it is definitely fun. And at some distant point in the future, if for some reason it was a possibility, I will think about it. But definitely, as you mentioned, one thing that we try to do is cycle out, because I can have fun in it, and that's all good and nice. And hopefully I can help another release go out the door. But providing the opportunity for other people to learn I think is a lot more important than just being the lead itself.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> is a site reliability engineer with Searchable AI and served as the Kubernetes 1.18 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Music and math: the Kubernetes 1.17 release interview</title><link>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adam Glick (Google)&lt;/p>
&lt;p>Every time the Kubernetes release train stops at the station, we like to ask the release lead to take a moment to reflect on their experience. That takes the form of an interview on the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> that I co-host with &lt;a href="https://twitter.com/craigbox">Craig Box&lt;/a>. If you're not familiar with the show, every week we summarise the new in the Cloud Native ecosystem, and have an insightful discussion with an interesting guest from the broader Kubernetes community.&lt;/p>
&lt;p>At the time of the 1.17 release in December, we &lt;a href="https://kubernetespodcast.com/episode/083-kubernetes-1.17/">talked to release team lead Guinevere Saenger&lt;/a>. We have &lt;a href="https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/">shared&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/">the&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/">transcripts&lt;/a> of previous interviews on the Kubernetes blog, and we're very happy to share another today.&lt;/p>
&lt;p>Next week we will bring you up to date with the story of Kubernetes 1.18, as we gear up for the release of 1.19 next month. &lt;a href="https://kubernetespodcast.com/subscribe/">Subscribe to the show&lt;/a> wherever you get your podcasts to make sure you don't miss that chat!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You have a nontraditional background for someone who works as a software engineer. Can you explain that background?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: My first career was as a &lt;a href="https://en.wikipedia.org/wiki/Collaborative_piano">collaborative pianist&lt;/a>, which is an academic way of saying &amp;quot;piano accompanist&amp;quot;. I was a classically trained pianist who spends most of her time onstage, accompanying other people and making them sound great.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that the piano equivalent of pair-programming?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: No one has said it to me like that before, but all sorts of things are starting to make sense in my head right now. I think that's a really great way of putting it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That's a really interesting background, as someone who also has a background with music. What made you decide to get into software development?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I found myself in a life situation where I needed more stable source of income, and teaching music, and performing for various gig opportunities, was really just not cutting it anymore. And I found myself to be working really, really hard with not much to show for it. I had a lot of friends who were software engineers. I live in Seattle. That's sort of a thing that happens to you when you live in Seattle — you get to know a bunch of software engineers, one way or the other.&lt;/p>
&lt;p>The ones I met were all lovely people, and they said, hey, I'm happy to show you how to program in Python. And so I did that for a bit, and then I heard about this program called &lt;a href="https://adadevelopersacademy.org/">Ada Developers Academy&lt;/a>. That's a year long coding school, targeted at women and non-binary folks that are looking for a second career in tech. And so I applied for that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What can you tell us about that program?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's incredibly selective, for starters. It's really popular in Seattle and has gotten quite a good reputation. It took me three tries to get in. They do two classes a year, and so it was a while before I got my response saying 'congratulations, we are happy to welcome you into Cohort 6'. I think what sets Ada Developers Academy apart from other bootcamp style coding programs are three things, I think? The main important one is that if you get in, you pay no tuition. The entire program is funded by company sponsors.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The other thing that really convinced me is that five months of the 11-month program are an industry internship, which means you get both practical experience, mentorship, and potential job leads at the end of it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So very much like a condensed version of the University of Waterloo degree, where you do co-op terms.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Interesting. I didn't know about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Having lived in Waterloo for a while, I knew a lot of people who did that. But what would you say the advantages were of going through such a condensed schooling process in computer science?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure that the condensed process is necessarily an advantage. I think it's a necessity, though. People have to quit their jobs to go do this program. It's not an evening school type of thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: And your internship is basically a full-time job when you do it. One thing that Ada was really, really good at is giving us practical experience that directly relates to the workplace. We learned how to use Git. We learned how to design websites using &lt;a href="https://rubyonrails.org/">Rails&lt;/a>. And we also learned how to collaborate, how to pair-program. We had a weekly retrospective, so we sort of got a soft introduction to workflows at a real workplace. Adding to that, the internship, and I think the overall experience is a little bit more 'practical workplace oriented' and a little bit less academic.&lt;/p>
&lt;p>When you're done with it, you don't have to relearn how to be an adult in a working relationship with other people. You come with a set of previous skills. There are Ada graduates who have previously been campaign lawyers, and veterinarians, and nannies, cooks, all sorts of people. And it turns out these skills tend to translate, and they tend to matter.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: With your background in music, what do you think that that allows you to bring to software development that could be missing from, say, standard software development training that people go through?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People tend to really connect the dots when I tell them I used to be a musician. Of course, I still consider myself a musician, because you don't really ever stop being a musician. But they say, 'oh, yeah, music and math', and that's just a similar sort of brain. And that makes so much sense. And I think there's a little bit of a point to that. When you learn a piece of music, you have to start recognizing patterns incredibly quickly, almost intuitively.&lt;/p>
&lt;p>And I think that is the main skill that translates into programming— recognizing patterns, finding the things that work, finding the things that don't work. And for me, especially as a collaborative pianist, it's the communicating with people, the finding out what people really want, where something is going, how to figure out what the general direction is that we want to take, before we start writing the first line of code.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: In your experience at Ada or with other experiences you've had, have you been able to identify patterns in other backgrounds for people that you'd recommend, 'hey, you're good at music, so therefore you might want to consider doing something like a course in computer science'?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Overall, I think ultimately writing code is just giving a set of instructions to a computer. And we do that in daily life all the time. We give instructions to our kids, we give instructions to our students. We do math, we write textbooks. We give instructions to a room full of people when you're in court as a lawyer.&lt;/p>
&lt;p>Actually, the entrance exam to Ada Developers Academy used to have questions from the &lt;a href="https://en.wikipedia.org/wiki/Law_School_Admission_Test">LSAT&lt;/a> on it to see if you were qualified to join the program. They changed that when I applied, but I think that's a thing that happened at one point. So, overall, I think software engineering is a much more varied field than we give it credit for, and that there are so many ways in which you can apply your so-called other skills and bring them under the umbrella of software engineering.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I do think that programming is effectively half art and half science. There's creativity to be applied. There is perhaps one way to solve a problem most efficiently. But there are many different ways that you can choose to express how you compiled something down to that way.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah, I mean, that's definitely true. I think one way that you could probably prove that is that if you write code at work and you're working on something with other people, you can probably tell which one of your co-workers wrote which package, just by the way it's written, or how it is documented, or how it is styled, or any of those things. I really do think that the human character shines through.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What got you interested in Kubernetes and open source?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The honest answer is absolutely nothing. Going back to my programming school— and remember that I had to do a five-month internship as part of my training— the way that the internship works is that sponsor companies for the program get interns in according to how much they sponsored a specific cohort of students.&lt;/p>
&lt;p>So at the time, Samsung and SDS offered to host two interns for five months on their &lt;a href="https://samsung-cnct.github.io/">Cloud Native Computing team&lt;/a> and have that be their practical experience. So I go out of a Ruby on Rails full stack web development bootcamp and show up at my internship, and they said, &amp;quot;Welcome to Kubernetes. Try to bring up a cluster.&amp;quot; And I said, &amp;quot;Kuber what?&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've all said that on occasion.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Trial by fire, wow.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I will say that that entire team was absolutely wonderful, delightful to work with, incredibly helpful. And I will forever be grateful for all of the help and support that I got in that environment. It was a great place to learn.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You now work on GitHub's Kubernetes infrastructure. Obviously, there was GitHub before there was a Kubernetes, so a migration happened. What can you tell us about the transition that GitHub made to running on Kubernetes?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A disclaimer here— I was not at GitHub at the time that the transition to Kubernetes was made. However, to the best of my knowledge, the decision to transition to Kubernetes was made and people decided, yes, we want to try Kubernetes. We want to use Kubernetes. And mostly, the only decision left was, which one of our applications should we move over to Kubernetes?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I thought GitHub was written on Rails, so there was only one application.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: [LAUGHING] We have a lot of supplementary stuff under the covers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm sure.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But yes, GitHub is written in Rails. It is still written in Rails. And most of the supplementary things are currently running on Kubernetes. We have a fair bit of stuff that currently does not run on Kubernetes. Mainly, that is GitHub Enterprise related things. I would know less about that because I am on the platform team that helps people use the Kubernetes infrastructure. But back to your question, leadership at the time decided that it would be a good idea to start with GitHub the Rails website as the first project to move to Kubernetes.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: High stakes!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The reason for this was that they decided if they were going to not start big, it really wasn't going to transition ever. It was really not going to happen. So they just decided to go all out, and it was successful, for which I think the lesson would probably be commit early, commit big.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are there any other lessons that you would take away or that you've learned kind of from the transition that the company made, and might be applicable to other people who are looking at moving their companies from a traditional infrastructure to a Kubernetes infrastructure?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure this is a lesson specifically, but I was on support recently, and it turned out that, due to unforeseen circumstances and a mix of human error, a bunch of the namespaces on one of our Kubernetes clusters got deleted.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, my.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It should not have affected any customers, I should mention, at this point. But all in all, it took a few of us a few hours to almost completely recover from this event. I think that, without Kubernetes, this would not have been possible.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Generally, deleting something like that is quite catastrophic. We've seen a number of other vendors suffer large outages when someone's done something to that effect, which is why we get &lt;a href="https://twitter.com/hashtag/hugops">#hugops&lt;/a> on Twitter all the time.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People did send me #hugops, that is a thing that happened. But overall, something like this was an interesting stress test and sort of proved that it wasn't nearly as catastrophic as a worst case scenario.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: GitHub &lt;a href="https://githubengineering.com/githubs-metal-cloud/">runs its own data centers&lt;/a>. Kubernetes was largely built for running on the cloud, but a lot of people do choose to run it on their own, bare metal. How do you manage clusters and provisioning of the machinery you run?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I started, my onboarding project was to deprovision an old cluster, make sure all the traffic got moved to somewhere where it would keep running, provision a new cluster, and then move website traffic onto the new cluster. That was a really exciting onboarding project. At the time, we provisioned bare metal machines using Puppet. We still do that to a degree, but I believe the team that now runs our computing resources actually inserts virtual machines as an extra layer between the bare metal and the Kubernetes nodes.&lt;/p>
&lt;p>Again, I was not intrinsically part of that decision, but my understanding is that it just makes for a greater reliability and reproducibility across the board. We've had some interesting hardware dependency issues come up, and the virtual machines basically avoid those.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've been working with Kubernetes for a couple of years now. How did you get involved in the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I first started in the project, I started at the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-contributor-experience#readme">special interest group for contributor experience&lt;/a>, namely because one of my co-workers at the time, Aaron Crickenberger, was a big Kubernetes community person. Still is.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've &lt;a href="https://kubernetespodcast.com/episode/046-kubernetes-1.14/">had him on the show&lt;/a> for one of these very release interviews!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: In fact, this is true! So Aaron and I actually go way back to Samsung SDS. Anyway, Aaron suggested that I should write up a contribution to the Kubernetes project, and I said, me? And he said, yes, of course. You will be &lt;a href="https://www.youtube.com/watch?v=TkCDUFR6xqw">speaking at KubeCon&lt;/a>, so you should probably get started with a PR or something. So I tried, and it was really, really hard. And I complained about it &lt;a href="https://github.com/kubernetes/community/issues/141">in a public GitHub issue&lt;/a>, and people said, yeah. Yeah, we know it's hard. Do you want to help with that?&lt;/p>
&lt;p>And so I started getting really involved with the &lt;a href="https://github.com/kubernetes/community/tree/master/contributors/guide">process for new contributors to get started&lt;/a> and have successes, kind of getting a foothold into a project that's as large and varied as Kubernetes. From there on, I began to talk to people, get to know people. The great thing about the Kubernetes community is that there is so much mentorship to go around.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: There are so many friendly people willing to help. It's really funny when I talk to other people about it. They say, what do you mean, your coworker? And I said, well, he's really a colleague. He really works for another company.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: He's sort-of officially a competitor.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But we're friends.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But he totally helped me when I didn't know how to git patch my borked pull request. So that happened. And eventually, somebody just suggested that I start following along in the release process and shadow someone on their release team role. And that, at the time, was Tim Pepper, who was bug triage lead, and I shadowed him for that role.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Another &lt;a href="https://kubernetespodcast.com/episode/010-kubernetes-1.11/">podcast guest&lt;/a> on the interview train.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This is a pattern that probably will make more sense once I explain to you about the shadow process of the release team.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Well, let's turn to the Kubernetes release and the release process. First up, what's new in this release of 1.17?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: We have only a very few new things. The one that I'm most excited about is that we have moved &lt;a href="https://github.com/kubernetes/enhancements/issues/563">IPv4 and IPv6 dual stack&lt;/a> support to alpha. That is the most major change, and it has been, I think, a year and a half in coming. So this is the very first cut of that feature, and I'm super excited about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The people who have been promised IPv6 for many, many years and still don't really see it, what will this mean for them?&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: And most importantly, why did we skip IPv5 support?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I don't know!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Please see &lt;a href="https://softwareengineering.stackexchange.com/questions/185380/ipv4-to-ipv6-where-is-ipv5">the appendix to this podcast&lt;/a> for technical explanations.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Having a dual stack configuration obviously enables people to have a much more flexible infrastructure and not have to worry so much about making decisions that will become outdated or that may be over-complicated. This basically means that pods can have dual stack addresses, and nodes can have dual stack addresses. And that basically just makes communication a lot easier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What about features that didn't make it into the release? We had a conversation with Lachie in the &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">1.16 interview&lt;/a>, where he mentioned &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md">sidecar containers&lt;/a>. They unfortunately didn't make it into that release. And I see now that they haven't made this one either.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: They have not, and we are actually currently undergoing an effort of tracking features that flip multiple releases.&lt;/p>
&lt;p>As a community, we need everyone's help. There are a lot of features that people want. There is also a lot of cleanup that needs to happen. And we have started talking at previous KubeCons repeatedly about problems with maintainer burnout, reviewer burnout, have a hard time finding reviews for your particular contributions, especially if you are not an entrenched member of the community. And it has become very clear that this is an area where the entire community needs to improve.&lt;/p>
&lt;p>So the unfortunate reality is that sometimes life happens, and people are busy. This is an open source project. This is not something that has company mandated OKRs. Particularly during the fourth quarter of the year in North America, but around the world, we have a lot of holidays. It is the end of the year. Kubecon North America happened as well. This makes it often hard to find a reviewer in time or to rally the support that you need for your enhancement proposal. Unfortunately, slipping releases is fairly common and, at this point, expected. We started out with having 42 enhancements and &lt;a href="https://docs.google.com/spreadsheets/d/1ebKGsYB1TmMnkx86bR2ZDOibm5KWWCs_UjV3Ys71WIs/edit#gid=0">landed with roughly half of that&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I was going to ask about the truncated schedule due to the fourth quarter of the year, where there are holidays in large parts of the world. Do you find that the Q4 release on the whole is smaller than others, if not for the fact that it's some week shorter?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Q4 releases are shorter by necessity because we are trying to finish the final release of the year before the end of the year holidays. Often, releases are under pressure of KubeCons, during which finding reviewers or even finding the time to do work can be hard to do, if you are attending. And even if you're not attending, your reviewers might be attending.&lt;/p>
&lt;p>It has been brought up last year to make the final release more of a stability release, meaning no new alpha features. In practice, for this release, this is actually quite close to the truth. We have four features graduating to beta and most of our features are graduating to stable. I am hoping to use this as a precedent to change our process to make the final release a stability release from here on out. The timeline fits. The past experience fits this model.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: On top of all of the release work that was going on, there was also KubeCon that happened. And you were involved in the &lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/11-contributor-summit">contributor summit&lt;/a>. How was the summit?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This was the first contributor summit where we had an organized events team with events organizing leads, and handbooks, and processes. And I have heard from multiple people— this is just word of mouth— that it was their favorite contributor summit ever.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Was someone allocated to hat production? &lt;a href="https://flickr.com/photos/143247548@N03/49093218951/">Everyone had sailor hats&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yes, the entire event staff had sailor hats with their GitHub handle on them, and it was pretty fantastic. You can probably see me wearing one in some of the pictures from the contributor summit. That literally was something that was pulled out of a box the morning of the contributor summit, and no one had any idea. But at first, I was a little skeptical, but then I put it on and looked at myself in the mirror. And I was like, yes. Yes, this is accurate. We should all wear these.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did getting everyone together for the contributor summit help with the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It did not. It did quite the opposite, really. Well, that's too strong.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that just a matter of the time taken up?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's just a completely different focus. Honestly, it helped getting to know people face-to-face that I had currently only interacted with on video. But we did have to cancel the release team meeting the day of the contributor summit because there was kind of no sense in having it happen. We moved it to the Tuesday, I believe.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The role of the release team leader has been described as servant leadership. Do you consider the position proactive or reactive?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Honestly, I think that depends on who's the release team lead, right? There are some people who are very watchful and look for trends, trying to detect problems before they happen. I tend to be in that camp, but I also know that sometimes it's not possible to predict things. There will be last minute bugs sometimes, sometimes not. If there is a last minute bug, you have to be ready to be on top of that. So for me, the approach has been I want to make sure that I have my priorities in order and also that I have backups in case I can't be available.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What was the most interesting part of the release process for you?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A release lead has to have served in other roles on the release team prior to being release team lead. To me, it was very interesting to see what other roles were responsible for, ones that I hadn't seen from the inside before, such as docs, CI signal. I had helped out with CI signal for a bit, but I want to give a big shout out to CI signal lead, Alena Varkockova, who was able to communicate effectively and kindly with everyone who was running into broken tests, failing tests. And she was very effective in getting all of our tests up and running.&lt;/p>
&lt;p>So that was actually really cool to see. And yeah, just getting to see more of the workings of the team, for me, it was exciting. The other big exciting thing, of course, was to see all the changes that were going in and all the efforts that were being made.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release lead for 1.18 has just been announced as &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcon&lt;/a>. What are you going to put in the proverbial envelope as advice for him?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I would want Jorge to be really on top of making sure that every Special Interest Group that enters a change, that has an enhancement for 1.18, is on top of the timelines and is responsive. Communication tends to be a problem. And I had hinted at this earlier, but some enhancements slipped simply because there wasn't enough reviewer bandwidth.&lt;/p>
&lt;p>Greater communication of timelines and just giving people more time and space to be able to get in their changes, or at least, seemingly give them more time and space by sending early warnings, is going to be helpful. Of course, he's going to have a slightly longer release, too, than I did. This might be related to a unique Q4 challenge. Overall, I would encourage him to take more breaks, to rely more on his release shadows, and split out the work in a fashion that allows everyone to have a turn and everyone to have a break as well.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would your advice be to someone who is hearing your experience and is inspired to get involved with the Kubernetes release or contributer process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Those are two separate questions. So let me tackle the Kubernetes release question first. Kubernetes &lt;a href="https://github.com/kubernetes/sig-release/#readme">SIG Release&lt;/a> has, in my opinion, a really excellent onboarding program for new members. We have what is called the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md">Release Team Shadow Program&lt;/a>. We also have the Release Engineering Shadow Program, or the Release Management Shadow Program. Those are two separate subprojects within SIG Release. And each subproject has a team of roles, and each role can have two to four shadows that are basically people who are part of that role team, and they are learning that role as they are doing it.&lt;/p>
&lt;p>So for example, if I am the lead for bug triage on the release team, I may have two, three or four people that I closely work with on the bug triage tasks. These people are my shadows. And once they have served one release cycle as a shadow, they are now eligible to be lead in that role. We have an application form for this process, and it should probably be going up in January. It usually happens the first week of the release once all the release leads are put together.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you think being a member of the release team is something that is a good first contribution to the Kubernetes project overall?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It depends on what your goals are, right? I believe so. I believe, for me, personally, it has been incredibly helpful looking into corners of the project that I don't know very much about at all, like API machinery, storage. It's been really exciting to look over all the areas of code that I normally never touch.&lt;/p>
&lt;p>It depends on what you want to get out of it. In general, I think that being a release team shadow is a really, really great on-ramp to being a part of the community because it has a paved path solution to contributing. All you have to do is show up to the meetings, ask questions of your lead, who is required to answer those questions.&lt;/p>
&lt;p>And you also do real work. You really help, you really contribute. If you go across the issues and pull requests in the repo, you will see, 'Hi, my name is so-and-so. I am shadowing the CI signal lead for the current release. Can you help me out here?' And that's a valuable contribution, and it introduces people to others. And then people will recognize your name. They'll see a pull request by you, and they're like oh yeah, I know this person. They're legit.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/guincodes">Guinevere Saenger&lt;/a> is a software engineer for GitHub and served as the Kubernetes 1.17 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: SIG-Windows Spotlight</title><link>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</guid><description>
&lt;p>&lt;em>This post tells the story of how Kubernetes contributors work together to provide a container orchestrator that works for both Linux and Windows.&lt;/em>&lt;/p>
&lt;img alt="Image of a computer with Kubernetes logo" width="30%" src="KubernetesComputer_transparent.png">
&lt;p>Most people who are familiar with Kubernetes are probably used to associating it with Linux. The connection makes sense, since Kubernetes ran on Linux from its very beginning. However, many teams and organizations working on adopting Kubernetes need the ability to orchestrate containers on Windows. Since the release of Docker and rise to popularity of containers, there have been efforts both from the community and from Microsoft itself to make container technology as accessible in Windows systems as it is in Linux systems.&lt;/p>
&lt;p>Within the Kubernetes community, those who are passionate about making Kubernetes accessible to the Windows community can find a home in the Windows Special Interest Group. To learn more about SIG-Windows and the future of Kubernetes on Windows, I spoke to co-chairs &lt;a href="https://github.com/marosset">Mark Rossetti&lt;/a> and &lt;a href="https://github.com/michmike">Michael Michael&lt;/a> about the SIG's goals and how others can contribute.&lt;/p>
&lt;h2 id="intro-to-windows-containers-kubernetes">Intro to Windows Containers &amp;amp; Kubernetes&lt;/h2>
&lt;p>Kubernetes is the most popular tool for orchestrating container workloads, so to understand the Windows Special Interest Group (SIG) within the Kubernetes project, it's important to first understand what we mean when we talk about running containers on Windows.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;When looking at Windows support in Kubernetes,&amp;quot; says SIG (Special Interest Group) Co-chairs Mark Rossetti and Michael Michael, &amp;quot;many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between operational limitations and differences between the Windows and Linux operating systems. Windows containers run the Windows operating system and Linux containers run Linux.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In essence, any &amp;quot;container&amp;quot; is simply a process being run on its host operating system, with some key tooling in place to isolate that process and its dependencies from the rest of the environment. The goal is to make that running process safely isolated, while taking up minimal resources from the system to perform that isolation. On Linux, the tooling used to isolate processes to create &amp;quot;containers&amp;quot; commonly boils down to cgroups and namespaces (among a few others), which are themselves tools built in to the Linux Kernel.&lt;/p>
&lt;img alt="A visual analogy using dogs to explain Linux cgroups and namespaces." width="40%" src="cgroupsNamespacesComboPic.png">
&lt;h4 id="if-dogs-were-processes-containerization-would-be-like-giving-each-dog-their-own-resources-like-toys-and-food-using-cgroups-and-isolating-troublesome-dogs-using-namespaces">&lt;em>If dogs were processes: containerization would be like giving each dog their own resources like toys and food using cgroups, and isolating troublesome dogs using namespaces.&lt;/em>&lt;/h4>
&lt;p>Native Windows processes are processes that are or must be run on a Windows operating system. This makes them fundamentally different from a process running on a Linux operating system. Since Linux containers are Linux processes being isolated by the Linux kernel tools known as cgroups and namespaces, containerizing native Windows processes meant implementing similar isolation tools within the Windows kernel itself. Thus, &amp;quot;Windows Containers&amp;quot; and &amp;quot;Linux Containers&amp;quot; are fundamentally different technologies, even though they have the same goals (isolating processes) and in some ways work similarly (using kernel level containerization).&lt;/p>
&lt;p>So when it comes to running containers on Windows, there are actually two very important concepts to consider:&lt;/p>
&lt;ul>
&lt;li>Native Windows processes running as native Windows Server style containers,&lt;/li>
&lt;li>and traditional Linux containers running on a Linux Kernel, generally hosted on a lightweight Hyper-V Virtual Machine.&lt;/li>
&lt;/ul>
&lt;p>You can learn more about Linux and Windows containers in this &lt;a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/linux-containers">tutorial&lt;/a> from Microsoft.&lt;/p>
&lt;h3 id="kubernetes-on-windows">Kubernetes on Windows&lt;/h3>
&lt;p>Kubernetes was initially designed with Linux containers in mind and was itself designed to run on Linux systems. Because of that, much of the functionality of Kubernetes involves unique Linux functionality. The Linux-specific work is intentional--we all want Kubernetes to run optimally on Linux--but there is a growing demand for similar optimization for Windows servers. For cases where users need container orchestration on Windows, the Kubernetes contributor community of SIG-Windows has incorporated functionality for Windows-specific use cases.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;A common question we get is, will I be able to have a Windows-only cluster. The answer is NO. Kubernetes control plane components will continue to be based on Linux, while SIG-Windows is concentrating on the experience of having Windows worker nodes in a Kubernetes cluster.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>Rather than separating out the concepts of &amp;quot;Windows Kubernetes,&amp;quot; and &amp;quot;Linux Kubernetes,&amp;quot; the community of SIG-Windows works toward adding functionality to the main Kubernetes project which allows it to handle use cases for Windows. These Windows capabilities mirror, and in some cases add unique functionality to, the Linux use cases Kubernetes has served since its release in 2014 (want to learn more history? Scroll through this &lt;a href="https://github.com/kubernetes/kubernetes/blob/e2b948dbfbba62b8cb681189377157deee93bb43/DESIGN.md">original design document&lt;/a>.&lt;/p>
&lt;h2 id="what-does-sig-windows-do">What Does SIG-Windows Do?&lt;/h2>
&lt;hr>
&lt;p>&lt;em>&amp;quot;SIG-Windows is really the center for all things Windows in Kubernetes,&amp;quot;&lt;/em> SIG chairs Mark and Michael said, &lt;em>&amp;quot;We mainly focus on the compute side of things, but really anything related to running Kubernetes on Windows is in scope for SIG-Windows.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In order to best serve users, SIG-Windows works to make the Kubernetes user experience as consistent as possible for users of Windows and Linux. However some use cases simply only apply to one Operating System, and as such, the SIG-Windows group also works to create functionality that is unique to Windows-only workloads.&lt;/p>
&lt;p>Many SIGs, or &amp;quot;Special Interest Groups&amp;quot; within Kubernetes have a narrow focus, allowing members to dive deep on a certain facet of the technology. While specific expertise is welcome, those interested in SIG-Windows will find it to be a great community to build broad understanding across many focus areas of Kubernetes. &amp;quot;Members from our SIG interface with storage, network, testing, cluster-lifecycle and others groups in Kubernetes.&amp;quot;&lt;/p>
&lt;h3 id="who-are-sig-windows-users">Who are SIG-Windows' Users?&lt;/h3>
&lt;p>The best way to understand the technology a group makes, is often to understand who their customers or users are.&lt;/p>
&lt;h4 id="a-majority-of-the-users-we-ve-interacted-with-have-business-critical-infrastructure-running-on-windows-developed-over-many-years-and-can-t-move-those-workloads-to-linux-for-various-reasons-cost-time-compliance-etc-the-sig-chairs-shared-by-transporting-those-workloads-into-windows-containers-and-running-them-in-kubernetes-they-are-able-to-quickly-modernize-their-infrastructure-and-help-migrate-it-to-the-cloud">&amp;quot;A majority of the users we've interacted with have business-critical infrastructure running on Windows developed over many years and can't move those workloads to Linux for various reasons (cost, time, compliance, etc),&amp;quot; the SIG chairs shared. &amp;quot;By transporting those workloads into Windows containers and running them in Kubernetes they are able to quickly modernize their infrastructure and help migrate it to the cloud.&amp;quot;&lt;/h4>
&lt;p>As anyone in the Kubernetes space can attest, companies around the world, in many different industries, see Kubernetes as their path to modernizing their infrastructure. Often this involves re-architecting or event totally re-inventing many of the ways they've been doing business. With the goal being to make their systems more scalable, more robust, and more ready for anything the future may bring. But not every application or workload can or should change the core operating system it runs on, so many teams need the ability to run containers at scale on Windows, or Linux, or both.&lt;/p>
&lt;p>&amp;quot;Sometimes the driver to Windows containers is a modernization effort and sometimes it’s because of expiring hardware warranties or end-of-support cycles for the current operating system. Our efforts in SIG-Windows enable Windows developers to take advantage of cloud native tools and Kubernetes to build and deploy distributed applications faster. That’s exciting! In essence, users can retain the benefits of application availability while decreasing costs.&amp;quot;&lt;/p>
&lt;h2 id="who-are-sig-windows">Who are SIG-Windows?&lt;/h2>
&lt;p>Who are these contributors working on enabling Windows workloads for Kubernetes? It could be you!&lt;/p>
&lt;p>Like with other Kubernetes SIGs, contributors to SIG-Windows can be anyone from independent hobbyists to professionals who work at many different companies. They come from many different parts of the world and bring to the table many different skill sets.&lt;/p>
&lt;img alt="Image of several people chatting pleasantly" width="30%" src="PeopleDoodle_transparent.png">
&lt;p>&lt;em>&amp;quot;Like most other Kubernetes SIGs, we are a very welcome and open community,&amp;quot; explained the SIG co-chairs Michael Michael and Mark Rosetti.&lt;/em>&lt;/p>
&lt;h3 id="becoming-a-contributor">Becoming a contributor&lt;/h3>
&lt;p>For anyone interested in getting started, the co-chairs added, &amp;quot;New contributors can view old community meetings on GitHub (we record every single meeting going back three years), read our documentation, attend new community meetings, ask questions in person or on Slack, and file some issues on Github. We also attend all KubeCon conferences and host 1-2 sessions, a contributor session, and meet-the-maintainer office hours.&amp;quot;&lt;/p>
&lt;p>The co-chairs also shared a glimpse into what the path looks like to becoming a member of the SIG-Windows community:&lt;/p>
&lt;p>&amp;quot;We encourage new contributors to initially just join our community and listen, then start asking some questions and get educated on Windows in Kubernetes. As they feel comfortable, they could graduate to improving our documentation, file some bugs/issues, and eventually they can be a code contributor by fixing some bugs. If they have long-term and sustained substantial contributions to Windows, they could become a technical lead or a chair of SIG-Windows. You won't know if you love this area unless you get started :) To get started, &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">visit this getting-started page&lt;/a>. It's a one stop shop with links to everything related to SIG-Windows in Kubernetes.&amp;quot;&lt;/p>
&lt;p>When asked if there were any useful skills for new contributors, the co-chairs said,&lt;/p>
&lt;p>&amp;quot;We are always looking for expertise in Go and Networking and Storage, along with a passion for Windows. Those are huge skills to have. However, we don’t require such skills, and we welcome any and all contributors, with varying skill sets. If you don’t know something, we will help you acquire it.&amp;quot;&lt;/p>
&lt;p>You can get in touch with the folks at SIG-Windows in their &lt;a href="https://kubernetes.slack.com/archives/C0SJ4AFB7">Slack channel&lt;/a> or attend one of their regular meetings - currently 30min long on Tuesdays at 12:30PM EST! You can find links to their regular meetings as well as past meeting notes and recordings from the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows#readme">SIG-Windows README&lt;/a> on GitHub.&lt;/p>
&lt;p>As a closing message from SIG-Windows:&lt;/p>
&lt;hr>
&lt;h4 id="we-welcome-you-to-get-involved-and-join-our-community-to-share-feedback-and-deployment-stories-and-contribute-to-code-docs-and-improvements-of-any-kind">&lt;em>&amp;quot;We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind.&amp;quot;&lt;/em>&lt;/h4>
&lt;hr></description></item><item><title>Blog: Working with Terraform and Kubernetes</title><link>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</link><pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://twitter.com/pst418">Philipp Strube&lt;/a>, Kubestack&lt;/p>
&lt;p>Maintaining Kubestack, an open-source &lt;a href="https://www.kubestack.com/lp/terraform-gitops-framework">Terraform GitOps Framework&lt;/a> for Kubernetes, I unsurprisingly spend a lot of time working with Terraform and Kubernetes. Kubestack provisions managed Kubernetes services like AKS, EKS and GKE using Terraform but also integrates cluster services from Kustomize bases into the GitOps workflow. Think of cluster services as everything that's required on your Kubernetes cluster, before you can deploy application workloads.&lt;/p>
&lt;p>Hashicorp recently announced &lt;a href="https://www.hashicorp.com/blog/deploy-any-resource-with-the-new-kubernetes-provider-for-hashicorp-terraform/">better integration between Terraform and Kubernetes&lt;/a>. I took this as an opportunity to give an overview of how Terraform can be used with Kubernetes today and what to be aware of.&lt;/p>
&lt;p>In this post I will however focus only on using Terraform to provision Kubernetes API resources, not Kubernetes clusters.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/intro/index.html">Terraform&lt;/a> is a popular infrastructure as code solution, so I will only introduce it very briefly here. In a nutshell, Terraform allows declaring a desired state for resources as code, and will determine and execute a plan to take the infrastructure from its current state, to the desired state.&lt;/p>
&lt;p>To be able to support different resources, Terraform requires providers that integrate the respective API. So, to create Kubernetes resources we need a Kubernetes provider. Here are our options:&lt;/p>
&lt;h2 id="terraform-kubernetes-provider-official">Terraform &lt;code>kubernetes&lt;/code> provider (official)&lt;/h2>
&lt;p>First, the &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes">official Kubernetes provider&lt;/a>. This provider is undoubtedly the most mature of the three. However, it comes with a big caveat that's probably the main reason why using Terraform to maintain Kubernetes resources is not a popular choice.&lt;/p>
&lt;p>Terraform requires a schema for each resource and this means the maintainers have to translate the schema of each Kubernetes resource into a Terraform schema. This is a lot of effort and was the reason why for a long time the supported resources where pretty limited. While this has improved over time, still not everything is supported. And especially &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources&lt;/a> are not possible to support this way.&lt;/p>
&lt;p>This schema translation also results in some edge cases to be aware of. For example, &lt;code>metadata&lt;/code> in the Terraform schema is a list of maps. Which means you have to refer to the &lt;code>metadata.name&lt;/code> of a Kubernetes resource like this in Terraform: &lt;code>kubernetes_secret.example.metadata.0.name&lt;/code>.&lt;/p>
&lt;p>On the plus side however, having a Terraform schema means full integration between Kubernetes and other Terraform resources. Like for &lt;a href="https://github.com/kbst/terraform-kubestack/blob/e5caa6d20926d546a045144ebe79c7cc8c0b4c8a/aws/_modules/eks/ingress.tf#L37">example&lt;/a>, using Terraform to create a Kubernetes service of type &lt;code>LoadBalancer&lt;/code> and then use the returned ELB hostname in a Route53 record to configure DNS.&lt;/p>
&lt;p>The biggest benefit when using Terraform to maintain Kubernetes resources is integration into the Terraform plan/apply life-cycle. So you can review planned changes before applying them. Also, using &lt;code>kubectl&lt;/code>, purging of resources from the cluster is not trivial without manual intervention. Terraform does this reliably.&lt;/p>
&lt;h2 id="terraform-kubernetes-alpha-provider">Terraform &lt;code>kubernetes-alpha&lt;/code> provider&lt;/h2>
&lt;p>Second, the new &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha">alpha Kubernetes provider&lt;/a>. As a response to the limitations of the current Kubernetes provider the Hashicorp team recently released an alpha version of a new provider.&lt;/p>
&lt;p>This provider uses dynamic resource types and server-side-apply to support all Kubernetes resources. I personally think this provider has the potential to be a game changer - even if &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha#moving-from-yaml-to-hcl">managing Kubernetes resources in HCL&lt;/a> may still not be for everyone. Maybe the Kustomize provider below will help with that.&lt;/p>
&lt;p>The only downside really is, that it's explicitly discouraged to use it for anything but testing. But the more people test it, the sooner it should be ready for prime time. So I encourage everyone to give it a try.&lt;/p>
&lt;h2 id="terraform-kustomize-provider">Terraform &lt;code>kustomize&lt;/code> provider&lt;/h2>
&lt;p>Last, we have the &lt;a href="https://github.com/kbst/terraform-provider-kustomize">&lt;code>kustomize&lt;/code> provider&lt;/a>. Kustomize provides a way to do customizations of Kubernetes resources using inheritance instead of templating. It is designed to output the result to &lt;code>stdout&lt;/code>, from where you can apply the changes using &lt;code>kubectl&lt;/code>. This approach means that &lt;code>kubectl&lt;/code> edge cases like no purging or changes to immutable attributes still make full automation difficult.&lt;/p>
&lt;p>Kustomize is a popular way to handle customizations. But I was looking for a more reliable way to automate applying changes. Since this is exactly what Terraform is great at the Kustomize provider was born.&lt;/p>
&lt;p>Not going into too much detail here, but from Terraform's perspective, this provider treats every Kubernetes resource as a JSON string. This way it can handle any Kubernetes resource resulting from the Kustomize build. But it has the big disadvantage that Kubernetes resources can not easily be integrated with other Terraform resources. Remember the load balancer example from above.&lt;/p>
&lt;p>Under the hood, similarly to the new Kubernetes alpha provider, the Kustomize provider also uses the dynamic Kubernetes client and server-side-apply. Going forward, I plan to deprecate this part of the Kustomize provider that overlaps with the new Kubernetes provider and only keep the Kustomize integration.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>For teams that are already invested into Terraform, or teams that are looking for ways to replace &lt;code>kubectl&lt;/code> in automation, Terraform's plan/apply life-cycle has always been a promising option to automate changes to Kubernetes resources. However, the limitations of the official Kubernetes provider resulted in this not seeing significant adoption.&lt;/p>
&lt;p>The new alpha provider removes the limitations and has the potential to make Terraform a prime option to automate changes to Kubernetes resources.&lt;/p>
&lt;p>Teams that have already adopted Kustomize, may find integrating Kustomize and Terraform using the Kustomize provider beneficial over &lt;code>kubectl&lt;/code> because it avoids common edge cases. Even if in this set up, Terraform can only easily be used to plan and apply the changes, not to adapt the Kubernetes resources. In the future, this issue may be resolved by combining the Kustomize provider with the new Kubernetes provider.&lt;/p>
&lt;p>If you have any questions regarding these three options, feel free to reach out to me on the Kubernetes Slack in either the &lt;a href="https://app.slack.com/client/T09NY5SBT/CMBCT7XRQ">#kubestack&lt;/a> or the &lt;a href="https://app.slack.com/client/T09NY5SBT/C9A5ALABG">#kustomize&lt;/a> channel. If you happen to give any of the providers a try and encounter a problem, please file a GitHub issue to help the maintainers fix it.&lt;/p></description></item><item><title>Blog: A Better Docs UX With Docsy</title><link>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen, Cloud Native Computing Foundation&lt;/p>
&lt;p>&lt;em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).&lt;/em>&lt;/p>
&lt;p>I'm pleased to announce that the &lt;a href="https://kubernetes.io">Kubernetes website&lt;/a> now features the &lt;a href="https://docsy.dev">Docsy Hugo theme&lt;/a>.&lt;/p>
&lt;p>The Docsy theme improves the site's organization and navigability, and opens a path to improved API references. After over 4 years with few meaningful UX improvements, Docsy implements some best practices for technical content. The theme makes the Kubernetes site easier to read and makes individual pages easier to navigate. It gives the site a much-needed facelift.&lt;/p>
&lt;p>For example: adding a right-hand rail for navigating topics on the page. No more scrolling up to navigate!&lt;/p>
&lt;p>The theme opens a path for future improvements to the website. The Docsy functionality I'm most excited about is the theme's &lt;a href="https://www.docsy.dev/docs/adding-content/shortcodes/#swaggerui">&lt;code>swaggerui&lt;/code> shortcode&lt;/a>, which provides native support for generating API references from an OpenAPI spec. The CNCF is partnering with &lt;a href="https://developers.google.com/season-of-docs">Google Season of Docs&lt;/a> (GSoD) for staffing to make better API references a reality in Q4 this year. We're hopeful to be chosen, and we're looking forward to Google's list of announced projects on August 16th. Better API references have been a personal goal since I first started working with SIG Docs in 2017. It's exciting to see the goal within reach.&lt;/p>
&lt;p>One of SIG Docs' tech leads, &lt;a href="https://github.com/kbhawkey">Karen Bradshaw&lt;/a> did a lot of heavy lifting to fix a wide range of site compatibility issues, including a fix to the last of our &lt;a href="https://github.com/kubernetes/website/pull/21359">legacy pieces&lt;/a> when we &lt;a href="2018-05-05-hugo-migration/">migrated from Jekyll to Hugo&lt;/a> in 2018. Our other tech leads, &lt;a href="https://github.com/sftim">Tim Bannister&lt;/a> and &lt;a href="https://github.com/onlydole">Taylor Dolezal&lt;/a> provided extensive reviews.&lt;/p>
&lt;p>Thanks also to &lt;a href="https://bep.is/">Björn-Erik Pedersen&lt;/a>, who provided invaluable advice about how to navigate a Hugo upgrade beyond &lt;a href="https://gohugo.io/news/0.60.0-relnotes/">version 0.60.0&lt;/a>.&lt;/p>
&lt;p>The CNCF contracted with &lt;a href="https://gearboxbuilt.com/">Gearbox&lt;/a> in Victoria, BC to apply the theme to the site. Thanks to Aidan, Troy, and the rest of the team for all their work!&lt;/p></description></item></channel></rss>